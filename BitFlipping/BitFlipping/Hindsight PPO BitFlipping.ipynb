{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Beta\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed_number = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_number)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed_number)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed_number)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "#         nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.uniform(m.weight, a=0, b=1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        a = nn.LogSoftmax()\n",
    "        act   = a(self.actor(x))\n",
    "        return act, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model,goal, vis=False):\n",
    "#     print ('\\nevaluate performance')\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done :\n",
    "        state_goal = np.concatenate((state,goal),0)\n",
    "        if vis: \n",
    "            env.render()\n",
    "        state_goal = torch.FloatTensor(state_goal).unsqueeze(0).to(device)\n",
    "        act, _ = model(state_goal)\n",
    "        next_state, reward, done, _ = env.step(np.argmax(torch.exp(act).data.cpu().numpy()[0]))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "#     print ('\\ntotal_reward: ', total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindsight GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_gae(next_value, rewards, masks, values, gamma=0.95, lamda=0.95):\n",
    "\n",
    "    for step in range(len(values)):\n",
    "        values[step] = torch.cat((values[step], next_value[step]),0)\n",
    "    returns = []\n",
    "    for outer_step in range(len(rewards)):\n",
    "        gae = 0\n",
    "        for step in range(len(rewards[outer_step])):\n",
    "            delta = rewards[outer_step][step]  - values[outer_step][step]+ gamma * values[outer_step][step + 1] * masks[outer_step][step]\n",
    "            gae = delta + gamma * lamda * masks[outer_step][step] * gae\n",
    "            returns.insert(0, gae + values[outer_step][step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Hindsight GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hindsight_gae(rewards, current_logprobs, desired_logprobs, masks, values, gamma = 0.5, lamda = 0.95):\n",
    "    returns = []\n",
    "    lambda_ret = 1\n",
    "    for outer_step in range(len(rewards)):\n",
    "        hindsight_gae = 0\n",
    "        for step in range(len(rewards[outer_step])):\n",
    "            temp = 0\n",
    "            is_weight_ratio = 1\n",
    "#             print ('len(rewards[outer]): ', len(rewards[outer_step]))\n",
    "            for step_ in range(1, len(rewards[outer_step])):\n",
    "                ratio = np.exp(current_logprobs[outer_step][step_] - desired_logprobs[outer_step][step_])\n",
    "                clipped_ratio = lambda_ret * np.clip(ratio, a_min = -1, a_max = 1)\n",
    "#                 print ('clipped_ratio: ', clipped_ratio)\n",
    "                is_weight_ratio = is_weight_ratio * clipped_ratio\n",
    "                # print ('is_weight_ratio: ', is_weight_ratio)\n",
    "            for step_ in range(1, len(rewards[outer_step])):\n",
    "                temp = temp + ((gamma ** (step_+1)) * rewards[outer_step][step_] - \n",
    "                               (gamma ** (step_)) * rewards[outer_step][step_])  \n",
    "            temp = temp - (gamma ** (step + 1)) * rewards[outer_step][step]\n",
    "\n",
    "            delta = rewards[outer_step][step] + is_weight_ratio * temp\n",
    "            hindsight_gae = delta + gamma * lamda * masks[outer_step][step] * hindsight_gae\n",
    "            returns.insert(0, torch.FloatTensor([hindsight_gae]).to(device) + values[outer_step][step])\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    actions = actions.view(len(actions),1)\n",
    "    log_probs = log_probs.view(len(log_probs),1)\n",
    "    returns = returns.view(len(returns),1)\n",
    "    advantage = advantage.view(len(advantage),1)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    print ('update ppo')\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    clip = 5\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            act, value = model(state)\n",
    "            action = np.argmax((act.data.cpu().numpy()),axis = 1)\n",
    "            one_hot_action = to_one_hot(action, BITS_NUMBER)\n",
    "            new_log_probs = np.sum((act.data.cpu().numpy() * one_hot_action),1)\n",
    "            ratio = np.exp(new_log_probs - old_log_probs)\n",
    "            ratio = torch.FloatTensor(ratio.float()).to(device)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - value).pow(2).mean()\n",
    "\n",
    "            actor_loss_list.append(actor_loss.data.cpu().numpy().item(0))\n",
    "            critic_loss_list.append(critic_loss.data.cpu().numpy().item(0))\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradient to prevent gradient exploding\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "    mean_actor_loss = np.mean(actor_loss_list)\n",
    "    mean_critic_loss = np.mean(critic_loss_list)\n",
    "    \n",
    "    mean_actor_loss_list.append(mean_actor_loss)\n",
    "    mean_critic_loss_list.append(mean_critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitFlip():\n",
    "    def __init__(self, n, reward_type, max_steps):\n",
    "        self.number_of_bits = n # number of bits\n",
    "        self.reward_type = reward_type\n",
    "        self.max_steps = max_steps\n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        self.goal = np.random.randint(2, size=(self.number_of_bits)) # a random sequence of 0's and 1's\n",
    "        self.state = np.random.randint(2, size=(self.number_of_bits)) \n",
    "        return np.array(self.state) ,np.array(self.goal)\n",
    "    def step(self, action):\n",
    "        if action >= self.number_of_bits:\n",
    "            raise Exception('Action out of range')\n",
    "        self.n_steps += 1\n",
    "        self.state[action] = 1-self.state[action] # flip this bit\n",
    "        done = np.array_equal(self.state, self.goal) or (self.max_steps <= self.n_steps)\n",
    "        if self.reward_type == 'sparse':\n",
    "            reward = 0 if np.array_equal(self.state, self.goal) else -1\n",
    "        else:\n",
    "            reward = -np.sum(np.square(self.state-self.goal))\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    def render(self):\n",
    "        print(\"\\rstate :\", np.array_str(self.state), \", goal :\", np.array_str(self.goal), end=' '*10)\n",
    "    def get_state_dim(self):\n",
    "        state_dim = self.number_of_bits\n",
    "        return state_dim\n",
    "    def get_action_dim(self):\n",
    "        action_dim = self.number_of_bits # since it is determining which bit to change\n",
    "        return action_dim\n",
    "    \n",
    "BITS_NUMBER = 8\n",
    "MAX_STEPS = BITS_NUMBER\n",
    "env = BitFlip(BITS_NUMBER, 'sparse', MAX_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Goal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial subgoal sampled is:  [0 1 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return random.randint(0,BITS_NUMBER-1)\n",
    "    \n",
    "agent = RandomAgent(env.get_action_dim())\n",
    "\n",
    "episode_count = 100\n",
    "reward = 0\n",
    "done = False\n",
    "initial_subgoals = []\n",
    "        \n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "#     print (state)\n",
    "    done_count = 0\n",
    "    while True:\n",
    "        action = agent.act(state, reward, done)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    initial_subgoals.append(state)\n",
    "    \n",
    "random.seed(seed_number)\n",
    "initial_subgoal = initial_subgoals[random.randint(0, len(initial_subgoals)-1)]\n",
    "print ('Initial subgoal sampled is: ', initial_subgoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(index, dim):\n",
    "    if isinstance(index, np.int) or isinstance(index, np.int64):\n",
    "        one_hot = np.zeros(dim)\n",
    "        one_hot[index] = 1.\n",
    "    else:\n",
    "        one_hot = np.zeros((len(index), dim))\n",
    "        one_hot[np.arange(len(index)), index] = 1.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = env.get_state_dim()\n",
    "num_outputs = env.get_action_dim()\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_restarts     = 16\n",
    "mini_batch_size  = 16\n",
    "ppo_epochs       = 4\n",
    "\n",
    "model = ActorCritic(2*num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ep_max     = 20\n",
    "ep_count   = 0\n",
    "eval_num   = 1\n",
    "num_steps  = 0\n",
    "\n",
    "test_rewards = []\n",
    "mean_actor_loss_list = []\n",
    "mean_critic_loss_list = []\n",
    "best_return = -99999\n",
    "\n",
    "# Model-based parameter\n",
    "use_modelbased = False\n",
    "\n",
    "early_stop = False\n",
    "zero_reward_count = 0\n",
    "done = False\n",
    "\n",
    "while ep_count < ep_max and not early_stop:\n",
    "    if ep_count == 0: \n",
    "        goal = initial_subgoal\n",
    "    else: \n",
    "        goal = temp\n",
    "                \n",
    "#     print ('goal: ', goal)\n",
    "#     print ('desired_goal: ', ep_desired_goal)\n",
    "    \n",
    "    ep_log_probs = []\n",
    "    ep_log_probs_desired = []\n",
    "    ep_values    = []\n",
    "    ep_states    = []\n",
    "    ep_actions   = []\n",
    "    ep_rewards   = []\n",
    "    ep_masks     = []\n",
    "    ep_state_goals = []\n",
    "    ep_next_state_goals = []\n",
    "    ep_state_desired_goals = []\n",
    "    subgoals = []\n",
    "    desired_goals = []\n",
    "    for _ in range(num_restarts):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        desired_goal = state[1]\n",
    "        state = state[0]\n",
    "        # print ('initial_state: ', state)\n",
    "        # print ('desired_goal: ', desired_goal)\n",
    "        \n",
    "        log_probs = []\n",
    "        log_probs_desired = []\n",
    "        values    = []\n",
    "        states    = []\n",
    "        actions   = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        state_goals = []\n",
    "        next_state_goals = []\n",
    "        state_desired_goals = []\n",
    "        \n",
    "        \n",
    "        desired_goals.append(desired_goal)\n",
    "        while not done:\n",
    "            # for subgoal\n",
    "            state_goal = np.concatenate((state,goal),0)\n",
    "            state_desired_goal = np.concatenate((state, desired_goal), 0)\n",
    "            \n",
    "            state_goal = torch.FloatTensor(state_goal).to(device)\n",
    "            state_desired_goal = torch.FloatTensor(state_desired_goal).to(device)\n",
    "            \n",
    "            # for subgoal\n",
    "            act, value = model(state_goal)\n",
    "            action = np.argmax(act.data.cpu().numpy())\n",
    "            one_hot_action = to_one_hot(action, BITS_NUMBER)\n",
    "            log_prob = np.sum(act.data.cpu().numpy() * one_hot_action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # for desired goal\n",
    "            act_desired, value_desired = model(state_desired_goal)\n",
    "            action_desired = np.argmax(act_desired.data.cpu().numpy())\n",
    "            one_hot_action_desired = to_one_hot(action_desired, BITS_NUMBER)\n",
    "            log_prob_desired = np.sum(act_desired.data.cpu().numpy() * one_hot_action)\n",
    "            if reward == 0:\n",
    "                zero_reward_count += 1 \n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            log_probs_desired.append(log_prob_desired)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            masks.append(1-done)\n",
    "\n",
    "            state_goal = list(state_goal.data.cpu().numpy())\n",
    "            \n",
    "            states.append(state)\n",
    "            state_goals.append(state_goal)\n",
    "            actions.append(action)\n",
    "            temp = state\n",
    "            state = next_state\n",
    "            \n",
    "            # for subgoal\n",
    "            next_state_goal = np.concatenate((next_state,goal),0)\n",
    "            next_state_goals.append(next_state_goal)\n",
    "            \n",
    "            num_steps += 1\n",
    "        subgoals.append(temp)\n",
    "        ep_states.append(states)\n",
    "        ep_actions.append(torch.FloatTensor(actions).to(device))\n",
    "        ep_state_goals.append(torch.FloatTensor(state_goals).to(device))\n",
    "        ep_values.append(torch.FloatTensor(values).to(device))\n",
    "        ep_values_copy = ep_values.copy()\n",
    "        ep_rewards.append(rewards)\n",
    "        ep_masks.append(masks)\n",
    "        ep_next_state_goals.append(next_state_goals)\n",
    "        ep_log_probs.append(torch.FloatTensor(log_probs).to(device))\n",
    "        ep_log_probs_desired.append(log_probs_desired)\n",
    "        \n",
    "    for i in range(len(subgoals)):\n",
    "        if np.array_equal(subgoals[i], desired_goals[i]):\n",
    "            temp = subgoals[i]\n",
    "            break\n",
    "        else:\n",
    "            index = random.randint(0,len(subgoals)-1)\n",
    "            temp = subgoals[index]\n",
    "\n",
    "    ep_count += 1\n",
    "    if ep_count % eval_num == 0:\n",
    "        test_reward = np.mean([test_env(model, goal, False) for _ in range(10)])\n",
    "        if test_reward >= best_return:\n",
    "            best_return = test_reward\n",
    "        if test_reward >= -3: \n",
    "            early_stop = True\n",
    "        if test_reward != -8.0:\n",
    "            test_rewards.append(test_reward)\n",
    "\n",
    "        print ('\\nepisode {0} mean_rewards: {1}'.format(ep_count,test_reward))\n",
    "            \n",
    "    zero_freq = float(zero_reward_count) / num_steps\n",
    "    other_freq = 1 - zero_freq \n",
    "    \n",
    "    next_state_goals = []\n",
    "    for step in range(len(ep_next_state_goals)):\n",
    "        next_state_goals.append(ep_next_state_goals[step][-1])\n",
    "    next_state_goals = torch.FloatTensor(next_state_goals).to(device)\n",
    "    _, next_value = model(next_state_goals)\n",
    "\n",
    "    current_logprobs     = ep_log_probs \n",
    "    desired_logprobs     = ep_log_probs_desired\n",
    "    \n",
    "#     returns      = is_hindsight_gae(ep_rewards, current_logprobs, desired_logprobs, ep_masks, ep_values)\n",
    "    returns = torch.FloatTensor(hindsight_gae(next_value, ep_rewards, ep_masks, ep_values_copy)).to(device)\n",
    "#     returns      = torch.cat(returns).detach()\n",
    "    ep_actions   = torch.cat(ep_actions).detach()\n",
    "    ep_values    = torch.cat(ep_values).detach()\n",
    "    ep_log_probs  = torch.cat(ep_log_probs).detach()\n",
    "    ep_state_goals  = torch.cat(ep_state_goals).detach()\n",
    "    advantage    = returns - ep_values\n",
    "\n",
    "    ppo_update(ppo_epochs, mini_batch_size, ep_state_goals, ep_actions, ep_log_probs, returns, advantage)\n",
    "\n",
    "    print ('\\nactor_loss: {0:.3f}, critic_loss: {1:.3f}, best_return: {2:.3f}'\n",
    "           .format(mean_actor_loss_list[ep_count-1], mean_critic_loss_list[ep_count-1], best_return))\n",
    "    print ('====================================================================')\n",
    "print ('zero_reward_count: ', zero_reward_count)\n",
    "print ('zero_freq: ', zero_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Test Reward Plot/test_rewards_hppo_bitflipping8', 'wb') as fp1:\n",
    "    pickle.dump(test_rewards, fp1)\n",
    "with open('./Loss Plot/mean_actor_loss_hppo_bitflipping8', 'wb') as fp2:\n",
    "    pickle.dump(mean_actor_loss_list, fp2)\n",
    "with open('./Loss Plot/mean_critic_loss_hppo_bitflipping8', 'wb') as fp3:\n",
    "    pickle.dump(mean_critic_loss_list, fp3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weixiang",
   "language": "python",
   "name": "weixiang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
