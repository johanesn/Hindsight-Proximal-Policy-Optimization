{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Beta\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed_number = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_number)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed_number)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed_number)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value\n",
    "\n",
    "class ModelBasedGoalNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(ModelBasedGoalNetwork, self).__init__()\n",
    "        \n",
    "        self.goalnetwork = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        subgoal = self.goalnetwork(x)\n",
    "        return subgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, goal, vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_goal = np.concatenate((state,goal),0)\n",
    "        state_goal = torch.FloatTensor(state_goal).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state_goal)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hindsight GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, lamda=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lamda * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importance Hindsight GAE </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_gae(rewards, current_logprobs, desired_logprobs, masks, values, gamma = 0.995, lamda = 0):\n",
    "    lambda_ret = 1\n",
    "    hindsight_gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in range(len(rewards)):\n",
    "        temp = 0\n",
    "        is_weight_ratio = 1\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            ratio = (current_logprobs[step_] - desired_logprobs[step_]).exp() \n",
    "            clipped_ratio = lambda_ret * torch.clamp(ratio, max = 1)\n",
    "            is_weight_ratio = is_weight_ratio * clipped_ratio\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            temp = temp + ((gamma ** (step_+1)) * rewards[step_] - (gamma ** (step_)) * rewards[step_])  \n",
    "        temp = temp - (gamma ** (step + 1)) * rewards[step]\n",
    "        \n",
    "        delta = rewards[step] + is_weight_ratio * temp\n",
    "        hindsight_gae = delta + gamma * lamda * masks[step] * hindsight_gae\n",
    "        returns.insert(0, hindsight_gae + values[step])\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Compute Return </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma = 0.995):\n",
    "    returns = 0\n",
    "    returns_list = []\n",
    "    for step in range(len(rewards)):\n",
    "        returns = returns + (gamma ** i) * rewards[step] \n",
    "        returns_list.insert(0,returns)\n",
    "    return returns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :],actions[rand_ids,:],log_probs[rand_ids,:],returns[rand_ids,:],advantage[rand_ids,:]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, \n",
    "               advantages, episode_count, test_reward, best_return, clip_param=0.2):\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    clip = 5\n",
    "    for ppo_epoch in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, \n",
    "                                                                         actions, log_probs, returns, advantages):\n",
    "            model.zero_grad()\n",
    "            dist, value = model(state)\n",
    "\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            # MSE Loss\n",
    "            critic_loss = (return_ - value).pow(2).mean() \n",
    "            \n",
    "            # Huber Loss\n",
    "            # critic_loss = nn.functional.smooth_l1_loss(value, return_)\n",
    "            \n",
    "            actor_loss_list.append(actor_loss.data.cpu().numpy().item(0))\n",
    "            critic_loss_list.append(critic_loss.data.cpu().numpy().item(0))\n",
    "            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.0001 * entropy\n",
    "            \n",
    "            # optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradient to prevent gradient exploding\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "    \n",
    "    mean_actor_loss = np.mean(actor_loss_list)\n",
    "    mean_critic_loss = np.mean(critic_loss_list)\n",
    "    \n",
    "    mean_actor_loss_list.append(mean_actor_loss)\n",
    "    mean_critic_loss_list.append(mean_critic_loss)\n",
    "    \n",
    "    assert ~np.isnan(mean_critic_loss), \"Assert error: critic loss has nan value.\" \n",
    "    assert ~np.isinf(mean_critic_loss), \"Assert error: critic loss has inf value.\"\n",
    "\n",
    "    print ('\\nEpisode: {0}, actor_loss: {1:.3f}, critic_loss: {2:.3f}, mean_reward: {3:.3f}, best_return: {4:.3f}'\n",
    "           .format(episode_count, mean_actor_loss, mean_critic_loss, test_reward, best_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env(i):\n",
    "    def _thunk():\n",
    "        \n",
    "        env = gym.make(env_name)\n",
    "        env.seed(i+seed_number)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env(i) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Goal Distribution Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 50\n",
    "reward = 0\n",
    "done = False\n",
    "initial_subgoals = []\n",
    "        \n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "#     print (state)\n",
    "    done_count = 0\n",
    "    while True:\n",
    "        action = agent.act(state, reward, done)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    initial_subgoals.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial subgoal sampled is:  [ 0.11320659 -0.99357147 -0.14532486]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed_number)\n",
    "initial_subgoal = initial_subgoals[random.randint(0, len(initial_subgoals)-1)]\n",
    "print ('Initial subgoal sampled is: ', initial_subgoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Based Goal Generator </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_trajectories(num_traj):\n",
    "    print ('generating random trajectories...')\n",
    "    dataset_random = []\n",
    "\n",
    "    game_rewards = []\n",
    "    for n in range(num_traj):\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            sampled_action = env.action_space.sample()\n",
    "            new_obs, reward, done, _ = env.step(sampled_action)\n",
    "\n",
    "            dataset_random.append([obs, new_obs, reward, done, sampled_action])\n",
    "\n",
    "            obs = new_obs\n",
    "            game_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # print some stats\n",
    "    print('mean rand_dataset reward:',np.round(np.sum(game_rewards)/num_traj,2), \n",
    "          'max rand_dataset reward:', np.round(np.max(game_rewards),2), np.round(len(game_rewards)/num_traj))\n",
    "\n",
    "    return dataset_random\n",
    "\n",
    "def flatten_rl_dataset(temp, rl_dataset):\n",
    "    for i in range(num_steps):\n",
    "        for j in range(num_envs):\n",
    "            rl_dataset.append([temp[0][0][i][j],temp[0][1][i][j],temp[0][2][i][j].cpu().numpy(),\n",
    "                               temp[0][3][i][j].astype('bool'),temp[0][4][i][j].cpu().numpy()])\n",
    "    return rl_dataset\n",
    "\n",
    "def filtered_prep(dataset, min_dist):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        curr_dist = np.linalg.norm(dataset[i][0]-desired_goal)\n",
    "        if (curr_dist > (min_dist - 0.2)) and (curr_dist < (min_dist + 0.2)):\n",
    "            new_dataset.append(dataset[i])\n",
    "    return np.array(new_dataset)\n",
    "                    \n",
    "def MSELoss(y_truth, y_pred):\n",
    "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
    "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
    "\n",
    "\n",
    "def goal_network_update(dataset, train_iter, goal_model, goal_optimizer):\n",
    "    # split dataset to 80% training and 20% validation\n",
    "    len_data = len(dataset)\n",
    "    \n",
    "    d_train  = dataset[:int(len_data * 0.8)]\n",
    "    d_valid  = dataset[int(len_data * 0.8):-1]\n",
    "    \n",
    "    state_action_input = np.concatenate((dataset[-1][0],dataset[-1][4]),0)\n",
    "    sff      = np.arange(len(d_train))\n",
    "    np.random.shuffle(sff)\n",
    "    d_train  = d_train[sff]\n",
    "    \n",
    "    # training dataset\n",
    "    x_train  = np.array([np.concatenate([s,a]) for s,_,_,_,a in d_train]) \n",
    "    y_train  = np.array([ns for _,ns,_,_,_ in d_train])\n",
    "    \n",
    "    # validation dataset\n",
    "    x_valid  = np.array([np.concatenate([s,a]) for s,_,_,_,a in d_valid])\n",
    "    y_valid  = np.array([ns for _,ns,_,_,_ in d_valid])\n",
    "\n",
    "    losses_goal = []\n",
    "    mse_valid = []\n",
    "    # go through max_model_iter supervised iterations\n",
    "    for i in range(train_iter):\n",
    "        x_train += np.random.normal(loc = 0, scale=0.001, size = x_train.shape)\n",
    "\n",
    "        goal_optimizer.zero_grad()\n",
    "        pred_goal = goal_model((torch.tensor(x_train)).type(torch.FloatTensor).to(device))\n",
    "        goal_loss = MSELoss(y_train, pred_goal)\n",
    "        # print ('goal_loss: ', goal_loss.cpu().detach().numpy())\n",
    "        losses_goal.append(goal_loss.cpu().detach().numpy())\n",
    "        goal_loss.backward()\n",
    "        goal_optimizer.step()\n",
    "\n",
    "        # iteratively evaluate\n",
    "        if i % 5 == 0:\n",
    "            goal_model.eval()\n",
    "            pred_goal = goal_model((torch.tensor(x_valid)).type(torch.FloatTensor).to(device))\n",
    "            goal_model.train(True)\n",
    "            valid_goal_loss = MSELoss(y_valid, pred_goal)\n",
    "            mse_valid.append(valid_goal_loss)\n",
    "            # print ('evaluation iteration: ',i, ' ,valid_goal_loss: ', valid_goal_loss.cpu().detach().numpy())\n",
    "    # final evaluation\n",
    "    goal_model.eval()\n",
    "    pred_goal = goal_model((torch.tensor(x_valid)).type(torch.FloatTensor).to(device))\n",
    "    goal_model.train(True)\n",
    "    valid_goal_loss = MSELoss(y_valid, pred_goal)\n",
    "    mse_valid.append(valid_goal_loss)\n",
    "    # print ('end of training validation goal loss: ', valid_goal_loss.cpu().detach().numpy())\n",
    "    return losses_goal, mse_valid, state_action_input\n",
    "\n",
    "def model_based_goal_generator(rand_dataset, rl_dataset, rand_num_traj, mb_train_iter, min_dist, \n",
    "                               goal_model, goal_optimizer):\n",
    "    print ('generate goal using supervised model based...')\n",
    "\n",
    "    \n",
    "    if len(rl_dataset) > 0:\n",
    "        concat_dataset    = np.concatenate([rand_dataset, rl_dataset], axis=0)\n",
    "    else:\n",
    "        concat_dataset    = np.array(rl_dataset)\n",
    "    print ('len after concat dataset: ', len(concat_dataset))\n",
    "\n",
    "    filtered_dataset  = filtered_prep (concat_dataset, min_dist)\n",
    "    print ('len filtered concat dataset: ', len(filtered_dataset))\n",
    "    # if filtered dataset empty\n",
    "    if len(filtered_dataset) > 0:\n",
    "        mb_loss, mse_eval, state_action_input = goal_network_update(filtered_dataset, mb_train_iter, \n",
    "                                                                goal_model, goal_optimizer)\n",
    "        all_mb_loss.append(mb_loss)\n",
    "        all_goal_mse_eval.append(mse_eval)\n",
    "\n",
    "        model_based_goal = goal_model(torch.FloatTensor(state_action_input).to(device))\n",
    "\n",
    "        return model_based_goal\n",
    "    else:\n",
    "        return [0]\n",
    "def evaluate_goals(state, num_envs, goal, desired_goal):\n",
    "    sf_subgoal = state[random.randint(0, num_envs - 1)]\n",
    "    mb_distance = np.linalg.norm(goal - desired_goal)\n",
    "    sf_distance = np.linalg.norm(sf_subgoal - desired_goal) \n",
    "    print ('mb subgoal: ', goal, ' distance: ', mb_distance)\n",
    "    print ('sample final subgoal: ', sf_subgoal, 'distance: ', sf_distance)\n",
    "    mb_distance_list.append(mb_distance)\n",
    "    sf_distance_list.append(sf_distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random trajectories...\n",
      "mean rand_dataset reward: -1275.48 max rand_dataset reward: -1.63 200.0\n",
      "\n",
      "Episode: 0, actor_loss: 0.052, critic_loss: 25.468, mean_reward: -1204.931, best_return: -1204.931\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  13\n",
      "mb subgoal:  [-0.8257032   0.66481185 -4.9530663 ]  distance:  5.065237095518225\n",
      "sample final subgoal:  [-0.56493398 -0.82513611 -6.43949962] distance:  6.516682844938181\n",
      "\n",
      "Episode: 1, actor_loss: 0.087, critic_loss: 25.867, mean_reward: -1228.206, best_return: -1204.931\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  26\n",
      "mb subgoal:  [-0.86883104  0.58116186 -6.817758  ]  distance:  6.897422834656414\n",
      "sample final subgoal:  [-0.99366727  0.11236257  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 2, actor_loss: 1.165, critic_loss: 17.923, mean_reward: -1258.006, best_return: -1204.931\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  39\n",
      "mb subgoal:  [-0.82998407  0.6344135  -6.844043  ]  distance:  6.923313915058913\n",
      "sample final subgoal:  [-0.57001187  0.82163646 -6.50625076] distance:  6.5826513605678425\n",
      "\n",
      "Episode: 3, actor_loss: 1.434, critic_loss: 18.354, mean_reward: -1235.341, best_return: -1204.931\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  42\n",
      "mb subgoal:  [-0.78877336  0.6979027  -6.7882495 ]  distance:  6.869465971562532\n",
      "sample final subgoal:  [ 0.45877945 -0.88855018  6.23927553] distance:  6.318904899467625\n",
      "\n",
      "Episode: 4, actor_loss: -0.238, critic_loss: 11.571, mean_reward: -1170.574, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  47\n",
      "mb subgoal:  [-0.75115705  0.74125135 -6.7493477 ]  distance:  6.831353062661569\n",
      "sample final subgoal:  [-0.90331566 -0.42897648  0.5590574 ] distance:  1.1456636389427828\n",
      "\n",
      "Episode: 5, actor_loss: 0.764, critic_loss: 6.101, mean_reward: -1219.240, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  56\n",
      "mb subgoal:  [-0.752955    0.66168875 -6.424519  ]  distance:  6.50224718348847\n",
      "sample final subgoal:  [-0.94559735  0.32533928 -4.78819746] distance:  4.891506402395048\n",
      "\n",
      "Episode: 6, actor_loss: 0.080, critic_loss: 4.557, mean_reward: -1211.215, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  65\n",
      "mb subgoal:  [-0.4732255   0.95374316 -6.20273   ]  distance:  6.293443418670186\n",
      "sample final subgoal:  [0.97737757 0.21150198 4.11091944] distance:  4.230798819357189\n",
      "\n",
      "Episode: 7, actor_loss: 0.550, critic_loss: 5.272, mean_reward: -1172.745, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  74\n",
      "mb subgoal:  [-0.39530873  0.97715163 -6.096721  ]  distance:  6.187172485075069\n",
      "sample final subgoal:  [-0.99778514 -0.06651931 -5.65984196] distance:  5.747504765510521\n",
      "\n",
      "Episode: 8, actor_loss: 0.031, critic_loss: 2.422, mean_reward: -1242.472, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  92\n",
      "mb subgoal:  [-0.3525792  0.9688709 -6.020723 ]  distance:  6.108365305481037\n",
      "sample final subgoal:  [0.72231103 0.69156834 5.0098259 ] distance:  5.108654960080417\n",
      "\n",
      "Episode: 9, actor_loss: -0.728, critic_loss: 9.922, mean_reward: -1180.645, best_return: -1170.574\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  113\n",
      "mb subgoal:  [-0.3285945   0.95417887 -5.9510427 ]  distance:  6.036003670652761\n",
      "sample final subgoal:  [0.46002144 0.88790781 0.41330825] distance:  1.0820460759409263\n",
      "\n",
      "Episode: 10, actor_loss: -0.517, critic_loss: 10.210, mean_reward: -1168.121, best_return: -1168.121\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  135\n",
      "mb subgoal:  [-0.3304819  0.9794917 -5.9009924]  distance:  5.990854155867272\n",
      "sample final subgoal:  [ 0.85236586 -0.52294593  4.40878535] distance:  4.520772975852199\n",
      "\n",
      "Episode: 11, actor_loss: 0.558, critic_loss: 7.094, mean_reward: -1240.669, best_return: -1168.121\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  183\n",
      "mb subgoal:  [-0.33415902  0.9850894  -5.8769727 ]  distance:  5.968322313915584\n",
      "sample final subgoal:  [-0.70956339 -0.70464161 -1.08941426] distance:  1.478791203958115\n",
      "\n",
      "Episode: 12, actor_loss: 0.554, critic_loss: 5.405, mean_reward: -1254.028, best_return: -1168.121\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  222\n",
      "mb subgoal:  [-0.3299641   0.97357947 -5.8678083 ]  distance:  5.957172821368434\n",
      "sample final subgoal:  [0.34470501 0.93871106 5.71505592] distance:  5.801884534487744\n",
      "\n",
      "Episode: 13, actor_loss: 0.331, critic_loss: 8.809, mean_reward: -1090.544, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  225\n",
      "mb subgoal:  [-0.3224695   0.96333003 -5.859571  ]  distance:  5.946979350747377\n",
      "sample final subgoal:  [-0.99738364  0.0722902   5.50623793] distance:  5.5963073657881495\n",
      "\n",
      "Episode: 14, actor_loss: -0.306, critic_loss: 5.399, mean_reward: -1127.685, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  257\n",
      "mb subgoal:  [-0.31593883  0.9565018  -5.8539953 ]  distance:  5.940031502207814\n",
      "sample final subgoal:  [-0.79640073  0.60476928  0.26284532] distance:  1.0339669529784212\n",
      "\n",
      "Episode: 15, actor_loss: -0.574, critic_loss: 8.447, mean_reward: -1383.559, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  274\n",
      "mb subgoal:  [-0.31374088  0.9549178  -5.849078  ]  distance:  5.934813970062501\n",
      "sample final subgoal:  [ 0.96500799 -0.26222048  2.30774263] distance:  2.5150896738078674\n",
      "\n",
      "Episode: 16, actor_loss: -1.439, critic_loss: 9.981, mean_reward: -1285.720, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  275\n",
      "mb subgoal:  [-0.17695189 -0.9946704   5.865246  ]  distance:  5.9516207603437445\n",
      "sample final subgoal:  [ 0.12520457 -0.99213095  3.25867831] distance:  3.4086631257498614\n",
      "\n",
      "Episode: 17, actor_loss: -2.783, critic_loss: 16.204, mean_reward: -1316.811, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  311\n",
      "mb subgoal:  [-0.27574086  0.96206975 -5.0627394 ]  distance:  5.16071130541366\n",
      "sample final subgoal:  [-0.60663181  0.79498292 -7.28392083] distance:  7.352244731067\n",
      "\n",
      "Episode: 18, actor_loss: 1.090, critic_loss: 16.882, mean_reward: -1318.392, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  310\n",
      "mb subgoal:  [-0.27511132  0.95793617 -5.024957  ]  distance:  5.122843214424737\n",
      "sample final subgoal:  [0.4605143  0.88765228 0.99708412] distance:  1.4121532289924124\n",
      "\n",
      "Episode: 19, actor_loss: 5.102, critic_loss: 28.431, mean_reward: -1344.590, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  305\n",
      "mb subgoal:  [ 0.10919569 -1.0032848   5.3203564 ]  distance:  5.415228158346864\n",
      "sample final subgoal:  [-0.40414623  0.91469439  0.38691208] distance:  1.0722410918818543\n",
      "\n",
      "Episode: 20, actor_loss: -1.700, critic_loss: 16.177, mean_reward: -1207.268, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  21\n",
      "mb subgoal:  [ 0.08734103 -1.0050944   5.3037963 ]  distance:  5.398897880666322\n",
      "sample final subgoal:  [ 0.34892193  0.9371518  -1.69662314] distance:  1.9693984038542378\n",
      "\n",
      "Episode: 21, actor_loss: 0.070, critic_loss: 10.771, mean_reward: -1216.719, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  27\n",
      "mb subgoal:  [ 0.1196329 -1.003813   5.32298  ]  distance:  5.41812402317232\n",
      "sample final subgoal:  [ 0.53175132  0.84690055 -5.59488939] distance:  5.683554107821056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 22, actor_loss: 1.786, critic_loss: 11.913, mean_reward: -1288.585, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  44\n",
      "mb subgoal:  [ 0.1135449 -0.9914758  5.331623 ]  distance:  5.424216200268805\n",
      "sample final subgoal:  [-0.6893503   0.72442816  6.01977121] distance:  6.10226559536636\n",
      "\n",
      "Episode: 23, actor_loss: 0.033, critic_loss: 8.348, mean_reward: -1259.072, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  59\n",
      "mb subgoal:  [ 0.11280993 -0.98376054  5.3346844 ]  distance:  5.4258057669888435\n",
      "sample final subgoal:  [-0.06028885  0.99818097 -0.58711908] distance:  1.1596158024353178\n",
      "\n",
      "Episode: 24, actor_loss: 2.128, critic_loss: 8.342, mean_reward: -1184.732, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  70\n",
      "mb subgoal:  [ 0.10788915 -0.983689    5.3393445 ]  distance:  5.43027474845188\n",
      "sample final subgoal:  [-0.97558861 -0.21960618 -7.74480659] distance:  7.809099120584496\n",
      "\n",
      "Episode: 25, actor_loss: -0.797, critic_loss: 6.733, mean_reward: -1241.489, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  77\n",
      "mb subgoal:  [ 0.11220514 -0.9775797   5.3476253 ]  distance:  5.437402685353198\n",
      "sample final subgoal:  [ 0.74758645 -0.66416451 -3.74730162] distance:  3.8784364633558193\n",
      "\n",
      "Episode: 26, actor_loss: 0.979, critic_loss: 10.305, mean_reward: -1135.430, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  109\n",
      "mb subgoal:  [ 0.1073399  -0.98500186  5.3431597 ]  distance:  5.434253016018598\n",
      "sample final subgoal:  [ 0.654734    0.75585937 -3.87115645] distance:  3.9982311382053477\n",
      "\n",
      "Episode: 27, actor_loss: 1.012, critic_loss: 9.910, mean_reward: -1170.278, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  109\n",
      "mb subgoal:  [ 0.10781395 -0.98542625  5.3461576 ]  distance:  5.4372869423786385\n",
      "sample final subgoal:  [-0.81067672 -0.58549402  5.70836388] distance:  5.795292759831959\n",
      "\n",
      "Episode: 28, actor_loss: 0.372, critic_loss: 5.137, mean_reward: -1174.756, best_return: -1090.544\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  102\n",
      "mb subgoal:  [ 0.10528159 -0.9898287   5.363523  ]  distance:  5.455109909148937\n",
      "sample final subgoal:  [-0.79993207  0.60009056  6.79853994] distance:  6.8716915873254605\n",
      "\n",
      "Episode: 29, actor_loss: 10.815, critic_loss: 27.367, mean_reward: -977.640, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  114\n",
      "mb subgoal:  [-0.5568294 -0.8120357 -6.4324036]  distance:  6.507324836989004\n",
      "sample final subgoal:  [-0.83707385  0.54708991 -0.2224211 ] distance:  1.0244369898725494\n",
      "\n",
      "Episode: 30, actor_loss: -1.114, critic_loss: 8.235, mean_reward: -1099.913, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  122\n",
      "mb subgoal:  [-0.55982804 -0.81840646 -6.433519  ]  distance:  6.50948245559152\n",
      "sample final subgoal:  [ 0.32189313 -0.94677601  0.54015618] distance:  1.1365600267127105\n",
      "\n",
      "Episode: 31, actor_loss: -0.084, critic_loss: 5.294, mean_reward: -1274.682, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  133\n",
      "mb subgoal:  [-0.5574143  -0.81637365 -6.4325395 ]  distance:  6.508051979531054\n",
      "sample final subgoal:  [-0.9769247   0.213584    1.76515586] distance:  2.028737342167778\n",
      "\n",
      "Episode: 32, actor_loss: 2.178, critic_loss: 9.499, mean_reward: -1315.572, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  127\n",
      "mb subgoal:  [-0.55746746 -0.81856745 -6.4338307 ]  distance:  6.50960833029529\n",
      "sample final subgoal:  [0.91732281 0.39814427 3.04288447] distance:  3.2029901450195215\n",
      "\n",
      "Episode: 33, actor_loss: -1.022, critic_loss: 7.301, mean_reward: -1213.849, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  122\n",
      "mb subgoal:  [-0.560027   -0.81967735 -6.4387956 ]  distance:  6.5148744849932125\n",
      "sample final subgoal:  [ 0.91256871 -0.4089234   3.05598136] distance:  3.2154349708493086\n",
      "\n",
      "Episode: 34, actor_loss: 1.844, critic_loss: 6.373, mean_reward: -1149.137, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  115\n",
      "mb subgoal:  [-0.7391865  -0.67411584 -6.21986   ]  distance:  6.299800652139847\n",
      "sample final subgoal:  [-0.37540369 -0.92686141 -2.96573644] distance:  3.12979114734646\n",
      "\n",
      "Episode: 35, actor_loss: 0.584, critic_loss: 10.120, mean_reward: -1160.702, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  123\n",
      "mb subgoal:  [-0.7373572 -0.6760213 -6.21682  ]  distance:  6.296788735164148\n",
      "sample final subgoal:  [-0.44283442  0.89660341 -3.6581797 ] distance:  3.7923974896838097\n",
      "\n",
      "Episode: 36, actor_loss: 0.623, critic_loss: 7.974, mean_reward: -1102.080, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  113\n",
      "mb subgoal:  [ 0.24815036 -0.96977806  4.54996   ]  distance:  4.658775089368252\n",
      "sample final subgoal:  [ 0.82006074 -0.57227649  0.91131588] distance:  1.3529584748095922\n",
      "\n",
      "Episode: 37, actor_loss: -1.611, critic_loss: 9.802, mean_reward: -1226.643, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  100\n",
      "mb subgoal:  [ 0.23893584 -0.97059155  4.557574  ]  distance:  4.665899397150223\n",
      "sample final subgoal:  [ 0.94872336 -0.31610757  3.47490755] distance:  3.6159345225303263\n",
      "\n",
      "Episode: 38, actor_loss: 0.535, critic_loss: 11.382, mean_reward: -1165.030, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  140\n",
      "mb subgoal:  [ 0.19938594 -0.9766894   4.438651  ]  distance:  4.549208765015355\n",
      "sample final subgoal:  [-0.97228244  0.23380945  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 39, actor_loss: 6.815, critic_loss: 23.476, mean_reward: -1261.013, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  127\n",
      "mb subgoal:  [ 0.20244268 -0.97921526  4.4352193 ]  distance:  4.546538870656381\n",
      "sample final subgoal:  [-0.98935898 -0.14549503 -0.54447624] distance:  1.1386195046829288\n",
      "\n",
      "Episode: 40, actor_loss: -1.582, critic_loss: 10.226, mean_reward: -1349.814, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  15\n",
      "mb subgoal:  [ 0.19640619 -0.9740272   4.4135203 ]  distance:  4.523987860917308\n",
      "sample final subgoal:  [ 0.59199367 -0.80594261  4.3158083 ] distance:  4.430146871489597\n",
      "\n",
      "Episode: 41, actor_loss: -1.248, critic_loss: 17.843, mean_reward: -1257.464, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  20\n",
      "mb subgoal:  [ 0.19554977 -0.9741488   4.426864  ]  distance:  4.536995899767198\n",
      "sample final subgoal:  [ 0.62642902 -0.77947847  0.50285771] distance:  1.119314914451429\n",
      "\n",
      "Episode: 42, actor_loss: -1.684, critic_loss: 22.805, mean_reward: -1301.547, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  31\n",
      "mb subgoal:  [-0.06128162  0.9956438  -4.07732   ]  distance:  4.197570867946541\n",
      "sample final subgoal:  [-0.99565903  0.09307573 -4.19888593] distance:  4.3163228614249975\n",
      "\n",
      "Episode: 43, actor_loss: -0.808, critic_loss: 12.990, mean_reward: -1378.814, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  49\n",
      "mb subgoal:  [-0.0655209   0.99934465 -4.069466  ]  distance:  4.190887395968723\n",
      "sample final subgoal:  [ 0.02483031 -0.99969168  4.69908982] distance:  4.804315261440589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 44, actor_loss: -1.010, critic_loss: 10.911, mean_reward: -1365.886, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  60\n",
      "mb subgoal:  [-0.0715403   0.99378455 -4.0853906 ]  distance:  4.205132795861147\n",
      "sample final subgoal:  [ 0.99557625 -0.09395711 -5.44570957] distance:  5.536763739327341\n",
      "\n",
      "Episode: 45, actor_loss: 0.302, critic_loss: 15.176, mean_reward: -1356.826, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  70\n",
      "mb subgoal:  [-0.07618105  0.99702823 -4.084574  ]  distance:  4.205189107197796\n",
      "sample final subgoal:  [ 0.99878502  0.04927967 -5.39059558] distance:  5.482565153649531\n",
      "\n",
      "Episode: 46, actor_loss: -0.236, critic_loss: 16.717, mean_reward: -1463.941, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  80\n",
      "mb subgoal:  [-0.07236557  0.9967176  -4.084405  ]  distance:  4.204883643209016\n",
      "sample final subgoal:  [-0.5967109   0.80245629 -5.67617837] distance:  5.763592703297605\n",
      "\n",
      "Episode: 47, actor_loss: 0.317, critic_loss: 16.591, mean_reward: -1445.681, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  90\n",
      "mb subgoal:  [-0.06493293  1.0045924  -4.0795765 ]  distance:  4.201947949383445\n",
      "sample final subgoal:  [0.17013724 0.98542038 5.14651693] distance:  5.242769929013508\n",
      "\n",
      "Episode: 48, actor_loss: -0.158, critic_loss: 6.787, mean_reward: -1416.297, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  100\n",
      "mb subgoal:  [-0.07070875  1.0006851  -4.0857687 ]  distance:  4.207122087068688\n",
      "sample final subgoal:  [0.80175904 0.59764743 3.0099375 ] distance:  3.1717067564216515\n",
      "\n",
      "Episode: 49, actor_loss: 4.026, critic_loss: 20.908, mean_reward: -1352.926, best_return: -977.640\n",
      "last 50 episode mean reward:  -1243.3456217648736\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  111\n",
      "mb subgoal:  [-0.07597052  0.99963254 -4.0764503 ]  distance:  4.197914264135207\n",
      "sample final subgoal:  [-0.93849186 -0.34530135  0.87117441] distance:  1.326252184053651\n",
      "\n",
      "Episode: 50, actor_loss: -0.684, critic_loss: 14.865, mean_reward: -1319.310, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  120\n",
      "mb subgoal:  [-0.07484306  0.99918354 -4.076842  ]  distance:  4.198167283311717\n",
      "sample final subgoal:  [ 0.6270797   0.7789551  -2.64993846] distance:  2.8323442286568956\n",
      "\n",
      "Episode: 51, actor_loss: 0.172, critic_loss: 15.896, mean_reward: -1369.204, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  130\n",
      "mb subgoal:  [-0.07389836  0.9957     -4.083544  ]  distance:  4.203832685006464\n",
      "sample final subgoal:  [-0.95461495  0.29784275  6.96424755] distance:  7.035676512559988\n",
      "\n",
      "Episode: 52, actor_loss: 0.382, critic_loss: 11.092, mean_reward: -1507.220, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  116\n",
      "mb subgoal:  [-0.07696196  1.0009434  -4.084227  ]  distance:  4.205796210162246\n",
      "sample final subgoal:  [-0.8621913  -0.50658282  0.63414981] distance:  1.1841224495313343\n",
      "\n",
      "Episode: 53, actor_loss: -0.439, critic_loss: 3.504, mean_reward: -1131.533, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  139\n",
      "mb subgoal:  [-0.07548386  1.0062613  -4.0760765 ]  distance:  4.199126029353511\n",
      "sample final subgoal:  [ 0.85255552  0.52263667 -5.62794722] distance:  5.716099185367878\n",
      "\n",
      "Episode: 54, actor_loss: -0.003, critic_loss: 3.530, mean_reward: -1198.744, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  134\n",
      "mb subgoal:  [-0.27524707 -0.9576414  -5.6891685 ]  distance:  5.775766246278299\n",
      "sample final subgoal:  [ 0.13932799 -0.99024629 -7.67810309] distance:  7.742949502999453\n",
      "\n",
      "Episode: 55, actor_loss: -0.048, critic_loss: 4.693, mean_reward: -1301.557, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  125\n",
      "mb subgoal:  [ 0.43030176 -0.90603817  3.9408135 ]  distance:  4.066457441768323\n",
      "sample final subgoal:  [-0.25909282 -0.96585243 -7.49685067] distance:  7.5632512808926595\n",
      "\n",
      "Episode: 56, actor_loss: -0.391, critic_loss: 3.582, mean_reward: -1401.664, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  148\n",
      "mb subgoal:  [ 0.4263069  -0.91120756  3.935404  ]  distance:  4.061950506914907\n",
      "sample final subgoal:  [ 0.55098132  0.83451757 -6.56657164] distance:  6.642278456125286\n",
      "\n",
      "Episode: 57, actor_loss: 0.426, critic_loss: 1.331, mean_reward: -1467.615, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  139\n",
      "mb subgoal:  [ 0.4222653  -0.90386647  3.942619  ]  distance:  4.066881830514247\n",
      "sample final subgoal:  [ 0.99904946  0.04359099 -5.90679337] distance:  5.99084367653243\n",
      "\n",
      "Episode: 58, actor_loss: 0.019, critic_loss: 0.868, mean_reward: -1372.457, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  165\n",
      "mb subgoal:  [ 0.43193033 -0.90713096  3.9418535 ]  distance:  4.06788146142383\n",
      "sample final subgoal:  [ 0.83477536 -0.55059069 -6.42019745] distance:  6.497609969392794\n",
      "\n",
      "Episode: 59, actor_loss: -1.849, critic_loss: 10.345, mean_reward: -1411.851, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  211\n",
      "mb subgoal:  [1.918003e-03 9.982722e-01 4.970442e+00]  distance:  5.0696985012613975\n",
      "sample final subgoal:  [-0.55579829 -0.83131719  0.91776576] distance:  1.3573113077055698\n",
      "\n",
      "Episode: 60, actor_loss: -0.520, critic_loss: 14.297, mean_reward: -1273.545, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  22\n",
      "mb subgoal:  [-8.803196e-04  9.923332e-01  4.951915e+00]  distance:  5.050364933675284\n",
      "sample final subgoal:  [-0.35784163  0.93378229  1.8327759 ] distance:  2.0878379921144075\n",
      "\n",
      "Episode: 61, actor_loss: 0.212, critic_loss: 15.689, mean_reward: -1411.069, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  36\n",
      "mb subgoal:  [0.01908045 0.9929474  4.9625373 ]  distance:  5.060937160579347\n",
      "sample final subgoal:  [-0.39116158 -0.92032202 -2.47578695] distance:  2.6701162920911994\n",
      "\n",
      "Episode: 62, actor_loss: -0.602, critic_loss: 22.226, mean_reward: -1459.317, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  52\n",
      "mb subgoal:  [0.01225071 0.99603707 4.971161  ]  distance:  5.069978352791552\n",
      "sample final subgoal:  [ 0.32444301 -0.94590525 -7.58715693] distance:  7.652774021040243\n",
      "\n",
      "Episode: 63, actor_loss: 1.039, critic_loss: 13.695, mean_reward: -1446.564, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  62\n",
      "mb subgoal:  [0.01469215 0.99288106 4.9802494 ]  distance:  5.0782785265242225\n",
      "sample final subgoal:  [ 0.94353448 -0.33127434 -6.32944301] distance:  6.407951997575024\n",
      "\n",
      "Episode: 64, actor_loss: 0.210, critic_loss: 7.382, mean_reward: -1487.534, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  77\n",
      "mb subgoal:  [0.01076206 0.998908   4.987369  ]  distance:  5.086431276130553\n",
      "sample final subgoal:  [ 1.18433876e-03 -9.99999299e-01 -3.41980927e+00] distance:  3.563017745084272\n",
      "\n",
      "Episode: 65, actor_loss: -0.738, critic_loss: 2.754, mean_reward: -1505.475, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  92\n",
      "mb subgoal:  [-0.00514865  1.0037552   4.9890404 ]  distance:  5.089015120312636\n",
      "sample final subgoal:  [ 0.6570322   0.75386251 -6.07015985] distance:  6.151978590975424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 66, actor_loss: -0.538, critic_loss: 1.380, mean_reward: -1449.716, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  107\n",
      "mb subgoal:  [-2.4756230e-03  1.0070016e+00  4.9938874e+00]  distance:  5.094405758942295\n",
      "sample final subgoal:  [ 0.91416941 -0.40533232 -6.32425721] distance:  6.402829782927688\n",
      "\n",
      "Episode: 67, actor_loss: -0.062, critic_loss: 1.258, mean_reward: -1355.011, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  125\n",
      "mb subgoal:  [-3.5356209e-03  1.0025853e+00  4.9869771e+00]  distance:  5.086760301774772\n",
      "sample final subgoal:  [-0.03862695 -0.9992537  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 68, actor_loss: -0.122, critic_loss: 1.989, mean_reward: -1407.965, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  130\n",
      "mb subgoal:  [1.0411628e-03 1.0045977e+00 4.9891000e+00]  distance:  5.089237286150769\n",
      "sample final subgoal:  [-0.68559433 -0.7279838  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 69, actor_loss: -4.231, critic_loss: 6.044, mean_reward: -1397.252, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  143\n",
      "mb subgoal:  [-7.8898296e-04  1.0033057e+00  4.9866447e+00]  distance:  5.086575341891685\n",
      "sample final subgoal:  [ 0.88637593 -0.46296621 -0.08947483] distance:  1.0039948933487812\n",
      "\n",
      "Episode: 70, actor_loss: -0.200, critic_loss: 13.387, mean_reward: -1478.559, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  156\n",
      "mb subgoal:  [3.9283298e-03 1.0061969e+00 4.9925737e+00]  distance:  5.092959854166353\n",
      "sample final subgoal:  [-0.59904415  0.800716    1.46129135] distance:  1.7706982869168204\n",
      "\n",
      "Episode: 71, actor_loss: 0.193, critic_loss: 11.041, mean_reward: -1279.221, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  167\n",
      "mb subgoal:  [4.4025481e-05 1.0021315e+00 4.9897003e+00]  distance:  5.089339517712823\n",
      "sample final subgoal:  [ 0.63555811  0.77205304 -1.29454846] distance:  1.6358043061881942\n",
      "\n",
      "Episode: 72, actor_loss: -0.116, critic_loss: 14.924, mean_reward: -1267.240, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  194\n",
      "mb subgoal:  [4.5608953e-03 1.0028273e+00 4.9902878e+00]  distance:  5.090054567838746\n",
      "sample final subgoal:  [ 0.92540834  0.3789715  -5.93276284] distance:  6.016450356792859\n",
      "\n",
      "Episode: 73, actor_loss: 0.048, critic_loss: 18.206, mean_reward: -1365.204, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  196\n",
      "mb subgoal:  [ 0.68246216 -0.728716    3.6180272 ]  distance:  3.7532522590197104\n",
      "sample final subgoal:  [ 0.30399779 -0.95267274  4.06724135] distance:  4.188371064940078\n",
      "\n",
      "Episode: 74, actor_loss: 1.064, critic_loss: 5.531, mean_reward: -1309.568, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  179\n",
      "mb subgoal:  [ 0.68506575 -0.7268405   3.6174712 ]  distance:  3.7528269607496005\n",
      "sample final subgoal:  [-0.96490737 -0.26259048 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 75, actor_loss: -0.357, critic_loss: 10.248, mean_reward: -1238.199, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  190\n",
      "mb subgoal:  [ 0.5704529  -0.82867134  3.2017167 ]  distance:  3.3560545745424246\n",
      "sample final subgoal:  [-0.2307575  -0.97301129  7.01711868] distance:  7.088014853152717\n",
      "\n",
      "Episode: 76, actor_loss: 0.172, critic_loss: 3.345, mean_reward: -1228.254, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  203\n",
      "mb subgoal:  [ 0.53616154 -0.84973466  3.1954994 ]  distance:  3.349736518366238\n",
      "sample final subgoal:  [-0.73927107 -0.67340797  7.93931302] distance:  8.002042939392496\n",
      "\n",
      "Episode: 77, actor_loss: -0.123, critic_loss: 5.128, mean_reward: -1200.828, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  215\n",
      "mb subgoal:  [ 0.536682  -0.8501199  3.1978285]  distance:  3.352139424339337\n",
      "sample final subgoal:  [-0.76100636 -0.64874442 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 78, actor_loss: 0.017, critic_loss: 3.212, mean_reward: -1127.003, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  244\n",
      "mb subgoal:  [ 0.5319048 -0.852758   3.1982665]  distance:  3.3524658937450824\n",
      "sample final subgoal:  [-0.58148213 -0.81355917 -7.76872131] distance:  7.832817549249609\n",
      "\n",
      "Episode: 79, actor_loss: -0.750, critic_loss: 14.326, mean_reward: -1114.840, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  277\n",
      "mb subgoal:  [ 0.5302321  -0.85183954  3.1985257 ]  distance:  3.352214655970079\n",
      "sample final subgoal:  [ 0.19010608 -0.98176356  0.28193758] distance:  1.0389845050871354\n",
      "\n",
      "Episode: 80, actor_loss: -0.398, critic_loss: 11.572, mean_reward: -1082.662, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  32\n",
      "mb subgoal:  [ 0.5385541  -0.87369955  3.193671  ]  distance:  3.354538085256441\n",
      "sample final subgoal:  [ 0.72900955  0.68450353 -0.38379012] distance:  1.0711185060043036\n",
      "\n",
      "Episode: 81, actor_loss: -0.210, critic_loss: 10.218, mean_reward: -1127.636, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  49\n",
      "mb subgoal:  [ 0.53059554 -0.8643627   3.1892433 ]  distance:  3.3466292694151387\n",
      "sample final subgoal:  [-0.91951517 -0.39305452  2.45086001] distance:  2.647019985489108\n",
      "\n",
      "Episode: 82, actor_loss: 0.461, critic_loss: 6.139, mean_reward: -1100.888, best_return: -977.640\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  63\n",
      "mb subgoal:  [ 0.5291952 -0.8445994  3.199447 ]  distance:  3.3510977769424235\n",
      "sample final subgoal:  [-0.24523532  0.96946358  6.95175665] distance:  7.023312642064876\n",
      "\n",
      "Episode: 83, actor_loss: 1.217, critic_loss: 10.184, mean_reward: -953.591, best_return: -953.591\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  82\n",
      "mb subgoal:  [ 0.5291494 -0.8502966  3.2047408]  distance:  3.357583475313126\n",
      "sample final subgoal:  [ 0.52860067 -0.84887062 -2.38618115] distance:  2.5872495972485936\n",
      "\n",
      "Episode: 84, actor_loss: -1.549, critic_loss: 5.304, mean_reward: -991.177, best_return: -953.591\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  111\n",
      "mb subgoal:  [ 0.51852995 -0.84913224  3.2004983 ]  distance:  3.35158000229923\n",
      "sample final subgoal:  [-0.35572554  0.93459047 -6.91805039] distance:  6.989951444434414\n",
      "\n",
      "Episode: 85, actor_loss: -0.014, critic_loss: 3.227, mean_reward: -1027.024, best_return: -953.591\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  125\n",
      "mb subgoal:  [ 0.5231107  -0.84775084  3.2026143 ]  distance:  3.3539625347768074\n",
      "sample final subgoal:  [-0.95311217 -0.30261722 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 86, actor_loss: 1.186, critic_loss: 4.298, mean_reward: -1132.503, best_return: -953.591\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  142\n",
      "mb subgoal:  [ 0.52627766 -0.84432167  3.205352  ]  distance:  3.3562075525222017\n",
      "sample final subgoal:  [-0.77300336  0.63440192 -7.60800142] distance:  7.673440268146298\n",
      "\n",
      "Episode: 87, actor_loss: -0.210, critic_loss: 2.151, mean_reward: -1003.373, best_return: -953.591\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  152\n",
      "mb subgoal:  [ 0.52506804 -0.8513961   3.200047  ]  distance:  3.3527410649527662\n",
      "sample final subgoal:  [-0.9994688  -0.03259024 -7.207788  ] distance:  7.276826767619679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 88, actor_loss: 0.219, critic_loss: 3.633, mean_reward: -865.391, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  168\n",
      "mb subgoal:  [ 0.52831274 -0.85099924  3.2008579 ]  distance:  3.3539238523554817\n",
      "sample final subgoal:  [ 0.85123042 -0.52479213 -2.45446458] distance:  2.6503577777542127\n",
      "\n",
      "Episode: 89, actor_loss: 5.013, critic_loss: 10.402, mean_reward: -953.510, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  215\n",
      "mb subgoal:  [ 0.52242893 -0.84778124  3.2021146 ]  distance:  3.3533867680064167\n",
      "sample final subgoal:  [-0.74572596 -0.6662528   0.50418355] distance:  1.1199111817489713\n",
      "\n",
      "Episode: 90, actor_loss: -2.145, critic_loss: 5.173, mean_reward: -963.042, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  214\n",
      "mb subgoal:  [ 0.51709867 -0.8539332   3.1996946 ]  distance:  3.3518112585671442\n",
      "sample final subgoal:  [ 0.55254969  0.83347996 -4.27158484] distance:  4.387076134498314\n",
      "\n",
      "Episode: 91, actor_loss: -0.609, critic_loss: 4.362, mean_reward: -997.441, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  253\n",
      "mb subgoal:  [0.253052   0.96549547 4.427394  ]  distance:  4.538505665216798\n",
      "sample final subgoal:  [-0.99835222 -0.05738325 -5.08816661] distance:  5.185502814734069\n",
      "\n",
      "Episode: 92, actor_loss: 0.260, critic_loss: 6.721, mean_reward: -1129.568, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  284\n",
      "mb subgoal:  [0.2546353  0.96771145 4.428867  ]  distance:  4.540502866251457\n",
      "sample final subgoal:  [-0.01905646  0.99981841  5.69663046] distance:  5.783735696291973\n",
      "\n",
      "Episode: 93, actor_loss: -0.963, critic_loss: 3.249, mean_reward: -1001.909, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  331\n",
      "mb subgoal:  [0.24684852 0.9713706  4.4242306 ]  distance:  4.536332348801666\n",
      "sample final subgoal:  [ 0.08027287 -0.99677293  5.85626471] distance:  5.941029904431366\n",
      "\n",
      "Episode: 94, actor_loss: 0.835, critic_loss: 3.962, mean_reward: -906.936, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  323\n",
      "mb subgoal:  [0.2484898 0.970988  4.4232364]  distance:  4.535370416369827\n",
      "sample final subgoal:  [ 0.01136294 -0.99993544  6.61727834] distance:  6.6924115669493105\n",
      "\n",
      "Episode: 95, actor_loss: 1.847, critic_loss: 4.329, mean_reward: -977.671, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  326\n",
      "mb subgoal:  [0.24872845 0.9711517  4.4250083 ]  distance:  4.537146671101007\n",
      "sample final subgoal:  [ 0.90723606 -0.42062184  1.79555996] distance:  2.0552458620521405\n",
      "\n",
      "Episode: 96, actor_loss: 1.104, critic_loss: 5.999, mean_reward: -1053.014, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  328\n",
      "mb subgoal:  [0.24645984 0.9721607  4.4264445 ]  distance:  4.538639670190027\n",
      "sample final subgoal:  [ 0.88871468 -0.4584607   0.09718941] distance:  1.0047117900823264\n",
      "\n",
      "Episode: 97, actor_loss: -0.649, critic_loss: 3.076, mean_reward: -1009.169, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  289\n",
      "mb subgoal:  [ 0.43733403 -0.901669    2.7741334 ]  distance:  2.949590556356485\n",
      "sample final subgoal:  [0.73986379 0.6727567  4.39337386] distance:  4.505744542617198\n",
      "\n",
      "Episode: 98, actor_loss: 1.267, critic_loss: 4.779, mean_reward: -968.898, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  324\n",
      "mb subgoal:  [ 0.43657503 -0.90064687  2.772194  ]  distance:  2.947341445475218\n",
      "sample final subgoal:  [-0.89465864  0.4467504   7.16280126] distance:  7.2322694844870306\n",
      "\n",
      "Episode: 99, actor_loss: 1.752, critic_loss: 10.202, mean_reward: -957.612, best_return: -865.391\n",
      "last 50 episode mean reward:  -1209.7306780034573\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  340\n",
      "mb subgoal:  [ 0.43891966 -0.90324205  2.7712333 ]  distance:  2.947580497613825\n",
      "sample final subgoal:  [ 0.25999705  0.96560941 -0.05818101] distance:  1.0016910848591727\n",
      "\n",
      "Episode: 100, actor_loss: -1.688, critic_loss: 6.888, mean_reward: -1071.481, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  33\n",
      "mb subgoal:  [ 0.42308134 -0.915475    2.7521112 ]  distance:  2.9310763129671997\n",
      "sample final subgoal:  [ 0.37326766 -0.92772369  0.06555035] distance:  1.0021461214851606\n",
      "\n",
      "Episode: 101, actor_loss: -1.246, critic_loss: 2.887, mean_reward: -1114.521, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  51\n",
      "mb subgoal:  [ 0.43883198 -0.9137774   2.767787  ]  distance:  2.9475765355187984\n",
      "sample final subgoal:  [-0.07277267 -0.99734855  6.32935396] distance:  6.407864038349246\n",
      "\n",
      "Episode: 102, actor_loss: -0.801, critic_loss: 2.280, mean_reward: -1109.324, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  74\n",
      "mb subgoal:  [ 0.43158692 -0.9082702   2.7672677 ]  distance:  2.9443152916267907\n",
      "sample final subgoal:  [-0.93685468  0.34971889 -7.27862801] distance:  7.347001135416825\n",
      "\n",
      "Episode: 103, actor_loss: 0.265, critic_loss: 3.706, mean_reward: -1003.311, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  83\n",
      "mb subgoal:  [ 0.43517226 -0.90066946  2.7677546 ]  distance:  2.9429654498751554\n",
      "sample final subgoal:  [ 0.42101683  0.90705282 -5.08375935] distance:  5.1811783538519816\n",
      "\n",
      "Episode: 104, actor_loss: 0.937, critic_loss: 8.135, mean_reward: -1069.651, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  102\n",
      "mb subgoal:  [ 0.43502593 -0.9072298   2.7789414 ]  distance:  2.955474361434453\n",
      "sample final subgoal:  [-0.04732263 -0.99887966 -5.95149279] distance:  6.034920584157871\n",
      "\n",
      "Episode: 105, actor_loss: -1.809, critic_loss: 3.583, mean_reward: -870.757, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  107\n",
      "mb subgoal:  [ 0.43856227 -0.90037596  2.7719681 ]  distance:  2.947341350229931\n",
      "sample final subgoal:  [-0.50288952  0.8643507  -7.80885708] distance:  7.872626552494383\n",
      "\n",
      "Episode: 106, actor_loss: 0.477, critic_loss: 7.322, mean_reward: -1036.758, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  113\n",
      "mb subgoal:  [ 0.43553358 -0.8937518  -3.583042  ]  distance:  3.718423198734094\n",
      "sample final subgoal:  [ 0.20469867 -0.97882504 -4.18300057] distance:  4.30087127607412\n",
      "\n",
      "Episode: 107, actor_loss: -1.494, critic_loss: 17.671, mean_reward: -892.933, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  118\n",
      "mb subgoal:  [ 0.4384308 -0.9017876 -3.5825055]  distance:  3.7201865269813132\n",
      "sample final subgoal:  [0.7777753  0.62854242 1.77934726] distance:  2.0410969270271866\n",
      "\n",
      "Episode: 108, actor_loss: -1.695, critic_loss: 9.830, mean_reward: -881.840, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  124\n",
      "mb subgoal:  [ 0.4289962 -0.8952268 -3.5725503]  distance:  3.7079083522622787\n",
      "sample final subgoal:  [ 0.19976579 -0.97984368  6.18184574] distance:  6.2622054259525655\n",
      "\n",
      "Episode: 109, actor_loss: 7.200, critic_loss: 14.283, mean_reward: -916.911, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  127\n",
      "mb subgoal:  [ 0.43594474 -0.89918554 -3.5743341 ]  distance:  3.7113942154291695\n",
      "sample final subgoal:  [ 0.78150401 -0.62390021 -0.07456839] distance:  1.0027763684069817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 110, actor_loss: -2.521, critic_loss: 11.923, mean_reward: -952.375, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  139\n",
      "mb subgoal:  [ 0.43908697 -0.9028258  -3.5766177 ]  distance:  3.7148467054887457\n",
      "sample final subgoal:  [ 0.12222425  0.99250251 -0.89378601] distance:  1.3412134169137233\n",
      "\n",
      "Episode: 111, actor_loss: -1.575, critic_loss: 7.951, mean_reward: -911.577, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  160\n",
      "mb subgoal:  [ 0.7548156  -0.65367186  2.6644325 ]  distance:  2.845388223531496\n",
      "sample final subgoal:  [-0.99510534 -0.09881984  4.4816046 ] distance:  4.591816607616492\n",
      "\n",
      "Episode: 112, actor_loss: -1.446, critic_loss: 3.201, mean_reward: -876.324, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  185\n",
      "mb subgoal:  [ 0.75209796 -0.6518128   2.6612768 ]  distance:  2.841285899356931\n",
      "sample final subgoal:  [-0.98856734  0.15078002 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 113, actor_loss: 0.500, critic_loss: 4.209, mean_reward: -938.359, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  197\n",
      "mb subgoal:  [ 0.756988  -0.6545752  2.6627808]  distance:  2.8446266768080966\n",
      "sample final subgoal:  [-0.72573213 -0.68797738 -5.59085629] distance:  5.679583970990279\n",
      "\n",
      "Episode: 114, actor_loss: 0.716, critic_loss: 8.385, mean_reward: -953.749, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  210\n",
      "mb subgoal:  [ 0.7551936 -0.6579641  2.6563575]  distance:  2.8389204710034948\n",
      "sample final subgoal:  [ 0.8742455  -0.48548409 -1.8805201 ] distance:  2.1298722572682585\n",
      "\n",
      "Episode: 115, actor_loss: -0.437, critic_loss: 4.172, mean_reward: -1083.064, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  221\n",
      "mb subgoal:  [ 0.7560842 -0.65114    2.66183  ]  distance:  2.8427073827538547\n",
      "sample final subgoal:  [-0.92043155 -0.39090378 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 116, actor_loss: 2.142, critic_loss: 7.443, mean_reward: -1085.843, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  235\n",
      "mb subgoal:  [ 0.7508086  -0.65135354  2.6687932 ]  distance:  2.8478820432638563\n",
      "sample final subgoal:  [ 0.36017778 -0.93288368  5.33391559] distance:  5.4268458196928036\n",
      "\n",
      "Episode: 117, actor_loss: 0.835, critic_loss: 13.936, mean_reward: -1085.425, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  230\n",
      "mb subgoal:  [ 0.7565265 -0.6585856  2.6643565]  distance:  2.846904071645738\n",
      "sample final subgoal:  [ 0.9555267  -0.29490462 -4.03382692] distance:  4.155930652813156\n",
      "\n",
      "Episode: 118, actor_loss: 0.676, critic_loss: 5.222, mean_reward: -1090.750, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  223\n",
      "mb subgoal:  [ 0.7542477 -0.6551783  2.6619654]  distance:  2.8432741476372843\n",
      "sample final subgoal:  [ 0.34103296  0.94005134 -6.14043439] distance:  6.221328999403259\n",
      "\n",
      "Episode: 119, actor_loss: 2.663, critic_loss: 11.237, mean_reward: -1061.944, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  231\n",
      "mb subgoal:  [ 0.75249684 -0.65609413  2.663347  ]  distance:  2.8443150806606567\n",
      "sample final subgoal:  [ 0.97087101 -0.23960274 -0.19572016] distance:  1.0189732002296685\n",
      "\n",
      "Episode: 120, actor_loss: -0.916, critic_loss: 4.689, mean_reward: -1090.929, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  21\n",
      "mb subgoal:  [ 0.73969996 -0.6830996   2.6973002 ]  distance:  2.879098726552416\n",
      "sample final subgoal:  [ 0.66956241 -0.7427558   0.20081942] distance:  1.0199649208150252\n",
      "\n",
      "Episode: 121, actor_loss: -0.398, critic_loss: 6.240, mean_reward: -1219.193, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  31\n",
      "mb subgoal:  [ 0.7560468 -0.6451801  2.6819093]  distance:  2.8601576366063153\n",
      "sample final subgoal:  [-0.97884428 -0.20460662 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 122, actor_loss: 0.552, critic_loss: 3.936, mean_reward: -1136.828, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  45\n",
      "mb subgoal:  [ 0.7567254 -0.6554301  2.6671371]  distance:  2.8488317658827493\n",
      "sample final subgoal:  [ 0.32285961  0.94644687 -6.13894116] distance:  6.219855185070242\n",
      "\n",
      "Episode: 123, actor_loss: -0.052, critic_loss: 3.755, mean_reward: -1222.101, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  52\n",
      "mb subgoal:  [ 0.76180124 -0.6566811   2.6632388 ]  distance:  2.846824887574739\n",
      "sample final subgoal:  [ 0.82937066  0.55869876 -5.11118592] distance:  5.2080919242343935\n",
      "\n",
      "Episode: 124, actor_loss: 0.105, critic_loss: 7.369, mean_reward: -1108.537, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  74\n",
      "mb subgoal:  [ 0.75435823 -0.64997196  2.669002  ]  distance:  2.848700031444982\n",
      "sample final subgoal:  [ 0.97250761  0.23287109 -4.5939719 ] distance:  4.7015505771297414\n",
      "\n",
      "Episode: 125, actor_loss: -0.919, critic_loss: 17.478, mean_reward: -981.509, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  86\n",
      "mb subgoal:  [ 0.7550847  -0.65478295  2.6550827 ]  distance:  2.8369627711438987\n",
      "sample final subgoal:  [-0.90851809 -0.41784553  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 126, actor_loss: 0.446, critic_loss: 19.188, mean_reward: -914.647, best_return: -865.391\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  106\n",
      "mb subgoal:  [ 0.7550919  -0.65112597  2.6597962 ]  distance:  2.840536012593206\n",
      "sample final subgoal:  [-0.965927   -0.25881467 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 127, actor_loss: -1.890, critic_loss: 13.463, mean_reward: -823.191, best_return: -823.191\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  134\n",
      "mb subgoal:  [ 0.8257261  -0.56084317  2.4776871 ]  distance:  2.671198627311477\n",
      "sample final subgoal:  [ 0.94990794 -0.31252983 -0.85923834] distance:  1.3184424648784614\n",
      "\n",
      "Episode: 128, actor_loss: 1.272, critic_loss: 11.239, mean_reward: -827.807, best_return: -823.191\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  175\n",
      "mb subgoal:  [ 0.82750803 -0.55748177  2.4757442 ]  distance:  2.669244282828535\n",
      "sample final subgoal:  [ 0.99747596  0.07100504 -0.66011025] distance:  1.1982259989539392\n",
      "\n",
      "Episode: 129, actor_loss: 5.651, critic_loss: 14.311, mean_reward: -934.284, best_return: -823.191\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  183\n",
      "mb subgoal:  [ 0.82815605 -0.55668116  2.4777665 ]  distance:  2.6711539185268967\n",
      "sample final subgoal:  [-0.14844427  0.98892078 -0.21094283] distance:  1.0220062993341508\n",
      "\n",
      "Episode: 130, actor_loss: -1.547, critic_loss: 12.398, mean_reward: -841.288, best_return: -823.191\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  191\n",
      "mb subgoal:  [ 0.8306253 -0.5563234  2.4784334]  distance:  2.6724644225324203\n",
      "sample final subgoal:  [ 0.74481638  0.66726948 -1.81378841] distance:  2.071190095895416\n",
      "\n",
      "Episode: 131, actor_loss: -2.511, critic_loss: 5.118, mean_reward: -987.361, best_return: -823.191\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  231\n",
      "mb subgoal:  [ 0.8298395 -0.55647    2.4710639]  distance:  2.6654172354373156\n",
      "sample final subgoal:  [ 0.60218671 -0.79835528 -0.69692419] distance:  1.218894304983832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 132, actor_loss: 0.539, critic_loss: 5.489, mean_reward: -794.139, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  250\n",
      "mb subgoal:  [ 0.8310774  -0.55416656  2.4759328 ]  distance:  2.669837751949929\n",
      "sample final subgoal:  [ 0.46660874 -0.88446384  4.9848022 ] distance:  5.084117717494321\n",
      "\n",
      "Episode: 133, actor_loss: 1.330, critic_loss: 5.890, mean_reward: -874.101, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  282\n",
      "mb subgoal:  [ 0.8260539  -0.55771184  2.4741147 ]  distance:  2.667330295226366\n",
      "sample final subgoal:  [ 0.69487641 -0.71912917 -0.96842854] distance:  1.3920681866512714\n",
      "\n",
      "Episode: 134, actor_loss: -0.859, critic_loss: 6.651, mean_reward: -918.820, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  300\n",
      "mb subgoal:  [ 0.82968074 -0.5561169   2.471247  ]  distance:  2.6654638697860475\n",
      "sample final subgoal:  [0.93126444 0.36434398 1.67315841] distance:  1.949220114416535\n",
      "\n",
      "Episode: 135, actor_loss: 0.813, critic_loss: 5.733, mean_reward: -920.903, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  303\n",
      "mb subgoal:  [ 0.8296666  -0.55516267  2.473729  ]  distance:  2.667561981725811\n",
      "sample final subgoal:  [-0.67608601  0.73682271 -7.91602075] distance:  7.9789337943555845\n",
      "\n",
      "Episode: 136, actor_loss: 1.371, critic_loss: 5.151, mean_reward: -1038.496, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  321\n",
      "mb subgoal:  [ 0.83036625 -0.5561084   2.4756021 ]  distance:  2.6697135969642423\n",
      "sample final subgoal:  [ 0.23086192  0.97298652 -5.95674433] distance:  6.040099586828215\n",
      "\n",
      "Episode: 137, actor_loss: -0.310, critic_loss: 3.939, mean_reward: -905.700, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  359\n",
      "mb subgoal:  [ 0.832392  -0.5566936  2.4714155]  distance:  2.6665856172865583\n",
      "sample final subgoal:  [ 0.98998208 -0.14119309 -0.37944072] distance:  1.0695677914670865\n",
      "\n",
      "Episode: 138, actor_loss: 1.123, critic_loss: 4.862, mean_reward: -947.648, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  377\n",
      "mb subgoal:  [ 0.8311735  -0.55544126  2.4734192 ]  distance:  2.667801872995741\n",
      "sample final subgoal:  [-0.8167705   0.57696269  7.16147095] distance:  7.230951959095746\n",
      "\n",
      "Episode: 139, actor_loss: 4.815, critic_loss: 9.260, mean_reward: -944.321, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  417\n",
      "mb subgoal:  [ 0.83046037 -0.55439264  2.473128  ]  distance:  2.6670916973387686\n",
      "sample final subgoal:  [0.7226659  0.69119751 0.41795711] distance:  1.0838303115964338\n",
      "\n",
      "Episode: 140, actor_loss: -2.493, critic_loss: 12.982, mean_reward: -863.859, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  37\n",
      "mb subgoal:  [ 0.831965   -0.54648197  2.472719  ]  distance:  2.6655482317904835\n",
      "sample final subgoal:  [ 0.47358805 -0.88074648  3.25061226] distance:  3.4009528139102927\n",
      "\n",
      "Episode: 141, actor_loss: -1.522, critic_loss: 4.852, mean_reward: -921.566, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  60\n",
      "mb subgoal:  [ 0.84084547 -0.54783124  2.4825606 ]  distance:  2.6777317763791713\n",
      "sample final subgoal:  [-0.81832158 -0.57476064  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 142, actor_loss: 1.779, critic_loss: 7.054, mean_reward: -897.662, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  81\n",
      "mb subgoal:  [ 0.8255756 -0.556958   2.4698772]  distance:  2.6630942316160437\n",
      "sample final subgoal:  [-0.11476461 -0.99339271 -4.01667664] distance:  4.139286317517281\n",
      "\n",
      "Episode: 143, actor_loss: 0.097, critic_loss: 8.037, mean_reward: -918.421, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  101\n",
      "mb subgoal:  [ 0.8315833 -0.5575548  2.4635804]  distance:  2.659252976575467\n",
      "sample final subgoal:  [0.95335114 0.30186357 3.00114495] distance:  3.1633638723787243\n",
      "\n",
      "Episode: 144, actor_loss: -0.178, critic_loss: 2.763, mean_reward: -902.590, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  122\n",
      "mb subgoal:  [ 0.8346869  -0.55805886  2.4709818 ]  distance:  2.6671863647276637\n",
      "sample final subgoal:  [ 0.82145445 -0.57027414  3.38362004] distance:  3.528297689871002\n",
      "\n",
      "Episode: 145, actor_loss: 0.900, critic_loss: 2.620, mean_reward: -815.439, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  129\n",
      "mb subgoal:  [ 0.82626575 -0.5482109   2.4721596 ]  distance:  2.6636109911327734\n",
      "sample final subgoal:  [ 0.94853206  0.31668112 -0.9104539 ] distance:  1.3523780207847402\n",
      "\n",
      "Episode: 146, actor_loss: -0.351, critic_loss: 4.688, mean_reward: -844.749, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  149\n",
      "mb subgoal:  [ 0.8247871 -0.5591757  2.465884 ]  distance:  2.6596118098965857\n",
      "sample final subgoal:  [-0.89874197  0.43847789  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 147, actor_loss: 0.320, critic_loss: 9.860, mean_reward: -843.499, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  160\n",
      "mb subgoal:  [ 0.8327037  -0.54985726  2.474979  ]  distance:  2.668568744904148\n",
      "sample final subgoal:  [ 0.95902815 -0.2833108   0.2038629 ] distance:  1.0205685092720316\n",
      "\n",
      "Episode: 148, actor_loss: 1.679, critic_loss: 8.925, mean_reward: -869.115, best_return: -794.139\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  168\n",
      "mb subgoal:  [ 0.6096703  -0.79455394  1.729958  ]  distance:  1.998941892320296\n",
      "sample final subgoal:  [0.24188835 0.97030409 4.48810454] distance:  4.598160760252113\n",
      "\n",
      "Episode: 149, actor_loss: 2.614, critic_loss: 13.465, mean_reward: -786.180, best_return: -786.180\n",
      "last 50 episode mean reward:  -962.0356524933903\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  195\n",
      "mb subgoal:  [ 0.60862154 -0.787518    1.7404381 ]  distance:  2.0049263347281343\n",
      "sample final subgoal:  [ 0.43157431  0.90207739 -0.16139624] distance:  1.012940643028911\n",
      "\n",
      "Episode: 150, actor_loss: -1.731, critic_loss: 9.663, mean_reward: -879.303, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  181\n",
      "mb subgoal:  [ 0.6125574 -0.789688   1.7406696]  distance:  2.0071782157999967\n",
      "sample final subgoal:  [-0.20379142 -0.97901433 -1.17893378] distance:  1.5459252463258544\n",
      "\n",
      "Episode: 151, actor_loss: -1.996, critic_loss: 8.779, mean_reward: -907.408, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  220\n",
      "mb subgoal:  [ 0.61014974 -0.7936541   1.7317746 ]  distance:  2.0003031441091226\n",
      "sample final subgoal:  [ 0.58610547 -0.81023476 -2.44521208] distance:  2.6417914570130705\n",
      "\n",
      "Episode: 152, actor_loss: 1.145, critic_loss: 7.847, mean_reward: -946.590, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  249\n",
      "mb subgoal:  [ 0.61430854 -0.79393345  1.7348512 ]  distance:  2.004348804761775\n",
      "sample final subgoal:  [0.10312768 0.99466813 4.59252353] distance:  4.700135355372545\n",
      "\n",
      "Episode: 153, actor_loss: 0.867, critic_loss: 9.126, mean_reward: -943.278, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  266\n",
      "mb subgoal:  [ 0.61247456 -0.7918164   1.7375968 ]  distance:  2.005328099488072\n",
      "sample final subgoal:  [0.85093445 0.52527189 1.22236213] distance:  1.5792938842558122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 154, actor_loss: -0.944, critic_loss: 4.699, mean_reward: -998.367, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  268\n",
      "mb subgoal:  [ 0.6113753  -0.79521686  1.7383062 ]  distance:  2.0069523905138773\n",
      "sample final subgoal:  [ 0.92852757 -0.37126344 -0.34801564] distance:  1.058827126698428\n",
      "\n",
      "Episode: 155, actor_loss: -0.784, critic_loss: 3.372, mean_reward: -904.978, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  332\n",
      "mb subgoal:  [ 0.61372375 -0.78674096  1.7361351 ]  distance:  2.002444345344962\n",
      "sample final subgoal:  [-0.89920153  0.43753469  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 156, actor_loss: 1.440, critic_loss: 5.691, mean_reward: -892.330, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  368\n",
      "mb subgoal:  [ 0.60933286 -0.79170257  1.7354649 ]  distance:  2.0024779688724172\n",
      "sample final subgoal:  [-0.96948872  0.24513593  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 157, actor_loss: 0.854, critic_loss: 2.108, mean_reward: -949.029, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  407\n",
      "mb subgoal:  [ 0.6112596  -0.78725266  1.7377869 ]  distance:  2.003324313094582\n",
      "sample final subgoal:  [0.96195541 0.2732065  3.59114168] distance:  3.7277739438890687\n",
      "\n",
      "Episode: 158, actor_loss: -0.635, critic_loss: 3.607, mean_reward: -857.198, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  450\n",
      "mb subgoal:  [ 0.6146229 -0.7901017  1.7384274]  distance:  2.006028871320833\n",
      "sample final subgoal:  [-0.06583658  0.99783042  5.15505711] distance:  5.251153566327762\n",
      "\n",
      "Episode: 159, actor_loss: 3.193, critic_loss: 8.133, mean_reward: -874.862, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  493\n",
      "mb subgoal:  [ 0.61396277 -0.79216355  1.7348188 ]  distance:  2.003514337017103\n",
      "sample final subgoal:  [ 0.8234664  -0.56736504  0.48726523] distance:  1.112397142669691\n",
      "\n",
      "Episode: 160, actor_loss: -2.454, critic_loss: 10.177, mean_reward: -871.951, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  35\n",
      "mb subgoal:  [ 0.6212877  -0.78760684  1.7469463 ]  distance:  2.0144836644805313\n",
      "sample final subgoal:  [-0.98754913 -0.15731086 -0.19002781] distance:  1.0178951653140311\n",
      "\n",
      "Episode: 161, actor_loss: -0.487, critic_loss: 7.669, mean_reward: -987.182, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  59\n",
      "mb subgoal:  [ 0.60962445 -0.78323925  1.7450329 ]  distance:  2.007547141392805\n",
      "sample final subgoal:  [0.08366792 0.99649369 4.3422486 ] distance:  4.455908759899423\n",
      "\n",
      "Episode: 162, actor_loss: -0.214, critic_loss: 5.449, mean_reward: -910.723, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  83\n",
      "mb subgoal:  [ 0.6217077 -0.7933892  1.7343773]  distance:  2.0060038281314645\n",
      "sample final subgoal:  [-0.92460943  0.38091652 -2.9256921 ] distance:  3.0918722902240763\n",
      "\n",
      "Episode: 163, actor_loss: 0.564, critic_loss: 4.434, mean_reward: -942.627, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  105\n",
      "mb subgoal:  [ 0.6098482 -0.7961037  1.7372212]  distance:  2.005899697201939\n",
      "sample final subgoal:  [ 0.67320551 -0.73945544 -2.74558563] distance:  2.9220267680604284\n",
      "\n",
      "Episode: 164, actor_loss: -1.739, critic_loss: 4.559, mean_reward: -950.835, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  129\n",
      "mb subgoal:  [0.61860186 0.7887537  2.927091  ]  distance:  3.09397182908641\n",
      "sample final subgoal:  [-0.47391975  0.88056804  6.32803488] distance:  6.406561122959854\n",
      "\n",
      "Episode: 165, actor_loss: 1.152, critic_loss: 9.907, mean_reward: -876.916, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  159\n",
      "mb subgoal:  [0.6200513  0.78907794 2.9219809 ]  distance:  3.08951125100693\n",
      "sample final subgoal:  [-0.14263473  0.9897754   5.7853805 ] distance:  5.871169183287208\n",
      "\n",
      "Episode: 166, actor_loss: 0.870, critic_loss: 10.500, mean_reward: -905.790, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  183\n",
      "mb subgoal:  [-0.5858586   0.80735976 -1.4851867 ]  distance:  1.7890890352329833\n",
      "sample final subgoal:  [0.99997211 0.00746902 1.32272326] distance:  1.6581908265476752\n",
      "\n",
      "Episode: 167, actor_loss: 2.098, critic_loss: 4.750, mean_reward: -873.695, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  206\n",
      "mb subgoal:  [-0.5895756  0.8096528 -1.4878982]  distance:  1.79359365087317\n",
      "sample final subgoal:  [ 0.93728305 -0.3485692   2.48188802] distance:  2.67577431164304\n",
      "\n",
      "Episode: 168, actor_loss: 1.151, critic_loss: 5.594, mean_reward: -808.392, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  230\n",
      "mb subgoal:  [-0.58231276  0.81072044 -1.4878223 ]  distance:  1.791639185423424\n",
      "sample final subgoal:  [ 0.89987288 -0.43615226 -1.3039028 ] distance:  1.6432171195304461\n",
      "\n",
      "Episode: 169, actor_loss: -0.275, critic_loss: 11.955, mean_reward: -868.339, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  255\n",
      "mb subgoal:  [-0.57923543  0.8147941  -1.4844447 ]  distance:  1.7896869363861323\n",
      "sample final subgoal:  [-0.94485754  0.32748166  0.47364433] distance:  1.1064985077829805\n",
      "\n",
      "Episode: 170, actor_loss: -1.944, critic_loss: 8.990, mean_reward: -800.874, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  289\n",
      "mb subgoal:  [-0.58316255  0.8113611  -1.4896644 ]  distance:  1.7937350636105536\n",
      "sample final subgoal:  [-0.54273922  0.83990127 -7.75544325] distance:  7.819648331610965\n",
      "\n",
      "Episode: 171, actor_loss: 1.100, critic_loss: 9.411, mean_reward: -836.680, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  302\n",
      "mb subgoal:  [-0.5819429  0.8131982 -1.4823978]  distance:  1.7881420821613874\n",
      "sample final subgoal:  [ 0.64091224 -0.76761416 -2.06310235] distance:  2.2926821210775064\n",
      "\n",
      "Episode: 172, actor_loss: 0.117, critic_loss: 4.504, mean_reward: -887.995, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  339\n",
      "mb subgoal:  [-0.5865139   0.81573784 -1.4854306 ]  distance:  1.7933016532078152\n",
      "sample final subgoal:  [0.40069212 0.91621276 3.47560717] distance:  3.6166068594306506\n",
      "\n",
      "Episode: 173, actor_loss: -0.597, critic_loss: 4.100, mean_reward: -849.022, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  349\n",
      "mb subgoal:  [-0.5830395  0.8127754 -1.4854888]  distance:  1.7908701195279648\n",
      "sample final subgoal:  [ 0.68013992  0.73308233 -4.00416297] distance:  4.127144420058665\n",
      "\n",
      "Episode: 174, actor_loss: 0.591, critic_loss: 4.657, mean_reward: -857.587, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  374\n",
      "mb subgoal:  [-0.5848038  0.8129798 -1.4865727]  distance:  1.7924369471756436\n",
      "sample final subgoal:  [ 0.99877451  0.04949229 -1.43099175] distance:  1.7457770181791257\n",
      "\n",
      "Episode: 175, actor_loss: 0.322, critic_loss: 1.758, mean_reward: -861.714, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  401\n",
      "mb subgoal:  [-0.58237535  0.8104191  -1.4874524 ]  distance:  1.7912159950769353\n",
      "sample final subgoal:  [-0.54785462 -0.83657356 -6.43104299] distance:  6.508326513015054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 176, actor_loss: -0.182, critic_loss: 3.933, mean_reward: -801.868, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  441\n",
      "mb subgoal:  [ 0.66548586 -0.74537724  1.4241335 ]  distance:  1.7397169308903897\n",
      "sample final subgoal:  [ 0.93504282 -0.35453479 -0.78736061] distance:  1.2727673483962916\n",
      "\n",
      "Episode: 177, actor_loss: 0.949, critic_loss: 11.861, mean_reward: -842.639, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  429\n",
      "mb subgoal:  [ 0.6661652 -0.746818   1.4253025]  distance:  1.7415511488608024\n",
      "sample final subgoal:  [ 0.99998918 -0.00465155  0.34860083] distance:  1.0590196123190436\n",
      "\n",
      "Episode: 178, actor_loss: 0.357, critic_loss: 10.400, mean_reward: -813.884, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  453\n",
      "mb subgoal:  [ 0.6622206 -0.7443585  1.4249997]  distance:  1.7387437564996626\n",
      "sample final subgoal:  [-0.97895639 -0.20406955  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 179, actor_loss: 3.340, critic_loss: 12.381, mean_reward: -796.844, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  436\n",
      "mb subgoal:  [ 0.6670538 -0.7488776  1.4260429]  distance:  1.7433808614144863\n",
      "sample final subgoal:  [-0.38844843  0.92147046 -0.75492939] distance:  1.2529638423777323\n",
      "\n",
      "Episode: 180, actor_loss: -2.673, critic_loss: 9.542, mean_reward: -814.871, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  39\n",
      "mb subgoal:  [ 0.6634432 -0.7475685  1.4103829]  distance:  1.7286397442670318\n",
      "sample final subgoal:  [-0.6840208   0.7294625   3.41968154] distance:  3.562895145243935\n",
      "\n",
      "Episode: 181, actor_loss: -1.087, critic_loss: 4.285, mean_reward: -813.140, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  57\n",
      "mb subgoal:  [ 0.6622649  -0.74766105  1.4231098 ]  distance:  1.7386297031775255\n",
      "sample final subgoal:  [ 0.7886069  -0.61489768  3.30536833] distance:  3.4533259023955725\n",
      "\n",
      "Episode: 182, actor_loss: 0.339, critic_loss: 5.794, mean_reward: -793.331, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  80\n",
      "mb subgoal:  [ 0.6693304  -0.74616134  1.4237168 ]  distance:  1.741186215225814\n",
      "sample final subgoal:  [0.0982603  0.99516075 1.75798108] distance:  2.0224978269691545\n",
      "\n",
      "Episode: 183, actor_loss: 0.385, critic_loss: 7.495, mean_reward: -837.085, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  97\n",
      "mb subgoal:  [ 0.6582251  -0.75498325  1.4191887 ]  distance:  1.7370540243069896\n",
      "sample final subgoal:  [ 0.61276609  0.79026433 -4.38066215] distance:  4.493350739794059\n",
      "\n",
      "Episode: 184, actor_loss: -1.127, critic_loss: 4.791, mean_reward: -876.571, best_return: -786.180\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  111\n",
      "mb subgoal:  [ 0.6584128 -0.7438497  1.4213184]  distance:  1.7340605035497045\n",
      "sample final subgoal:  [ 0.9882552  -0.15281249 -0.19922209] distance:  1.0196516282589418\n",
      "\n",
      "Episode: 185, actor_loss: 1.928, critic_loss: 4.923, mean_reward: -745.864, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  142\n",
      "mb subgoal:  [ 0.67161775 -0.7521014   1.4286815 ]  distance:  1.748673140729107\n",
      "sample final subgoal:  [0.51741123 0.85573689 2.87600762] distance:  3.0449006281095063\n",
      "\n",
      "Episode: 186, actor_loss: 0.029, critic_loss: 3.072, mean_reward: -860.313, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  155\n",
      "mb subgoal:  [ 0.6641638 -0.743434   1.4270418]  distance:  1.7407630312420173\n",
      "sample final subgoal:  [0.79144197 0.61124431 2.36675521] distance:  2.569344317133754\n",
      "\n",
      "Episode: 187, actor_loss: -0.490, critic_loss: 5.187, mean_reward: -841.040, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  176\n",
      "mb subgoal:  [ 0.66125363 -0.7447856   1.4226832 ]  distance:  1.7366604628809272\n",
      "sample final subgoal:  [-0.98776948 -0.15592132  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 188, actor_loss: -0.296, critic_loss: 6.020, mean_reward: -759.179, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  214\n",
      "mb subgoal:  [ 0.67040354 -0.74539053  1.421697  ]  distance:  1.7396178814858068\n",
      "sample final subgoal:  [ 0.88818805 -0.45948012 -0.68365155] distance:  1.2113543829646392\n",
      "\n",
      "Episode: 189, actor_loss: 3.178, critic_loss: 14.318, mean_reward: -847.996, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  225\n",
      "mb subgoal:  [ 0.6630571 -0.747396   1.4262106]  distance:  1.7413564446456795\n",
      "sample final subgoal:  [ 0.82854142 -0.55992778 -0.97986567] distance:  1.4000488329330183\n",
      "\n",
      "Episode: 190, actor_loss: -3.212, critic_loss: 10.815, mean_reward: -799.583, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  245\n",
      "mb subgoal:  [ 0.6659327  -0.74796075  1.4186188 ]  distance:  1.7364881092536757\n",
      "sample final subgoal:  [ 0.37110249 -0.92859191 -1.25939658] distance:  1.6081292710662536\n",
      "\n",
      "Episode: 191, actor_loss: -0.326, critic_loss: 8.356, mean_reward: -756.925, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  266\n",
      "mb subgoal:  [ 0.66487    -0.74770164  1.4238269 ]  distance:  1.7402278681555807\n",
      "sample final subgoal:  [0.41773484 0.90856898 1.70958807] distance:  1.980578543749194\n",
      "\n",
      "Episode: 192, actor_loss: -2.028, critic_loss: 11.868, mean_reward: -799.265, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  274\n",
      "mb subgoal:  [ 0.665061  -0.7479078  1.4290571]  distance:  1.7446708826850006\n",
      "sample final subgoal:  [0.99797463 0.06361324 0.30075688] distance:  1.0442483907019389\n",
      "\n",
      "Episode: 193, actor_loss: 1.252, critic_loss: 3.476, mean_reward: -855.259, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  280\n",
      "mb subgoal:  [ 0.66662085 -0.7448982   1.4258393 ]  distance:  1.74134270328592\n",
      "sample final subgoal:  [ 0.33732875  0.94138691 -5.16594721] distance:  5.2618447877807855\n",
      "\n",
      "Episode: 194, actor_loss: -0.135, critic_loss: 3.108, mean_reward: -794.400, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  301\n",
      "mb subgoal:  [ 0.66750586 -0.7499576   1.418047  ]  distance:  1.7374860180989933\n",
      "sample final subgoal:  [-0.29645345  0.9550473   5.57981254] distance:  5.668713076391347\n",
      "\n",
      "Episode: 195, actor_loss: -0.443, critic_loss: 3.182, mean_reward: -762.737, best_return: -745.864\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  335\n",
      "mb subgoal:  [ 0.66443753 -0.7444936   1.4281696 ]  distance:  1.742244641260591\n",
      "sample final subgoal:  [-0.38263184  0.9239009   5.65118469] distance:  5.738979732865103\n",
      "\n",
      "Episode: 196, actor_loss: 1.112, critic_loss: 9.792, mean_reward: -714.282, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  355\n",
      "mb subgoal:  [ 0.66513795 -0.74387544  1.4251158 ]  distance:  1.739745462760391\n",
      "sample final subgoal:  [ 0.99860172 -0.05286399  0.48183521] distance:  1.1100293531345975\n",
      "\n",
      "Episode: 197, actor_loss: 1.955, critic_loss: 6.188, mean_reward: -760.847, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  374\n",
      "mb subgoal:  [ 0.66782796 -0.75181365  1.4211642 ]  distance:  1.7409553438473828\n",
      "sample final subgoal:  [0.81943784 0.57316806 0.08543198] distance:  1.0036426768402709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 198, actor_loss: -1.808, critic_loss: 6.620, mean_reward: -749.423, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  393\n",
      "mb subgoal:  [ 0.6623396  -0.74620074  1.4280369 ]  distance:  1.7420673912324198\n",
      "sample final subgoal:  [ 0.99759884 -0.06925723  0.16212699] distance:  1.0130573337023656\n",
      "\n",
      "Episode: 199, actor_loss: 4.763, critic_loss: 12.292, mean_reward: -718.361, best_return: -714.282\n",
      "last 50 episode mean reward:  -847.9874919609651\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  453\n",
      "mb subgoal:  [-0.96338063  0.27244335  1.935683  ]  distance:  2.179265094399166\n",
      "sample final subgoal:  [0.94116154 0.33795704 0.4664    ] distance:  1.1034169474689457\n",
      "\n",
      "Episode: 200, actor_loss: -4.696, critic_loss: 8.455, mean_reward: -753.160, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  34\n",
      "mb subgoal:  [-0.969721    0.28302488  1.9609311 ]  distance:  2.205836024966906\n",
      "sample final subgoal:  [-0.82544965 -0.56447575 -1.51252944] distance:  1.8132140860324382\n",
      "\n",
      "Episode: 201, actor_loss: -0.221, critic_loss: 4.876, mean_reward: -911.721, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  57\n",
      "mb subgoal:  [-0.96652824  0.2667485   1.931085  ]  distance:  2.1758724293998433\n",
      "sample final subgoal:  [-0.89426097  0.44754588  5.27963852] distance:  5.3735075023207886\n",
      "\n",
      "Episode: 202, actor_loss: 0.419, critic_loss: 3.542, mean_reward: -949.217, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  80\n",
      "mb subgoal:  [-0.70625794  0.71555763  1.9339315 ]  distance:  2.179659132979188\n",
      "sample final subgoal:  [-0.63505433  0.77246747 -6.57846956] distance:  6.654041011515862\n",
      "\n",
      "Episode: 203, actor_loss: -0.145, critic_loss: 2.841, mean_reward: -729.025, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  103\n",
      "mb subgoal:  [-0.7066688   0.71388847  1.9378853 ]  distance:  2.182754430213879\n",
      "sample final subgoal:  [ 0.95661347 -0.29136002  0.85763125] distance:  1.3173956734535992\n",
      "\n",
      "Episode: 204, actor_loss: -0.352, critic_loss: 2.830, mean_reward: -773.916, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  119\n",
      "mb subgoal:  [-0.7000891   0.71505713  1.9331164 ]  distance:  2.1767798705208112\n",
      "sample final subgoal:  [ 0.85490544  0.51878385 -3.20712216] distance:  3.3594095540118323\n",
      "\n",
      "Episode: 205, actor_loss: 0.764, critic_loss: 3.922, mean_reward: -769.626, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  144\n",
      "mb subgoal:  [-0.7032443  0.7098367  1.9348338]  distance:  2.177613936156337\n",
      "sample final subgoal:  [-0.99764054  0.06865385 -7.61023487] distance:  7.675654679388094\n",
      "\n",
      "Episode: 206, actor_loss: -0.654, critic_loss: 5.628, mean_reward: -717.705, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  172\n",
      "mb subgoal:  [-0.7073796  0.7084422  1.9364463]  distance:  2.179931316368386\n",
      "sample final subgoal:  [ 0.95812339 -0.28635568 -0.18020788] distance:  1.0161077116983648\n",
      "\n",
      "Episode: 207, actor_loss: 0.822, critic_loss: 6.244, mean_reward: -828.681, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  195\n",
      "mb subgoal:  [-0.70275545  0.71654624  1.9382106 ]  distance:  2.182650702715653\n",
      "sample final subgoal:  [ 0.9635638  -0.26747862  0.66505849] distance:  1.200959116584382\n",
      "\n",
      "Episode: 208, actor_loss: 0.209, critic_loss: 3.778, mean_reward: -835.068, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  227\n",
      "mb subgoal:  [-0.70186096  0.70883113  1.9359807 ]  distance:  2.177859400923015\n",
      "sample final subgoal:  [0.9441191  0.3296045  0.69914235] distance:  1.2201639314458503\n",
      "\n",
      "Episode: 209, actor_loss: 2.840, critic_loss: 5.716, mean_reward: -721.668, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  242\n",
      "mb subgoal:  [-0.7027653  0.7106206  1.9360163]  distance:  2.178765672291603\n",
      "sample final subgoal:  [-0.26700545  0.96369502  0.99874536] distance:  1.413326677487266\n",
      "\n",
      "Episode: 210, actor_loss: -2.964, critic_loss: 8.567, mean_reward: -733.980, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  276\n",
      "mb subgoal:  [-0.7009301  0.7102139  1.9372041]  distance:  2.1790976634164947\n",
      "sample final subgoal:  [ 0.96600103 -0.25853821  1.31418699] distance:  1.6513895529495812\n",
      "\n",
      "Episode: 211, actor_loss: -1.838, critic_loss: 12.666, mean_reward: -721.470, best_return: -714.282\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  324\n",
      "mb subgoal:  [-0.7012749   0.71367306  1.9371021 ]  distance:  2.1802477248047056\n",
      "sample final subgoal:  [ 0.93002979 -0.36748414 -0.77677517] distance:  1.266246289105124\n",
      "\n",
      "Episode: 212, actor_loss: 0.003, critic_loss: 12.340, mean_reward: -701.705, best_return: -701.705\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  364\n",
      "mb subgoal:  [-0.70421094  0.70811576  1.9371985 ]  distance:  2.179467612330381\n",
      "sample final subgoal:  [-0.19436549 -0.98092918 -4.15594217] distance:  4.274559076923371\n",
      "\n",
      "Episode: 213, actor_loss: 1.224, critic_loss: 9.271, mean_reward: -723.646, best_return: -701.705\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  377\n",
      "mb subgoal:  [-0.70447403  0.7120661   1.9381921 ]  distance:  2.181721921265976\n",
      "sample final subgoal:  [0.9588991  0.28374728 0.60031319] distance:  1.166351545136019\n",
      "\n",
      "Episode: 214, actor_loss: 1.364, critic_loss: 5.650, mean_reward: -784.237, best_return: -701.705\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  419\n",
      "mb subgoal:  [-0.702582   0.7111527  1.9344066]  distance:  2.1774500303747426\n",
      "sample final subgoal:  [ 0.98367879 -0.17993342 -3.66708234] distance:  3.800985781034406\n",
      "\n",
      "Episode: 215, actor_loss: -0.215, critic_loss: 6.566, mean_reward: -751.128, best_return: -701.705\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  430\n",
      "mb subgoal:  [-0.704394    0.71146846  1.9367782 ]  distance:  2.1802449400601\n",
      "sample final subgoal:  [ 0.99953537  0.03048018 -0.14731173] distance:  1.0107921377868652\n",
      "\n",
      "Episode: 216, actor_loss: 0.332, critic_loss: 17.778, mean_reward: -684.961, best_return: -684.961\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  456\n",
      "mb subgoal:  [-0.7036957  0.7114168  1.9369184]  distance:  2.180127128710364\n",
      "sample final subgoal:  [-0.87060884 -0.49197586 -6.96941649] distance:  7.0407930080692935\n",
      "\n",
      "Episode: 217, actor_loss: -2.153, critic_loss: 23.544, mean_reward: -684.739, best_return: -684.739\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  463\n",
      "mb subgoal:  [-0.7042905   0.71018326  1.9342328 ]  distance:  2.1775311783538087\n",
      "sample final subgoal:  [ 0.98399754  0.17818202 -3.58155419] distance:  3.7185387466563657\n",
      "\n",
      "Episode: 218, actor_loss: 1.488, critic_loss: 15.357, mean_reward: -766.704, best_return: -684.739\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  470\n",
      "mb subgoal:  [-0.70280594  0.7088807   1.9335635 ]  distance:  2.176032113430955\n",
      "sample final subgoal:  [ 0.97454411 -0.22419585 -1.35355339] distance:  1.6828864437001836\n",
      "\n",
      "Episode: 219, actor_loss: 4.169, critic_loss: 18.201, mean_reward: -806.190, best_return: -684.739\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  495\n",
      "mb subgoal:  [-0.7013773   0.71113986  1.9335954 ]  distance:  2.1763366513347853\n",
      "sample final subgoal:  [-0.21812622  0.97592057 -0.06420985] distance:  1.0020593318748414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 220, actor_loss: -1.136, critic_loss: 9.317, mean_reward: -713.562, best_return: -684.739\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  42\n",
      "mb subgoal:  [-0.69575053  0.70578784  1.9351892 ]  distance:  2.1742039232988786\n",
      "sample final subgoal:  [ 0.46885084 -0.88327736 -0.29241395] distance:  1.0418761528622724\n",
      "\n",
      "Episode: 221, actor_loss: -2.343, critic_loss: 7.248, mean_reward: -864.851, best_return: -684.739\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  66\n",
      "mb subgoal:  [-0.70480096  0.709592    1.9345459 ]  distance:  2.1777816519906015\n",
      "sample final subgoal:  [ 0.22886349 -0.97345853  6.65537082] distance:  6.730078805128931\n",
      "\n",
      "Episode: 222, actor_loss: 0.505, critic_loss: 6.729, mean_reward: -606.615, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  89\n",
      "mb subgoal:  [-0.6972405  0.7107895  1.9359136]  distance:  2.1769536774553697\n",
      "sample final subgoal:  [-0.40928384  0.91240711  6.21504297] distance:  6.294978883978693\n",
      "\n",
      "Episode: 223, actor_loss: 1.425, critic_loss: 15.919, mean_reward: -757.958, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  115\n",
      "mb subgoal:  [-0.702684    0.71234274  1.939479  ]  distance:  2.1823784564003987\n",
      "sample final subgoal:  [0.54160141 0.84063542 2.56264787] distance:  2.750847889744384\n",
      "\n",
      "Episode: 224, actor_loss: 0.165, critic_loss: 7.786, mean_reward: -609.701, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  150\n",
      "mb subgoal:  [-0.70495    0.7102619  1.9339446]  distance:  2.1775141922046037\n",
      "sample final subgoal:  [-0.99025068 -0.13929674  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 225, actor_loss: -0.174, critic_loss: 9.916, mean_reward: -738.029, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  183\n",
      "mb subgoal:  [-0.69893706  0.70959556  1.9340163 ]  distance:  2.1754213619667158\n",
      "sample final subgoal:  [ 0.905503   -0.42433987  3.00727217] distance:  3.1691774818533984\n",
      "\n",
      "Episode: 226, actor_loss: 1.063, critic_loss: 4.858, mean_reward: -650.262, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  212\n",
      "mb subgoal:  [-0.7009017  0.7101076  1.9332037]  distance:  2.175498229910862\n",
      "sample final subgoal:  [ 0.874805   -0.48447519 -0.41627992] distance:  1.083184643891892\n",
      "\n",
      "Episode: 227, actor_loss: 0.300, critic_loss: 7.183, mean_reward: -675.744, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  240\n",
      "mb subgoal:  [-0.6997603  0.710915   1.9357042]  distance:  2.177616943317854\n",
      "sample final subgoal:  [ 0.92369355 -0.38313212 -0.60882435] distance:  1.1707549208055028\n",
      "\n",
      "Episode: 228, actor_loss: 1.660, critic_loss: 10.522, mean_reward: -671.308, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  258\n",
      "mb subgoal:  [-0.703242   0.7097013  1.9362524]  distance:  2.178829602170767\n",
      "sample final subgoal:  [-0.58164985 -0.81343928 -5.55019408] distance:  5.639561533117982\n",
      "\n",
      "Episode: 229, actor_loss: 2.184, critic_loss: 15.793, mean_reward: -688.475, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  294\n",
      "mb subgoal:  [-0.7017646  0.7116192  1.9355216]  distance:  2.1783294719689663\n",
      "sample final subgoal:  [0.67609675 0.73681286 0.64633579] distance:  1.1906930578317063\n",
      "\n",
      "Episode: 230, actor_loss: -5.019, critic_loss: 12.984, mean_reward: -670.826, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  312\n",
      "mb subgoal:  [-0.70390064  0.70901406  1.9312857 ]  distance:  2.1744060440380286\n",
      "sample final subgoal:  [ 0.78807527 -0.61557889  0.055788  ] distance:  1.0015549414388625\n",
      "\n",
      "Episode: 231, actor_loss: -0.091, critic_loss: 7.142, mean_reward: -649.313, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  351\n",
      "mb subgoal:  [-0.70074403  0.71029305  1.9349309 ]  distance:  2.1770429693673012\n",
      "sample final subgoal:  [-0.87615411 -0.4820311   8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 232, actor_loss: -1.211, critic_loss: 10.218, mean_reward: -676.317, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  405\n",
      "mb subgoal:  [-0.7028234  0.7126736  1.9371281]  distance:  2.180442511703692\n",
      "sample final subgoal:  [0.95382801 0.30035333 0.17917206] distance:  1.0159245189788058\n",
      "\n",
      "Episode: 233, actor_loss: 0.859, critic_loss: 9.667, mean_reward: -750.216, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  450\n",
      "mb subgoal:  [-0.7033989  0.7093508  1.9343512]  distance:  2.1770767484363556\n",
      "sample final subgoal:  [0.99481636 0.10168783 0.02386592] distance:  1.000284750605739\n",
      "\n",
      "Episode: 234, actor_loss: -0.091, critic_loss: 12.846, mean_reward: -790.647, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  481\n",
      "mb subgoal:  [-0.7010971  0.7137014  1.937752 ]  distance:  2.1807773101093133\n",
      "sample final subgoal:  [ 0.99895664  0.04566879 -0.06076551] distance:  1.0018445226396555\n",
      "\n",
      "Episode: 235, actor_loss: 1.432, critic_loss: 12.538, mean_reward: -768.726, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  480\n",
      "mb subgoal:  [-0.70221496  0.7111411   1.9366634 ]  distance:  2.1793330643782434\n",
      "sample final subgoal:  [ 0.99695617 -0.0779641   0.04019753] distance:  1.0008075944211425\n",
      "\n",
      "Episode: 236, actor_loss: -0.525, critic_loss: 18.800, mean_reward: -759.809, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  544\n",
      "mb subgoal:  [-0.7066758  0.7101887  1.9360856]  distance:  2.1799509343281485\n",
      "sample final subgoal:  [-0.16786868  0.98580937  5.25875352] distance:  5.352988749927682\n",
      "\n",
      "Episode: 237, actor_loss: 0.198, critic_loss: 6.863, mean_reward: -885.010, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  612\n",
      "mb subgoal:  [-0.69768876  0.7127885   1.9363797 ]  distance:  2.17816512497089\n",
      "sample final subgoal:  [0.93666967 0.35021412 0.43674688] distance:  1.0912139295624252\n",
      "\n",
      "Episode: 238, actor_loss: -0.106, critic_loss: 6.826, mean_reward: -805.902, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  665\n",
      "mb subgoal:  [-0.7026177  0.710143   1.9337568]  distance:  2.176554670308016\n",
      "sample final subgoal:  [ 0.93420765 -0.35672968  1.66843711] distance:  1.9451689836326087\n",
      "\n",
      "Episode: 239, actor_loss: 5.215, critic_loss: 13.716, mean_reward: -741.853, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  699\n",
      "mb subgoal:  [-0.41302812 -0.9118602  -1.4319917 ]  distance:  1.747192468453564\n",
      "sample final subgoal:  [-0.51426263  0.85763276  0.78933906] distance:  1.2739922129194565\n",
      "\n",
      "Episode: 240, actor_loss: -1.645, critic_loss: 12.513, mean_reward: -696.458, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  56\n",
      "mb subgoal:  [-0.41147655 -0.9153098  -1.429208  ]  distance:  1.7463506467290575\n",
      "sample final subgoal:  [ 0.04009892 -0.99919571 -1.81877324] distance:  2.0755568188159055\n",
      "\n",
      "Episode: 241, actor_loss: -1.949, critic_loss: 6.945, mean_reward: -813.966, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  95\n",
      "mb subgoal:  [-0.40498817 -0.91157156 -1.4219222 ]  distance:  1.736905551019861\n",
      "sample final subgoal:  [ 0.46558035 -0.88500561 -0.57111061] distance:  1.1515933885182301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 242, actor_loss: -3.060, critic_loss: 6.138, mean_reward: -698.781, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  133\n",
      "mb subgoal:  [-0.4126229  -0.90792924 -1.4239378 ]  distance:  1.7384452869635478\n",
      "sample final subgoal:  [ 0.79718997 -0.60372854  0.01527056] distance:  1.0001165881836815\n",
      "\n",
      "Episode: 243, actor_loss: -0.476, critic_loss: 6.709, mean_reward: -779.851, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  171\n",
      "mb subgoal:  [-0.41256174 -0.90526104 -1.4213277 ]  distance:  1.7348997673323834\n",
      "sample final subgoal:  [ 0.19472749  0.98085738 -3.17261172] distance:  3.326479391689477\n",
      "\n",
      "Episode: 244, actor_loss: -0.753, critic_loss: 3.645, mean_reward: -788.330, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  219\n",
      "mb subgoal:  [-0.41174108 -0.9124063  -1.4309365 ]  distance:  1.7463090099059233\n",
      "sample final subgoal:  [-0.90422613  0.42705398  6.35207226] distance:  6.430304972663422\n",
      "\n",
      "Episode: 245, actor_loss: -1.162, critic_loss: 2.000, mean_reward: -792.659, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  259\n",
      "mb subgoal:  [-0.41497827 -0.91107076 -1.432218  ]  distance:  1.7474281588955485\n",
      "sample final subgoal:  [ 0.97661017 -0.21501763  0.01670405] distance:  1.0001395029705198\n",
      "\n",
      "Episode: 246, actor_loss: 0.334, critic_loss: 1.609, mean_reward: -785.296, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  300\n",
      "mb subgoal:  [-0.4178438 -0.9055146 -1.4318421]  distance:  1.7449131456489526\n",
      "sample final subgoal:  [0.12023749 0.99274516 2.67978295] distance:  2.860286115098702\n",
      "\n",
      "Episode: 247, actor_loss: 0.136, critic_loss: 2.706, mean_reward: -768.057, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  331\n",
      "mb subgoal:  [-0.41545916 -0.9090144  -1.4328116 ]  distance:  1.746958107378736\n",
      "sample final subgoal:  [-0.38454536  0.9231061  -5.50495565] distance:  5.595045724431161\n",
      "\n",
      "Episode: 248, actor_loss: -1.726, critic_loss: 4.339, mean_reward: -606.750, best_return: -606.615\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  415\n",
      "mb subgoal:  [-0.41358724 -0.9120238  -1.4334279 ]  distance:  1.748587263326402\n",
      "sample final subgoal:  [0.62686743 0.77912594 0.92777313] distance:  1.364097861546977\n",
      "\n",
      "Episode: 249, actor_loss: 6.155, critic_loss: 17.715, mean_reward: -554.556, best_return: -554.556\n",
      "last 50 episode mean reward:  -742.1674945247213\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  479\n",
      "mb subgoal:  [-0.41642278 -0.910978   -1.4325348 ]  distance:  1.7479830891123127\n",
      "sample final subgoal:  [0.95264117 0.30409669 0.50790647] distance:  1.1215921658733006\n",
      "\n",
      "Episode: 250, actor_loss: -0.635, critic_loss: 6.628, mean_reward: -820.498, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  547\n",
      "mb subgoal:  [-0.98474854  0.1355578  -0.4658069 ]  distance:  1.0977621228240966\n",
      "sample final subgoal:  [-0.30510813 -0.95231771 -0.81537052] distance:  1.2902825610915505\n",
      "\n",
      "Episode: 251, actor_loss: -2.686, critic_loss: 11.733, mean_reward: -706.189, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  594\n",
      "mb subgoal:  [-0.9870078   0.13719583 -0.46306503]  distance:  1.0988340665280318\n",
      "sample final subgoal:  [0.41787576 0.90850418 2.36901268] distance:  2.5714239395557894\n",
      "\n",
      "Episode: 252, actor_loss: 2.247, critic_loss: 7.420, mean_reward: -735.124, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  630\n",
      "mb subgoal:  [-0.98844767  0.13493428 -0.4628107 ]  distance:  1.099740781843806\n",
      "sample final subgoal:  [-0.99848046 -0.05510684  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 253, actor_loss: -1.644, critic_loss: 6.358, mean_reward: -723.148, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  691\n",
      "mb subgoal:  [-0.9895213   0.13542262 -0.46719694]  distance:  1.1026172133545953\n",
      "sample final subgoal:  [ 0.83394105 -0.55185354  4.32970513] distance:  4.4436861373516345\n",
      "\n",
      "Episode: 254, actor_loss: 1.159, critic_loss: 8.372, mean_reward: -757.450, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  753\n",
      "mb subgoal:  [-0.9903729   0.13524342 -0.46491992]  distance:  1.1023972929299108\n",
      "sample final subgoal:  [ 0.99325    -0.11599329  2.23589917] distance:  2.449335642255245\n",
      "\n",
      "Episode: 255, actor_loss: -0.156, critic_loss: 8.133, mean_reward: -689.666, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  818\n",
      "mb subgoal:  [-0.9908004   0.13595152 -0.46740043]  distance:  1.1039163797232188\n",
      "sample final subgoal:  [0.19463234 0.98087627 3.84785835] distance:  3.975677787703784\n",
      "\n",
      "Episode: 256, actor_loss: -0.419, critic_loss: 19.403, mean_reward: -756.315, best_return: -554.556\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  988\n",
      "mb subgoal:  [-0.9907084  0.1346321 -0.4671921]  distance:  1.1035839012619642\n",
      "sample final subgoal:  [ 0.1064466  -0.99431842  7.18193999] distance:  7.251224862167687\n",
      "\n",
      "Episode: 257, actor_loss: 0.547, critic_loss: 13.996, mean_reward: -514.409, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  1065\n",
      "mb subgoal:  [-0.9902785   0.1341139  -0.46719614]  distance:  1.1031365460403415\n",
      "sample final subgoal:  [0.14159463 0.98992472 3.98861406] distance:  4.112060572312153\n",
      "\n",
      "Episode: 258, actor_loss: 1.238, critic_loss: 14.523, mean_reward: -545.232, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  1180\n",
      "mb subgoal:  [-0.99068934  0.1330283  -0.46785003]  distance:  1.1036510051888713\n",
      "sample final subgoal:  [ 0.95732     0.28903014 -0.85825684] distance:  1.3178030237359297\n",
      "\n",
      "Episode: 259, actor_loss: 5.635, critic_loss: 23.297, mean_reward: -653.220, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  1302\n",
      "mb subgoal:  [-0.9904085   0.13480483 -0.46710244]  distance:  1.1032977782764606\n",
      "sample final subgoal:  [ 0.33181246 -0.94334537  0.04663547] distance:  1.0010868428813113\n",
      "\n",
      "Episode: 260, actor_loss: -4.562, critic_loss: 15.943, mean_reward: -564.153, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  92\n",
      "mb subgoal:  [-0.98584354  0.13243915 -0.45869455]  distance:  1.0953667424705418\n",
      "sample final subgoal:  [-0.99803177  0.06271026 -2.64479022] distance:  2.8275281265201504\n",
      "\n",
      "Episode: 261, actor_loss: -1.327, critic_loss: 13.070, mean_reward: -663.623, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  170\n",
      "mb subgoal:  [-0.98955923  0.13450415 -0.4726068 ]  distance:  1.1048420875570535\n",
      "sample final subgoal:  [ 0.88857425 -0.45873282  3.79451366] distance:  3.9240710906478906\n",
      "\n",
      "Episode: 262, actor_loss: 0.418, critic_loss: 15.758, mean_reward: -668.807, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  244\n",
      "mb subgoal:  [-0.9916245   0.13768262 -0.46480852]  distance:  1.103776501418859\n",
      "sample final subgoal:  [-0.69902749  0.7150948   6.58593187] distance:  6.661418659991503\n",
      "\n",
      "Episode: 263, actor_loss: 0.863, critic_loss: 8.783, mean_reward: -521.058, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  318\n",
      "mb subgoal:  [-0.9914775   0.13542901 -0.4694724 ]  distance:  1.1053383965884342\n",
      "sample final subgoal:  [ 0.58649762 -0.80995095  5.91520668] distance:  5.999139111787252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 264, actor_loss: -0.991, critic_loss: 9.168, mean_reward: -717.828, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  398\n",
      "mb subgoal:  [-0.9904399   0.13318413 -0.47044358]  distance:  1.1045480305735962\n",
      "sample final subgoal:  [-0.29478844  0.95556254  5.93824075] distance:  6.021852143325613\n",
      "\n",
      "Episode: 265, actor_loss: 1.251, critic_loss: 31.764, mean_reward: -685.541, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  492\n",
      "mb subgoal:  [-0.99013484  0.13257796 -0.4698252 ]  distance:  1.103938237653639\n",
      "sample final subgoal:  [0.64477374 0.76437348 2.84850958] distance:  3.0189413400431637\n",
      "\n",
      "Episode: 266, actor_loss: 1.815, critic_loss: 14.920, mean_reward: -645.131, best_return: -514.409\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  579\n",
      "mb subgoal:  [-0.98931235  0.13431194 -0.47015798]  distance:  1.10355206163342\n",
      "sample final subgoal:  [ 0.99977955  0.02099659 -0.13434833] distance:  1.0089843775653518\n",
      "\n",
      "Episode: 267, actor_loss: -0.513, critic_loss: 13.124, mean_reward: -489.510, best_return: -489.510\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  667\n",
      "mb subgoal:  [-0.989239    0.13604558 -0.47277933]  distance:  1.104817836232781\n",
      "sample final subgoal:  [ 0.82919996  0.55895207 -1.81855166] distance:  2.0753626562525462\n",
      "\n",
      "Episode: 268, actor_loss: 0.251, critic_loss: 22.174, mean_reward: -490.278, best_return: -489.510\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  803\n",
      "mb subgoal:  [-0.5657682   0.82526225  0.5374986 ]  distance:  1.1358063887545935\n",
      "sample final subgoal:  [-0.78837199 -0.61519884  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 269, actor_loss: 2.072, critic_loss: 22.929, mean_reward: -452.029, best_return: -452.029\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  910\n",
      "mb subgoal:  [-0.56139207  0.827671    0.5404396 ]  distance:  1.1367828695246462\n",
      "sample final subgoal:  [-0.74058256  0.67196537 -0.80168003] distance:  1.2816750233391898\n",
      "\n",
      "Episode: 270, actor_loss: -3.271, critic_loss: 6.528, mean_reward: -465.271, best_return: -452.029\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  998\n",
      "mb subgoal:  [-0.5642258   0.82587117  0.53954875]  distance:  1.1364536034385329\n",
      "sample final subgoal:  [-0.73534299 -0.67769513 -1.11046249] distance:  1.4943650630413798\n",
      "\n",
      "Episode: 271, actor_loss: -0.043, critic_loss: 10.341, mean_reward: -505.607, best_return: -452.029\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  1072\n",
      "mb subgoal:  [-0.5584002   0.82941175  0.54109037]  distance:  1.1368876062349929\n",
      "sample final subgoal:  [0.80542237 0.59270127 0.6807196 ] distance:  1.2097021016732348\n",
      "\n",
      "Episode: 272, actor_loss: -1.719, critic_loss: 11.174, mean_reward: -484.718, best_return: -452.029\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  1146\n",
      "mb subgoal:  [-0.56102484  0.8277824   0.5389434 ]  distance:  1.1359720792090573\n",
      "sample final subgoal:  [ 0.54157158  0.84065464 -2.29723029] distance:  2.5054474624230774\n",
      "\n",
      "Episode: 273, actor_loss: 0.983, critic_loss: 15.814, mean_reward: -475.625, best_return: -452.029\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  1232\n",
      "mb subgoal:  [-0.560579   0.8254361  0.5396186]  distance:  1.1343640700668756\n",
      "sample final subgoal:  [0.55438998 0.83225702 1.91009291] distance:  2.156027584131562\n",
      "\n",
      "Episode: 274, actor_loss: -1.878, critic_loss: 6.439, mean_reward: -410.634, best_return: -410.634\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  1318\n",
      "mb subgoal:  [-0.5641681   0.8273755   0.54207605]  distance:  1.1387195688117902\n",
      "sample final subgoal:  [ 0.64383639 -0.76516318 -1.59142106] distance:  1.8795268027387764\n",
      "\n",
      "Episode: 275, actor_loss: 1.793, critic_loss: 16.233, mean_reward: -618.807, best_return: -410.634\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  1387\n",
      "mb subgoal:  [-0.56220645  0.8272532   0.54146165]  distance:  1.1373674419289357\n",
      "sample final subgoal:  [ 0.01762768 -0.99984462 -3.44000308] distance:  3.582404383663744\n",
      "\n",
      "Episode: 276, actor_loss: -1.309, critic_loss: 7.593, mean_reward: -514.937, best_return: -410.634\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  1471\n",
      "mb subgoal:  [-0.5594502  0.8302789  0.5425456]  distance:  1.1387288045515176\n",
      "sample final subgoal:  [ 0.86909335 -0.49464811 -0.59435765] distance:  1.1632974743819833\n",
      "\n",
      "Episode: 277, actor_loss: 1.018, critic_loss: 39.717, mean_reward: -403.219, best_return: -403.219\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  1538\n",
      "mb subgoal:  [-0.5617882   0.8278678   0.54227144]  distance:  1.1379935838119202\n",
      "sample final subgoal:  [-0.29583027  0.95524052 -6.34266873] distance:  6.421016008704392\n",
      "\n",
      "Episode: 278, actor_loss: 1.370, critic_loss: 16.943, mean_reward: -489.404, best_return: -403.219\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  1622\n",
      "mb subgoal:  [-0.5628033   0.82636905  0.54141915]  distance:  1.136999596510535\n",
      "sample final subgoal:  [0.99686379 0.07913646 0.08116003] distance:  1.0032880695578446\n",
      "\n",
      "Episode: 279, actor_loss: 5.727, critic_loss: 44.484, mean_reward: -406.382, best_return: -403.219\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  1665\n",
      "mb subgoal:  [-0.5630903   0.8257147   0.53959197]  distance:  1.1357970696405368\n",
      "sample final subgoal:  [-0.59721629  0.80208023 -0.56634724] distance:  1.1492385264851037\n",
      "\n",
      "Episode: 280, actor_loss: -3.786, critic_loss: 22.880, mean_reward: -307.859, best_return: -307.859\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  106\n",
      "mb subgoal:  [-0.56460977  0.8310819   0.5406452 ]  distance:  1.1409551130191198\n",
      "sample final subgoal:  [-0.42646735  0.90450296  2.4638479 ] distance:  2.659049919833772\n",
      "\n",
      "Episode: 281, actor_loss: -2.340, critic_loss: 15.905, mean_reward: -361.082, best_return: -307.859\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  187\n",
      "mb subgoal:  [-0.56036574  0.83247364  0.54125786]  distance:  1.1401676113025052\n",
      "sample final subgoal:  [ 0.73066567 -0.68273544  4.42635444] distance:  4.537908508155752\n",
      "\n",
      "Episode: 282, actor_loss: -2.713, critic_loss: 7.107, mean_reward: -352.108, best_return: -307.859\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  268\n",
      "mb subgoal:  [-0.5681249  0.8224903  0.5329121]  distance:  1.132806878315698\n",
      "sample final subgoal:  [ 0.97682326 -0.21404746  0.06536135] distance:  1.0021337767560576\n",
      "\n",
      "Episode: 283, actor_loss: 0.337, critic_loss: 14.773, mean_reward: -306.204, best_return: -306.204\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  349\n",
      "mb subgoal:  [-0.56705135  0.8226396   0.536563  ]  distance:  1.1341000623736743\n",
      "sample final subgoal:  [ 0.06509052 -0.99787936  7.04481763] distance:  7.115437826532856\n",
      "\n",
      "Episode: 284, actor_loss: 1.355, critic_loss: 26.597, mean_reward: -328.813, best_return: -306.204\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  419\n",
      "mb subgoal:  [-0.5651289  0.8282906  0.5400737]  distance:  1.1389098210108295\n",
      "sample final subgoal:  [0.99923929 0.03899795 0.03348509] distance:  1.0005604684529124\n",
      "\n",
      "Episode: 285, actor_loss: 3.081, critic_loss: 75.008, mean_reward: -292.944, best_return: -292.944\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  485\n",
      "mb subgoal:  [-0.5602924  0.8271205  0.5401915]  distance:  1.1357212326305066\n",
      "sample final subgoal:  [0.958627   0.28466519 0.08199151] distance:  1.0033556739920262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 286, actor_loss: 2.490, critic_loss: 56.655, mean_reward: -327.051, best_return: -292.944\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  548\n",
      "mb subgoal:  [-0.5597958   0.8261192   0.54078084]  distance:  1.1350278237060887\n",
      "sample final subgoal:  [ 0.99094851  0.13424247 -0.1138396 ] distance:  1.0064588682766877\n",
      "\n",
      "Episode: 287, actor_loss: -0.805, critic_loss: 51.512, mean_reward: -366.827, best_return: -292.944\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  622\n",
      "mb subgoal:  [-0.56160873  0.82731855  0.5398896 ]  distance:  1.1363719028209358\n",
      "sample final subgoal:  [ 0.9671292  -0.25428548  0.0094672 ] distance:  1.0000448128913721\n",
      "\n",
      "Episode: 288, actor_loss: 2.110, critic_loss: 69.358, mean_reward: -394.727, best_return: -292.944\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  687\n",
      "mb subgoal:  [-0.56167823  0.8274079   0.5394916 ]  distance:  1.1362822897306666\n",
      "sample final subgoal:  [ 0.9976911  -0.06791523 -0.06121254] distance:  1.0018717357566889\n",
      "\n",
      "Episode: 289, actor_loss: 12.433, critic_loss: 38.443, mean_reward: -334.096, best_return: -292.944\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  751\n",
      "mb subgoal:  [-0.5617147  0.8230687  0.5363803]  distance:  1.131666593999266\n",
      "sample final subgoal:  [ 0.4319268  -0.90190867  0.53108596] distance:  1.1322774805819213\n",
      "\n",
      "Episode: 290, actor_loss: -3.025, critic_loss: 41.512, mean_reward: -217.985, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  801\n",
      "mb subgoal:  [-0.5606037  0.825143   0.5405592]  distance:  1.1346107878806282\n",
      "sample final subgoal:  [ 0.64531797  0.76391407 -3.19601687] distance:  3.3488093198161377\n",
      "\n",
      "Episode: 291, actor_loss: -5.535, critic_loss: 42.085, mean_reward: -389.561, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  860\n",
      "mb subgoal:  [-0.5630297   0.82610065  0.5380508 ]  distance:  1.1353164149294634\n",
      "sample final subgoal:  [ 0.99633041  0.08559039 -0.24418721] distance:  1.029382045475381\n",
      "\n",
      "Episode: 292, actor_loss: -1.529, critic_loss: 19.020, mean_reward: -293.833, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  917\n",
      "mb subgoal:  [-0.56090266  0.82738775  0.5372783 ]  distance:  1.1348348969920639\n",
      "sample final subgoal:  [-0.72923728  0.6842609  -6.13814183] distance:  6.2190662563159185\n",
      "\n",
      "Episode: 293, actor_loss: 2.650, critic_loss: 20.925, mean_reward: -269.137, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  986\n",
      "mb subgoal:  [-0.5643362  0.8257603  0.5422153]  distance:  1.1376962794289984\n",
      "sample final subgoal:  [-0.94253047 -0.3341202  -5.0820026 ] distance:  5.179454645213653\n",
      "\n",
      "Episode: 294, actor_loss: -0.870, critic_loss: 47.657, mean_reward: -459.255, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  1025\n",
      "mb subgoal:  [-0.5633718   0.82703197  0.5397965 ]  distance:  1.1369915874409802\n",
      "sample final subgoal:  [0.99731461 0.07323644 0.03064501] distance:  1.0004694481531473\n",
      "\n",
      "Episode: 295, actor_loss: -1.749, critic_loss: 17.521, mean_reward: -327.417, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  1076\n",
      "mb subgoal:  [-0.56222916  0.8261053   0.5413244 ]  distance:  1.1364786257125508\n",
      "sample final subgoal:  [ 0.9935086  -0.11375707 -0.05221064] distance:  1.0013620478619003\n",
      "\n",
      "Episode: 296, actor_loss: 1.880, critic_loss: 44.375, mean_reward: -382.210, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  1108\n",
      "mb subgoal:  [-0.5594186  0.826949   0.5408808]  distance:  1.1354936720049236\n",
      "sample final subgoal:  [ 0.99989778 -0.01429811  0.05608242] distance:  1.0015713840648472\n",
      "\n",
      "Episode: 297, actor_loss: 0.388, critic_loss: 22.795, mean_reward: -252.669, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  1171\n",
      "mb subgoal:  [-0.55688673  0.8264399   0.5419774 ]  distance:  1.1344008452050975\n",
      "sample final subgoal:  [ 0.99993746  0.01118371 -0.11596742] distance:  1.006701764235175\n",
      "\n",
      "Episode: 298, actor_loss: 2.202, critic_loss: 25.723, mean_reward: -501.200, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  1235\n",
      "mb subgoal:  [-0.5607868   0.82727474  0.540605  ]  distance:  1.1362742129911285\n",
      "sample final subgoal:  [ 0.99288202 -0.11910201  0.05621224] distance:  1.0015786616702451\n",
      "\n",
      "Episode: 299, actor_loss: 6.327, critic_loss: 45.449, mean_reward: -474.105, best_return: -217.985\n",
      "last 50 episode mean reward:  -490.25787478086016\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  1279\n",
      "mb subgoal:  [-0.56053054  0.8272123   0.5413979 ]  distance:  1.136479808054402\n",
      "sample final subgoal:  [-0.16569808 -0.98617653 -0.144114  ] distance:  1.010331056489425\n",
      "\n",
      "Episode: 300, actor_loss: -3.129, critic_loss: 31.912, mean_reward: -585.246, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  80\n",
      "mb subgoal:  [-0.5510448   0.82822603  0.5426494 ]  distance:  1.1331712627671913\n",
      "sample final subgoal:  [-0.23289496 -0.9725019  -0.46804889] distance:  1.104114922071278\n",
      "\n",
      "Episode: 301, actor_loss: -2.755, critic_loss: 56.197, mean_reward: -362.077, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  132\n",
      "mb subgoal:  [-0.56465065  0.8299212   0.5432131 ]  distance:  1.1413500954771054\n",
      "sample final subgoal:  [ 0.44198609  0.8970219  -5.34353864] distance:  5.4363043713429855\n",
      "\n",
      "Episode: 302, actor_loss: 1.791, critic_loss: 34.482, mean_reward: -345.601, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  183\n",
      "mb subgoal:  [-0.56145906  0.82803637  0.5493561 ]  distance:  1.1413468515249092\n",
      "sample final subgoal:  [0.95336926 0.30180633 0.15113616] distance:  1.011356583500225\n",
      "\n",
      "Episode: 303, actor_loss: -0.481, critic_loss: 14.412, mean_reward: -416.846, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  222\n",
      "mb subgoal:  [-0.5562888   0.82906985  0.5402201 ]  distance:  1.1351879857782112\n",
      "sample final subgoal:  [-0.81965438 -0.57285835 -7.00069411] distance:  7.071754946658288\n",
      "\n",
      "Episode: 304, actor_loss: 0.039, critic_loss: 15.355, mean_reward: -401.793, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  272\n",
      "mb subgoal:  [-0.5566221   0.826088    0.53452164]  distance:  1.1304702269788587\n",
      "sample final subgoal:  [0.9969425  0.07813866 0.02626595] distance:  1.000344890488711\n",
      "\n",
      "Episode: 305, actor_loss: -0.977, critic_loss: 10.069, mean_reward: -312.141, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  323\n",
      "mb subgoal:  [-0.5634638  0.8283951  0.543305 ]  distance:  1.1396974385790741\n",
      "sample final subgoal:  [-0.83265703 -0.55378901 -7.17355254] distance:  7.242917644872368\n",
      "\n",
      "Episode: 306, actor_loss: 0.435, critic_loss: 8.564, mean_reward: -484.017, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  366\n",
      "mb subgoal:  [-0.5613185   0.82656467  0.5427326 ]  distance:  1.137033993099414\n",
      "sample final subgoal:  [0.99097407 0.13405372 0.00454498] distance:  1.0000103283500366\n",
      "\n",
      "Episode: 307, actor_loss: 0.657, critic_loss: 6.057, mean_reward: -503.208, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  416\n",
      "mb subgoal:  [-0.5585674   0.82462645  0.53929543]  distance:  1.132627867778883\n",
      "sample final subgoal:  [ 0.97949456  0.20147062 -0.01451987] distance:  1.0001054077364244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 308, actor_loss: -0.244, critic_loss: 6.101, mean_reward: -316.609, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  457\n",
      "mb subgoal:  [-0.5634953  0.8248345  0.5395611]  distance:  1.1353436027260253\n",
      "sample final subgoal:  [ 0.99022034  0.1395123  -0.20579279] distance:  1.0209557639723872\n",
      "\n",
      "Episode: 309, actor_loss: -3.858, critic_loss: 161.024, mean_reward: -341.233, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  496\n",
      "mb subgoal:  [-0.5641854  0.8253979  0.5371381]  distance:  1.134946780676285\n",
      "sample final subgoal:  [ 0.76379549 -0.64545832 -0.30398534] distance:  1.0451828011137303\n",
      "\n",
      "Episode: 310, actor_loss: -5.855, critic_loss: 32.707, mean_reward: -306.235, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  522\n",
      "mb subgoal:  [-0.5618124   0.8266884   0.54040915]  distance:  1.1362609497834577\n",
      "sample final subgoal:  [-0.3272355   0.94494282  2.86469574] distance:  3.034218467438982\n",
      "\n",
      "Episode: 311, actor_loss: 2.153, critic_loss: 37.776, mean_reward: -365.711, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  532\n",
      "mb subgoal:  [-0.5621824   0.8239884   0.53729045]  distance:  1.1329990995733217\n",
      "sample final subgoal:  [ 0.97210239 -0.23455689 -0.29684037] distance:  1.0431271287913528\n",
      "\n",
      "Episode: 312, actor_loss: 0.929, critic_loss: 16.040, mean_reward: -290.269, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  548\n",
      "mb subgoal:  [-0.5620978  0.8253575  0.5400429]  distance:  1.1352599833287662\n",
      "sample final subgoal:  [ 0.98455112 -0.17509737 -3.72379423] distance:  3.855728655960788\n",
      "\n",
      "Episode: 313, actor_loss: 1.418, critic_loss: 25.799, mean_reward: -318.426, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  532\n",
      "mb subgoal:  [-0.56290954  0.8260107   0.54247075]  distance:  1.1372929945438972\n",
      "sample final subgoal:  [-0.68406489 -0.72942116 -5.77987019] distance:  5.8657394640102245\n",
      "\n",
      "Episode: 314, actor_loss: -0.243, critic_loss: 24.435, mean_reward: -284.704, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  523\n",
      "mb subgoal:  [-0.5640373  0.8266338  0.5402601]  distance:  1.1372521749826172\n",
      "sample final subgoal:  [ 0.97193504 -0.23524941  3.96112317] distance:  4.085400443706941\n",
      "\n",
      "Episode: 315, actor_loss: 0.262, critic_loss: 9.599, mean_reward: -291.440, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  541\n",
      "mb subgoal:  [-0.5625244   0.8294495   0.54056615]  distance:  1.1386974427177776\n",
      "sample final subgoal:  [ 0.99584477 -0.09106701 -0.06255629] distance:  1.00195473410544\n",
      "\n",
      "Episode: 316, actor_loss: -0.395, critic_loss: 13.417, mean_reward: -313.090, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  556\n",
      "mb subgoal:  [-0.5612714   0.8287463   0.53881365]  distance:  1.1367348979052776\n",
      "sample final subgoal:  [ 0.97179012  0.23584735 -4.74469167] distance:  4.848927618144218\n",
      "\n",
      "Episode: 317, actor_loss: -0.210, critic_loss: 6.939, mean_reward: -341.940, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  516\n",
      "mb subgoal:  [-0.5609009   0.82861567  0.5448657 ]  distance:  1.1393385938240848\n",
      "sample final subgoal:  [ 0.95525987  0.29576778 -5.32802311] distance:  5.421054351900875\n",
      "\n",
      "Episode: 318, actor_loss: 1.322, critic_loss: 26.069, mean_reward: -330.662, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  467\n",
      "mb subgoal:  [-0.5625067   0.8281382   0.54444903]  distance:  1.1395838455033374\n",
      "sample final subgoal:  [ 0.78056147 -0.62507903 -4.37322158] distance:  4.486097079408365\n",
      "\n",
      "Episode: 319, actor_loss: -13.099, critic_loss: 66.057, mean_reward: -252.828, best_return: -217.985\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  451\n",
      "mb subgoal:  [-0.563472   0.8261564  0.5374757]  distance:  1.1353039938631309\n",
      "sample final subgoal:  [ 0.99514083  0.09846176 -0.06244336] distance:  1.001947689851619\n",
      "\n",
      "Episode: 320, actor_loss: -2.417, critic_loss: 24.416, mean_reward: -128.803, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  31\n",
      "mb subgoal:  [-0.575251    0.8265827   0.55304426]  distance:  1.1489171238301734\n",
      "sample final subgoal:  [ 0.9566121   0.29136454 -0.01210922] distance:  1.0000733138787823\n",
      "\n",
      "Episode: 321, actor_loss: -0.578, critic_loss: 19.773, mean_reward: -471.336, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  47\n",
      "mb subgoal:  [-0.5585384   0.83132994  0.54236376]  distance:  1.1389613874966624\n",
      "sample final subgoal:  [-0.50041606 -0.86578506 -5.55484529] distance:  5.644139102465248\n",
      "\n",
      "Episode: 322, actor_loss: 0.396, critic_loss: 40.676, mean_reward: -423.816, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  60\n",
      "mb subgoal:  [-0.5627098   0.82118064  0.54030615]  distance:  1.1326564842661404\n",
      "sample final subgoal:  [-0.83481079 -0.55053695  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 323, actor_loss: 2.619, critic_loss: 12.105, mean_reward: -281.762, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  73\n",
      "mb subgoal:  [-0.56167996  0.8289604   0.5472575 ]  distance:  1.1411180943070025\n",
      "sample final subgoal:  [0.58519514 0.8108925  2.55817954] distance:  2.7466857429583156\n",
      "\n",
      "Episode: 324, actor_loss: 1.143, critic_loss: 18.118, mean_reward: -415.841, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  88\n",
      "mb subgoal:  [-0.5671982   0.82883745  0.53879976]  distance:  1.1397326560501437\n",
      "sample final subgoal:  [ 0.77255962 -0.63494222 -4.91223535] distance:  5.012988740132694\n",
      "\n",
      "Episode: 325, actor_loss: -0.203, critic_loss: 10.359, mean_reward: -521.873, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  85\n",
      "mb subgoal:  [-0.5607852   0.8277273   0.54100305]  distance:  1.1367923393662078\n",
      "sample final subgoal:  [ 9.99999592e-01 -9.03224439e-04 -4.40956576e-02] distance:  1.00097174136717\n",
      "\n",
      "Episode: 326, actor_loss: 0.709, critic_loss: 15.895, mean_reward: -411.249, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  91\n",
      "mb subgoal:  [-0.5621803  0.8259831  0.5417814]  distance:  1.1365834215667912\n",
      "sample final subgoal:  [0.95762667 0.28801244 3.33581776] distance:  3.4824818921509366\n",
      "\n",
      "Episode: 327, actor_loss: -0.657, critic_loss: 13.718, mean_reward: -446.698, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  92\n",
      "mb subgoal:  [-0.5577878  0.8250505  0.53706  ]  distance:  1.1314897056570763\n",
      "sample final subgoal:  [ 0.2531313  -0.96743193  6.99151031] distance:  7.0626635466803\n",
      "\n",
      "Episode: 328, actor_loss: -0.063, critic_loss: 11.686, mean_reward: -214.252, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  93\n",
      "mb subgoal:  [-0.56385595  0.82417685  0.54295766]  distance:  1.1366635513536487\n",
      "sample final subgoal:  [ 0.99759533 -0.06930772 -0.08409717] distance:  1.003529936456881\n",
      "\n",
      "Episode: 329, actor_loss: 5.337, critic_loss: 85.806, mean_reward: -319.602, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  62\n",
      "mb subgoal:  [-0.5611457   0.8277362   0.54046804]  distance:  1.1367222336267095\n",
      "sample final subgoal:  [ 0.809276    0.5874286  -0.78017218] distance:  1.2683330102838315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 330, actor_loss: -3.239, critic_loss: 28.932, mean_reward: -723.548, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  23\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.25228968  0.96765175 -6.00835197] distance:  6.091001020316049\n",
      "\n",
      "Episode: 331, actor_loss: -2.243, critic_loss: 26.739, mean_reward: -511.978, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.42286793  0.90619132  1.41948347] distance:  1.736356335410217\n",
      "\n",
      "Episode: 332, actor_loss: -2.075, critic_loss: 13.232, mean_reward: -459.958, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.67736397 -0.73564805  5.36054634] distance:  5.453022744461628\n",
      "\n",
      "Episode: 333, actor_loss: 1.614, critic_loss: 25.819, mean_reward: -528.167, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99434328 -0.10621413  0.25121744] distance:  1.0310723544844045\n",
      "\n",
      "Episode: 334, actor_loss: -2.860, critic_loss: 26.057, mean_reward: -286.670, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99170146 -0.12856209 -0.01762588] distance:  1.0001553237744216\n",
      "\n",
      "Episode: 335, actor_loss: 1.607, critic_loss: 19.634, mean_reward: -673.056, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98615581  0.16582132 -0.16075932] distance:  1.012839354955263\n",
      "\n",
      "Episode: 336, actor_loss: -0.354, critic_loss: 16.786, mean_reward: -426.451, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.69698078 -0.71708981  8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 337, actor_loss: -0.369, critic_loss: 30.294, mean_reward: -181.355, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99714713 0.07548245 0.18270607] distance:  1.0165537404594787\n",
      "\n",
      "Episode: 338, actor_loss: -1.286, critic_loss: 12.476, mean_reward: -208.932, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98530225 -0.17082003  0.07525955] distance:  1.0028280010486073\n",
      "\n",
      "Episode: 339, actor_loss: 10.587, critic_loss: 37.928, mean_reward: -240.003, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.89777143 -0.44046164 -0.53193998] distance:  1.132678302910511\n",
      "\n",
      "Episode: 340, actor_loss: -5.137, critic_loss: 48.620, mean_reward: -254.734, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.68505466 -0.72849167  0.45073444] distance:  1.0968872026011527\n",
      "\n",
      "Episode: 341, actor_loss: -1.715, critic_loss: 40.529, mean_reward: -129.467, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.95331772 -0.30196909 -0.28221522] distance:  1.0390598794787615\n",
      "\n",
      "Episode: 342, actor_loss: -1.703, critic_loss: 48.509, mean_reward: -206.702, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99806367 -0.06220056 -5.42238141] distance:  5.513820832094182\n",
      "\n",
      "Episode: 343, actor_loss: -1.928, critic_loss: 37.793, mean_reward: -348.037, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.68500423 -0.72853909 -0.6968395 ] distance:  1.2188458860578488\n",
      "\n",
      "Episode: 344, actor_loss: -3.577, critic_loss: 20.078, mean_reward: -400.518, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.69592068  0.71811866 -5.51975642] distance:  5.609608799338598\n",
      "\n",
      "Episode: 345, actor_loss: 0.521, critic_loss: 18.647, mean_reward: -552.532, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99811712 0.06133693 0.14149905] distance:  1.0099613763882123\n",
      "\n",
      "Episode: 346, actor_loss: 1.682, critic_loss: 30.579, mean_reward: -391.572, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.98358046 0.18047016 0.26959856] distance:  1.0357042944119121\n",
      "\n",
      "Episode: 347, actor_loss: 2.222, critic_loss: 26.007, mean_reward: -370.492, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.32773793  0.94476867  4.87329869] distance:  4.974840712362977\n",
      "\n",
      "Episode: 348, actor_loss: -1.473, critic_loss: 13.499, mean_reward: -199.054, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.98683788 0.16171273 0.05929307] distance:  1.0017562920250906\n",
      "\n",
      "Episode: 349, actor_loss: 20.357, critic_loss: 87.506, mean_reward: -360.798, best_return: -128.803\n",
      "last 50 episode mean reward:  -365.0666448987904\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83589424 -0.54889054  0.55649739] distance:  1.1444165952268102\n",
      "\n",
      "Episode: 350, actor_loss: -6.024, critic_loss: 58.208, mean_reward: -323.351, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.90209545 -0.43153655  1.10988828] distance:  1.4939384199190278\n",
      "\n",
      "Episode: 351, actor_loss: -4.257, critic_loss: 51.858, mean_reward: -178.038, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.23224825 -0.97265654 -0.41071117] distance:  1.081056735235916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 352, actor_loss: -2.244, critic_loss: 15.562, mean_reward: -381.422, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99782607 0.06590244 0.28288312] distance:  1.0392414829430918\n",
      "\n",
      "Episode: 353, actor_loss: -0.986, critic_loss: 26.143, mean_reward: -232.515, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83270327  0.55371949  6.6613394 ] distance:  6.7359811885391565\n",
      "\n",
      "Episode: 354, actor_loss: -0.270, critic_loss: 19.372, mean_reward: -174.099, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99963379 -0.02706062  0.07811136] distance:  1.0030460533193208\n",
      "\n",
      "Episode: 355, actor_loss: 2.039, critic_loss: 71.094, mean_reward: -227.491, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99992993 -0.01183781  0.07240265] distance:  1.0026176457987206\n",
      "\n",
      "Episode: 356, actor_loss: 0.625, critic_loss: 5.563, mean_reward: -129.142, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99886992 -0.04752764 -0.02688472] distance:  1.0003613288197435\n",
      "\n",
      "Episode: 357, actor_loss: 1.858, critic_loss: 21.381, mean_reward: -310.190, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99053812 -0.13723785  0.01279502] distance:  1.0000818528932336\n",
      "\n",
      "Episode: 358, actor_loss: 2.616, critic_loss: 42.396, mean_reward: -223.268, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99974153  0.02273481 -0.02197102] distance:  1.0002413337247167\n",
      "\n",
      "Episode: 359, actor_loss: 19.437, critic_loss: 85.326, mean_reward: -269.706, best_return: -128.803\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.32028407 -0.94732155 -0.40472206] distance:  1.0787956007735997\n",
      "\n",
      "Episode: 360, actor_loss: -6.484, critic_loss: 35.346, mean_reward: -104.639, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.04901425 -0.99879808 -0.81573023] distance:  1.2905099014087182\n",
      "\n",
      "Episode: 361, actor_loss: -2.265, critic_loss: 31.453, mean_reward: -305.062, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.95255124 -0.30437827  0.21349691] distance:  1.0225365188085567\n",
      "\n",
      "Episode: 362, actor_loss: -6.577, critic_loss: 35.048, mean_reward: -281.503, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.40920792 -0.91244117  4.87395232] distance:  4.975481006817767\n",
      "\n",
      "Episode: 363, actor_loss: -0.781, critic_loss: 27.543, mean_reward: -205.235, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99872529 -0.05047562 -0.03655975] distance:  1.0006680845298725\n",
      "\n",
      "Episode: 364, actor_loss: 0.045, critic_loss: 6.632, mean_reward: -369.492, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99998962 0.00455707 0.09339296] distance:  1.004351654338425\n",
      "\n",
      "Episode: 365, actor_loss: 0.610, critic_loss: 8.507, mean_reward: -252.265, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998854 -0.00478696  0.12903754] distance:  1.0082909728758551\n",
      "\n",
      "Episode: 366, actor_loss: 2.117, critic_loss: 31.924, mean_reward: -249.622, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99933256e-01  1.15535154e-02 -2.39363811e-05] distance:  1.000000000286475\n",
      "\n",
      "Episode: 367, actor_loss: 2.017, critic_loss: 38.353, mean_reward: -319.049, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99804543 -0.06249257 -0.01418498] distance:  1.0001006018241263\n",
      "\n",
      "Episode: 368, actor_loss: 1.991, critic_loss: 34.870, mean_reward: -431.253, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996832 -0.00795952  0.16384735] distance:  1.0133340788182765\n",
      "\n",
      "Episode: 369, actor_loss: 20.059, critic_loss: 92.561, mean_reward: -373.350, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.54333706 0.83951464 0.97806365] distance:  1.3987882260585613\n",
      "\n",
      "Episode: 370, actor_loss: -7.105, critic_loss: 34.275, mean_reward: -378.748, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.24385495 -0.96981172 -0.29752371] distance:  1.0433217889087825\n",
      "\n",
      "Episode: 371, actor_loss: -6.694, critic_loss: 40.437, mean_reward: -128.585, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99343938 -0.11435992 -0.25255321] distance:  1.031398624552234\n",
      "\n",
      "Episode: 372, actor_loss: -1.261, critic_loss: 36.561, mean_reward: -880.076, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99581165 -0.09142839 -0.10048276] distance:  1.0050357128793015\n",
      "\n",
      "Episode: 373, actor_loss: 1.173, critic_loss: 20.270, mean_reward: -433.320, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9998509  0.01726767 0.39250157] distance:  1.0742706729904163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 374, actor_loss: 1.313, critic_loss: 28.514, mean_reward: -639.867, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999401  0.01094489 0.18410303] distance:  1.0168057460760256\n",
      "\n",
      "Episode: 375, actor_loss: 4.444, critic_loss: 54.378, mean_reward: -259.665, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99974794 -0.02245111  0.15229634] distance:  1.0115306101833097\n",
      "\n",
      "Episode: 376, actor_loss: -1.102, critic_loss: 21.473, mean_reward: -279.967, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998656 -0.00518507 -0.06791591] distance:  1.0023036320565266\n",
      "\n",
      "Episode: 377, actor_loss: -0.039, critic_loss: 29.209, mean_reward: -763.036, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99549877  0.09477443 -0.33865599] distance:  1.0557878019842932\n",
      "\n",
      "Episode: 378, actor_loss: 2.245, critic_loss: 47.759, mean_reward: -651.230, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99744174 -0.07148411  0.27195474] distance:  1.0363201150962873\n",
      "\n",
      "Episode: 379, actor_loss: 11.026, critic_loss: 90.994, mean_reward: -399.772, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.6250198  -0.7806089   0.70509951] distance:  1.2235870723932762\n",
      "\n",
      "Episode: 380, actor_loss: -10.489, critic_loss: 47.861, mean_reward: -391.915, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96416069  0.26531897  3.98440004] distance:  4.107973187407185\n",
      "\n",
      "Episode: 381, actor_loss: -5.400, critic_loss: 51.676, mean_reward: -281.273, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.28543631  0.95839768 -2.45292557] distance:  2.6489325837599336\n",
      "\n",
      "Episode: 382, actor_loss: -5.582, critic_loss: 33.515, mean_reward: -311.321, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99977858 -0.02104238  0.10064378] distance:  1.005051824535435\n",
      "\n",
      "Episode: 383, actor_loss: 4.011, critic_loss: 68.053, mean_reward: -315.748, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99590685 -0.09038552  0.22790542] distance:  1.0256416910320774\n",
      "\n",
      "Episode: 384, actor_loss: -2.116, critic_loss: 17.655, mean_reward: -230.534, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99660238 -0.08236319  0.02811952] distance:  1.0003952756979164\n",
      "\n",
      "Episode: 385, actor_loss: 1.598, critic_loss: 34.433, mean_reward: -179.515, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99058322 -0.13691193 -0.1000085 ] distance:  1.0049884079870737\n",
      "\n",
      "Episode: 386, actor_loss: 1.908, critic_loss: 46.978, mean_reward: -201.477, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9988149  0.04867032 0.03246146] distance:  1.0005267343719377\n",
      "\n",
      "Episode: 387, actor_loss: 1.741, critic_loss: 23.200, mean_reward: -174.429, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99321175 0.11632031 0.31035119] distance:  1.0470519864665224\n",
      "\n",
      "Episode: 388, actor_loss: 2.458, critic_loss: 31.654, mean_reward: -176.911, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996079 -0.00885576  0.16647248] distance:  1.0137618492370928\n",
      "\n",
      "Episode: 389, actor_loss: 18.769, critic_loss: 76.912, mean_reward: -125.413, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.57776548 -0.81620282 -0.01971025] distance:  1.0001942281468628\n",
      "\n",
      "Episode: 390, actor_loss: -6.787, critic_loss: 35.811, mean_reward: -226.737, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.13353166  0.99104455 -5.00533405] distance:  5.104250083740011\n",
      "\n",
      "Episode: 391, actor_loss: -3.477, critic_loss: 22.266, mean_reward: -226.480, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99972829 -0.02330993 -0.0704288 ] distance:  1.0024770403533259\n",
      "\n",
      "Episode: 392, actor_loss: -1.300, critic_loss: 47.438, mean_reward: -251.918, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99892238 -0.0464121  -0.11395161] distance:  1.0064715444408685\n",
      "\n",
      "Episode: 393, actor_loss: -1.549, critic_loss: 9.830, mean_reward: -178.913, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.49765857 -0.86737301  5.59076291] distance:  5.679492052227814\n",
      "\n",
      "Episode: 394, actor_loss: 1.157, critic_loss: 17.746, mean_reward: -223.239, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998665 -0.00516779 -0.08347793] distance:  1.0034782335433214\n",
      "\n",
      "Episode: 395, actor_loss: 2.663, critic_loss: 35.844, mean_reward: -332.534, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995883 -0.00907441  0.07217257] distance:  1.0026010571908095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 396, actor_loss: 1.712, critic_loss: 29.714, mean_reward: -151.235, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99970512  0.02428338 -0.08685613] distance:  1.003764906449828\n",
      "\n",
      "Episode: 397, actor_loss: 1.797, critic_loss: 36.105, mean_reward: -349.208, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99996672 0.00815897 0.07592426] distance:  1.0028781052223659\n",
      "\n",
      "Episode: 398, actor_loss: 1.138, critic_loss: 15.843, mean_reward: -315.375, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99983924  0.01793008 -0.21845844] distance:  1.023583943738746\n",
      "\n",
      "Episode: 399, actor_loss: 15.947, critic_loss: 50.727, mean_reward: -430.665, best_return: -104.639\n",
      "last 50 episode mean reward:  -304.5960225038531\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.2081242  -0.97810241  0.09269497] distance:  1.004286990025941\n",
      "\n",
      "Episode: 400, actor_loss: -5.996, critic_loss: 36.266, mean_reward: -259.195, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930421  0.03729749 -0.00481822] distance:  1.000011607555302\n",
      "\n",
      "Episode: 401, actor_loss: -1.797, critic_loss: 26.386, mean_reward: -184.003, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.61020356 -0.79224467  1.93385707] distance:  2.1771088979309825\n",
      "\n",
      "Episode: 402, actor_loss: -1.168, critic_loss: 28.983, mean_reward: -229.128, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9970453  -0.07681576  0.06849931] distance:  1.0023433322388626\n",
      "\n",
      "Episode: 403, actor_loss: 0.911, critic_loss: 33.701, mean_reward: -253.438, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.66346562 -0.74820677 -1.0747284 ] distance:  1.4680058359297254\n",
      "\n",
      "Episode: 404, actor_loss: -2.688, critic_loss: 23.886, mean_reward: -269.095, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.92323041 -0.38424681  0.98092244] distance:  1.40078864905857\n",
      "\n",
      "Episode: 405, actor_loss: -0.931, critic_loss: 8.948, mean_reward: -297.438, best_return: -104.639\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99903497 -0.04392191  0.15168928] distance:  1.0114393895346818\n",
      "\n",
      "Episode: 406, actor_loss: 1.327, critic_loss: 31.714, mean_reward: -102.763, best_return: -102.763\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930918  0.03716401 -0.11494613] distance:  1.0065846278601696\n",
      "\n",
      "Episode: 407, actor_loss: 2.046, critic_loss: 34.327, mean_reward: -177.743, best_return: -102.763\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99896304 -0.0455284  -0.03770581] distance:  1.0007106116222388\n",
      "\n",
      "Episode: 408, actor_loss: 1.124, critic_loss: 38.142, mean_reward: -253.488, best_return: -102.763\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99723125  0.07436291 -0.07520776] distance:  1.0028241160944094\n",
      "\n",
      "Episode: 409, actor_loss: 19.550, critic_loss: 73.045, mean_reward: -222.532, best_return: -102.763\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.85998033 -0.51032717  0.71438281] distance:  1.2289600490381478\n",
      "\n",
      "Episode: 410, actor_loss: -8.934, critic_loss: 70.922, mean_reward: -228.508, best_return: -102.763\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.08759452  0.99615621 -4.15105655] distance:  4.269809187475213\n",
      "\n",
      "Episode: 411, actor_loss: -5.340, critic_loss: 35.515, mean_reward: -99.222, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999672  0.00256316 -0.07297272] distance:  1.0026589741369034\n",
      "\n",
      "Episode: 412, actor_loss: -2.848, critic_loss: 28.241, mean_reward: -175.828, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.46617094 -0.88469467  7.7050029 ] distance:  7.769624805220342\n",
      "\n",
      "Episode: 413, actor_loss: -2.044, critic_loss: 12.128, mean_reward: -261.447, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9993734  -0.03539499  0.23706761] distance:  1.0277164256848985\n",
      "\n",
      "Episode: 414, actor_loss: 2.232, critic_loss: 26.732, mean_reward: -348.200, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99947956 0.03225859 0.10686049] distance:  1.0056933751228168\n",
      "\n",
      "Episode: 415, actor_loss: 2.061, critic_loss: 28.839, mean_reward: -191.851, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99934358 0.03622726 0.11842987] distance:  1.0069883984750636\n",
      "\n",
      "Episode: 416, actor_loss: 2.355, critic_loss: 32.516, mean_reward: -213.309, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99969482  0.02470344 -0.08412919] distance:  1.0035326203835158\n",
      "\n",
      "Episode: 417, actor_loss: 1.908, critic_loss: 18.660, mean_reward: -222.548, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99957699  0.02908341 -0.01608526] distance:  1.000129359378928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 418, actor_loss: 1.838, critic_loss: 36.827, mean_reward: -256.158, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997726  0.00674315 -0.12483438] distance:  1.0077616890368584\n",
      "\n",
      "Episode: 419, actor_loss: 14.436, critic_loss: 53.962, mean_reward: -203.011, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9750548   0.22196429 -0.51245848] distance:  1.1236608425442254\n",
      "\n",
      "Episode: 420, actor_loss: -7.333, critic_loss: 46.545, mean_reward: -124.331, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.04882452  0.99880737 -1.51732421] distance:  1.8172156646985296\n",
      "\n",
      "Episode: 421, actor_loss: -2.010, critic_loss: 26.253, mean_reward: -222.745, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.60647468 -0.7951028   1.38070371] distance:  1.7047999115860002\n",
      "\n",
      "Episode: 422, actor_loss: -3.016, critic_loss: 14.489, mean_reward: -228.667, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999402 -0.00345872 -0.10026289] distance:  1.0050137545435978\n",
      "\n",
      "Episode: 423, actor_loss: -1.196, critic_loss: 18.202, mean_reward: -201.167, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9911284  -0.13290787  0.18065695] distance:  1.016187450542838\n",
      "\n",
      "Episode: 424, actor_loss: -2.145, critic_loss: 8.317, mean_reward: -275.162, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9972446  -0.07418361  0.06601663] distance:  1.0021767288493615\n",
      "\n",
      "Episode: 425, actor_loss: 0.972, critic_loss: 8.137, mean_reward: -268.468, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99789862 -0.06479458  0.02567886] distance:  1.0003296476039092\n",
      "\n",
      "Episode: 426, actor_loss: 1.686, critic_loss: 30.400, mean_reward: -226.836, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99584024 -0.09111654 -0.08852424] distance:  1.0039106236290314\n",
      "\n",
      "Episode: 427, actor_loss: 2.066, critic_loss: 24.638, mean_reward: -180.413, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996232 -0.00868051  0.08368221] distance:  1.003495248146785\n",
      "\n",
      "Episode: 428, actor_loss: 1.829, critic_loss: 35.311, mean_reward: -228.970, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99974203  0.02271265 -0.02784432] distance:  1.0003875780380371\n",
      "\n",
      "Episode: 429, actor_loss: 13.452, critic_loss: 44.520, mean_reward: -322.947, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.683491    0.72995894 -0.56456676] distance:  1.1483621509450945\n",
      "\n",
      "Episode: 430, actor_loss: -5.335, critic_loss: 26.633, mean_reward: -227.272, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.470577   -0.88235893 -1.07721345] distance:  1.469826116014365\n",
      "\n",
      "Episode: 431, actor_loss: -3.205, critic_loss: 26.082, mean_reward: -153.154, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.67090871  0.74153995 -1.88166099] distance:  2.1308796479590506\n",
      "\n",
      "Episode: 432, actor_loss: -6.233, critic_loss: 49.057, mean_reward: -153.464, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.89396887  0.44812908 -1.76885531] distance:  2.0319569650702993\n",
      "\n",
      "Episode: 433, actor_loss: -2.525, critic_loss: 18.945, mean_reward: -220.590, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.09371347 -0.99559921 -3.59540198] distance:  3.731878266803458\n",
      "\n",
      "Episode: 434, actor_loss: -1.831, critic_loss: 15.418, mean_reward: -104.063, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96392357  0.26617918 -0.6127429 ] distance:  1.1727974534958865\n",
      "\n",
      "Episode: 435, actor_loss: 0.760, critic_loss: 19.372, mean_reward: -124.538, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99957544 0.0291367  0.07613149] distance:  1.0028938147784818\n",
      "\n",
      "Episode: 436, actor_loss: 1.831, critic_loss: 29.021, mean_reward: -286.536, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99967017  0.02568169 -0.06454222] distance:  1.0020806846647914\n",
      "\n",
      "Episode: 437, actor_loss: 1.874, critic_loss: 24.199, mean_reward: -281.603, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99978532  0.02072012 -0.1634183 ] distance:  1.0132647932533976\n",
      "\n",
      "Episode: 438, actor_loss: 2.088, critic_loss: 14.185, mean_reward: -293.203, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99874075 0.05016878 0.16341695] distance:  1.0132645747468416\n",
      "\n",
      "Episode: 439, actor_loss: 17.766, critic_loss: 47.138, mean_reward: -224.424, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.37358728 -0.92759503  0.81786397] distance:  1.2918596971698146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 440, actor_loss: -5.065, critic_loss: 26.545, mean_reward: -103.341, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.14154271  0.98993215 -2.27613607] distance:  2.4861205576381065\n",
      "\n",
      "Episode: 441, actor_loss: -8.530, critic_loss: 44.639, mean_reward: -306.959, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.23937674 -0.97092676  5.51039025] distance:  5.600392907257322\n",
      "\n",
      "Episode: 442, actor_loss: -2.397, critic_loss: 26.754, mean_reward: -259.091, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.83159802 -0.55537801 -0.491269  ] distance:  1.1141567368562098\n",
      "\n",
      "Episode: 443, actor_loss: 1.223, critic_loss: 47.326, mean_reward: -152.839, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.23482638  0.97203733 -4.49298593] distance:  4.602925430952978\n",
      "\n",
      "Episode: 444, actor_loss: -2.863, critic_loss: 12.431, mean_reward: -250.234, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9996573  -0.02617803 -0.01094018] distance:  1.00005984200247\n",
      "\n",
      "Episode: 445, actor_loss: 0.995, critic_loss: 27.886, mean_reward: -366.078, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99865785  0.05179293 -0.22087007] distance:  1.024101356777565\n",
      "\n",
      "Episode: 446, actor_loss: 2.407, critic_loss: 21.237, mean_reward: -156.996, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988163 -0.01538576  0.040959  ] distance:  1.0008384681256746\n",
      "\n",
      "Episode: 447, actor_loss: 2.504, critic_loss: 23.153, mean_reward: -229.410, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99989052e-01  4.67938229e-03 -8.58321100e-04] distance:  1.0000003683574876\n",
      "\n",
      "Episode: 448, actor_loss: 2.337, critic_loss: 12.843, mean_reward: -197.681, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997759  0.00669477 -0.09468998] distance:  1.0044730922487495\n",
      "\n",
      "Episode: 449, actor_loss: 17.160, critic_loss: 42.508, mean_reward: -244.132, best_return: -99.222\n",
      "last 50 episode mean reward:  -221.28440124337158\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99723971 -0.07424933  0.20867986] distance:  1.0215416213037674\n",
      "\n",
      "Episode: 450, actor_loss: -11.852, critic_loss: 62.736, mean_reward: -170.660, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.04004093  0.99919804 -3.78814592] distance:  3.917913922980082\n",
      "\n",
      "Episode: 451, actor_loss: -4.753, critic_loss: 32.533, mean_reward: -202.156, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.21849644 -0.97583775  5.39454401] distance:  5.486447396099675\n",
      "\n",
      "Episode: 452, actor_loss: -3.743, critic_loss: 24.282, mean_reward: -244.663, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99732752  0.07306043 -0.20791889] distance:  1.021386441845265\n",
      "\n",
      "Episode: 453, actor_loss: 0.229, critic_loss: 13.390, mean_reward: -260.245, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99620728  0.08701187 -0.05553213] distance:  1.0015407217128658\n",
      "\n",
      "Episode: 454, actor_loss: 1.374, critic_loss: 16.777, mean_reward: -203.120, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99733699  0.07293096 -0.18068037] distance:  1.0161916132916504\n",
      "\n",
      "Episode: 455, actor_loss: 2.017, critic_loss: 26.727, mean_reward: -258.305, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99927032  0.03819462 -0.0037348 ] distance:  1.0000069743334943\n",
      "\n",
      "Episode: 456, actor_loss: 2.006, critic_loss: 14.898, mean_reward: -183.088, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99765825  0.06839601 -0.12585879] distance:  1.0078890985076088\n",
      "\n",
      "Episode: 457, actor_loss: 2.233, critic_loss: 17.085, mean_reward: -128.515, best_return: -99.222\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999762 0.002184   0.02259862] distance:  1.000255316232831\n",
      "\n",
      "Episode: 458, actor_loss: 1.914, critic_loss: 18.709, mean_reward: -53.935, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9982777  -0.05866544 -0.01119074] distance:  1.0000626143912097\n",
      "\n",
      "Episode: 459, actor_loss: 16.740, critic_loss: 59.280, mean_reward: -157.087, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.60164617 0.79876272 0.04220824] distance:  1.0008903713999822\n",
      "\n",
      "Episode: 460, actor_loss: -10.604, critic_loss: 36.501, mean_reward: -203.631, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.19783289  0.98023576 -2.15767422] distance:  2.3781417215474736\n",
      "\n",
      "Episode: 461, actor_loss: -5.921, critic_loss: 32.787, mean_reward: -196.839, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.47219816  0.88149243 -1.1353699 ] distance:  1.5129655691641266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 462, actor_loss: 0.318, critic_loss: 36.394, mean_reward: -209.253, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.43110919 -0.90229976 -4.02497552] distance:  4.147339861108415\n",
      "\n",
      "Episode: 463, actor_loss: -0.355, critic_loss: 35.675, mean_reward: -205.255, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99737695 0.07238244 0.05632888] distance:  1.0015852146512112\n",
      "\n",
      "Episode: 464, actor_loss: 0.273, critic_loss: 5.837, mean_reward: -188.818, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99728007  0.07370525 -0.02161984] distance:  1.0002336814028554\n",
      "\n",
      "Episode: 465, actor_loss: 1.485, critic_loss: 24.457, mean_reward: -295.783, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98951032  0.1444622  -0.25277348] distance:  1.031452584399224\n",
      "\n",
      "Episode: 466, actor_loss: 1.753, critic_loss: 30.559, mean_reward: -176.209, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99923113 -0.03920646  0.32058319] distance:  1.0501302689150103\n",
      "\n",
      "Episode: 467, actor_loss: 2.324, critic_loss: 25.255, mean_reward: -509.710, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998692  0.00511471 -0.13835178] distance:  1.0095252417619907\n",
      "\n",
      "Episode: 468, actor_loss: 2.269, critic_loss: 16.799, mean_reward: -312.586, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99981346 -0.01931448 -0.32909306] distance:  1.0527593462954574\n",
      "\n",
      "Episode: 469, actor_loss: 17.681, critic_loss: 50.371, mean_reward: -205.690, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98298185 -0.18370269  0.37275651] distance:  1.0672147931362228\n",
      "\n",
      "Episode: 470, actor_loss: -7.818, critic_loss: 34.760, mean_reward: -450.303, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.40433721 -0.91460998 -0.35804801] distance:  1.062166832161409\n",
      "\n",
      "Episode: 471, actor_loss: -1.590, critic_loss: 27.598, mean_reward: -411.252, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.2270486  -0.97388343  4.42321637] distance:  4.534847631161151\n",
      "\n",
      "Episode: 472, actor_loss: -4.854, critic_loss: 22.561, mean_reward: -314.266, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.56849605 -0.82268599 -0.69742027] distance:  1.219178016294421\n",
      "\n",
      "Episode: 473, actor_loss: -1.015, critic_loss: 9.272, mean_reward: -312.107, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.00649947  0.99997888 -3.7131909 ] distance:  3.8454891322762585\n",
      "\n",
      "Episode: 474, actor_loss: -1.201, critic_loss: 10.502, mean_reward: -444.617, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.05027243 -0.99873554  3.45039298] distance:  3.5923824523935206\n",
      "\n",
      "Episode: 475, actor_loss: -1.570, critic_loss: 13.045, mean_reward: -314.637, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99881667 -0.04863384 -0.14554825] distance:  1.0105366365136914\n",
      "\n",
      "Episode: 476, actor_loss: -0.448, critic_loss: 18.310, mean_reward: -286.750, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.17392323 -0.98475921  3.32308238] distance:  3.47028478963223\n",
      "\n",
      "Episode: 477, actor_loss: -4.647, critic_loss: 50.711, mean_reward: -230.096, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9825581   0.18595586  6.14424974] distance:  6.225094769287487\n",
      "\n",
      "Episode: 478, actor_loss: 1.690, critic_loss: 28.696, mean_reward: -182.704, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999416  0.00341831 -0.08582292] distance:  1.003676030089496\n",
      "\n",
      "Episode: 479, actor_loss: 13.547, critic_loss: 123.981, mean_reward: -131.206, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.03316519  0.99944988 -0.29747274] distance:  1.0433072559698249\n",
      "\n",
      "Episode: 480, actor_loss: -6.428, critic_loss: 42.418, mean_reward: -208.886, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.37632595 -0.92648733  0.04853964] distance:  1.0011773551816048\n",
      "\n",
      "Episode: 481, actor_loss: -3.164, critic_loss: 48.560, mean_reward: -180.221, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.12227179  0.99249665 -2.19212342] distance:  2.409440827880802\n",
      "\n",
      "Episode: 482, actor_loss: -4.928, critic_loss: 21.691, mean_reward: -210.802, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.69747496 -0.71660915  6.35534563] distance:  6.43353853286812\n",
      "\n",
      "Episode: 483, actor_loss: -1.157, critic_loss: 21.279, mean_reward: -234.817, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99504347 -0.09944085 -0.06350995] distance:  1.0020147274799998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 484, actor_loss: 1.695, critic_loss: 21.549, mean_reward: -207.466, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99751188 -0.07049854 -0.01795439] distance:  1.0001611670898325\n",
      "\n",
      "Episode: 485, actor_loss: -1.323, critic_loss: 18.209, mean_reward: -154.251, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99910873 -0.04221079  0.12588909] distance:  1.0078928821600932\n",
      "\n",
      "Episode: 486, actor_loss: 2.848, critic_loss: 40.010, mean_reward: -283.725, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99040468 -0.13819759  0.13839792] distance:  1.0095315666104199\n",
      "\n",
      "Episode: 487, actor_loss: 3.085, critic_loss: 31.068, mean_reward: -151.058, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99871971 -0.05058606 -0.14447497] distance:  1.0103826091798727\n",
      "\n",
      "Episode: 488, actor_loss: 2.710, critic_loss: 26.032, mean_reward: -185.074, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99902313 0.04419029 0.1449459 ] distance:  1.0104500558035725\n",
      "\n",
      "Episode: 489, actor_loss: 26.576, critic_loss: 101.670, mean_reward: -268.376, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.68528851  0.72827169 -0.57405114] distance:  1.153054513010865\n",
      "\n",
      "Episode: 490, actor_loss: -7.903, critic_loss: 32.282, mean_reward: -241.717, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.35027593 -0.93664656  0.13740663] distance:  1.0093961468851569\n",
      "\n",
      "Episode: 491, actor_loss: -4.631, critic_loss: 39.918, mean_reward: -308.415, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.44470607 -0.89567656  2.41760951] distance:  2.6162636956798204\n",
      "\n",
      "Episode: 492, actor_loss: -2.161, critic_loss: 13.986, mean_reward: -311.341, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.35971322  0.93306291 -3.57834309] distance:  3.715446041986816\n",
      "\n",
      "Episode: 493, actor_loss: -2.234, critic_loss: 25.806, mean_reward: -320.790, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99945102 -0.0331309  -0.01981495] distance:  1.0001962968386786\n",
      "\n",
      "Episode: 494, actor_loss: -1.775, critic_loss: 10.976, mean_reward: -262.106, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99960414 -0.02813484  0.0896779 ] distance:  1.0040130104251666\n",
      "\n",
      "Episode: 495, actor_loss: 1.491, critic_loss: 31.337, mean_reward: -214.064, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99964513  0.02663862 -0.20006925] distance:  1.0198174868825736\n",
      "\n",
      "Episode: 496, actor_loss: 0.810, critic_loss: 2.492, mean_reward: -263.235, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994725 -0.01027105 -0.04723304] distance:  1.0011148588044614\n",
      "\n",
      "Episode: 497, actor_loss: 2.375, critic_loss: 36.278, mean_reward: -354.629, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997483  0.00709528 -0.13736371] distance:  1.0093903049531967\n",
      "\n",
      "Episode: 498, actor_loss: 2.054, critic_loss: 19.395, mean_reward: -243.084, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998808 -0.00488306 -0.18755227] distance:  1.0174359208024022\n",
      "\n",
      "Episode: 499, actor_loss: 19.458, critic_loss: 62.623, mean_reward: -181.184, best_return: -53.935\n",
      "last 50 episode mean reward:  -244.57453743155685\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.76068247 0.64912416 0.40010385] distance:  1.0770715352552414\n",
      "\n",
      "Episode: 500, actor_loss: -7.823, critic_loss: 37.125, mean_reward: -239.176, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.22687606  0.97392364 -0.85264312] distance:  1.3141538287003471\n",
      "\n",
      "Episode: 501, actor_loss: -3.756, critic_loss: 36.935, mean_reward: -295.477, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.63318796  0.77399806 -2.28463664] distance:  2.493905483593667\n",
      "\n",
      "Episode: 502, actor_loss: -4.766, critic_loss: 25.735, mean_reward: -377.378, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.52246815  0.8526588   4.1051925 ] distance:  4.225234371345198\n",
      "\n",
      "Episode: 503, actor_loss: -0.928, critic_loss: 28.699, mean_reward: -286.112, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.05306245 -0.9985912   3.46895392] distance:  3.6102134686891594\n",
      "\n",
      "Episode: 504, actor_loss: -4.908, critic_loss: 34.403, mean_reward: -169.803, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.53194393 -0.84677958  1.67968558] distance:  1.9548257352318812\n",
      "\n",
      "Episode: 505, actor_loss: -0.994, critic_loss: 13.079, mean_reward: -131.248, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9998492  -0.01736623  0.02043849] distance:  1.0002088441288366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 506, actor_loss: 1.010, critic_loss: 10.722, mean_reward: -129.372, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99882895 -0.04838114 -0.0265343 ] distance:  1.0003519724973207\n",
      "\n",
      "Episode: 507, actor_loss: 2.543, critic_loss: 35.037, mean_reward: -281.514, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99980039 -0.01997938 -0.02525386] distance:  1.0003188279000033\n",
      "\n",
      "Episode: 508, actor_loss: 2.657, critic_loss: 19.141, mean_reward: -137.866, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99981771 -0.0190929  -0.09081361] distance:  1.0041150886029946\n",
      "\n",
      "Episode: 509, actor_loss: 22.707, critic_loss: 82.559, mean_reward: -366.923, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.79996119 -0.60005174 -0.85878538] distance:  1.3181473064821874\n",
      "\n",
      "Episode: 510, actor_loss: -6.179, critic_loss: 35.420, mean_reward: -232.912, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.07280826 -0.99734596  0.08540157] distance:  1.0036400891352673\n",
      "\n",
      "Episode: 511, actor_loss: -6.143, critic_loss: 40.257, mean_reward: -256.270, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930251 -0.03734281  0.00189811] distance:  1.0000018014084142\n",
      "\n",
      "Episode: 512, actor_loss: -2.407, critic_loss: 20.767, mean_reward: -78.707, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99970276 -0.02438024 -0.2500717 ] distance:  1.0307937996672527\n",
      "\n",
      "Episode: 513, actor_loss: -1.732, critic_loss: 9.338, mean_reward: -235.327, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99960809 -0.02799416 -0.14555274] distance:  1.0105372825165873\n",
      "\n",
      "Episode: 514, actor_loss: 2.463, critic_loss: 38.374, mean_reward: -207.596, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99953653 0.03044224 0.02245667] distance:  1.0002521191385947\n",
      "\n",
      "Episode: 515, actor_loss: 2.284, critic_loss: 19.714, mean_reward: -176.683, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9998046   0.01976784 -0.05601947] distance:  1.0015678614986707\n",
      "\n",
      "Episode: 516, actor_loss: 2.054, critic_loss: 20.382, mean_reward: -204.598, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999877  -0.00496068 -0.02503054] distance:  1.000313214805663\n",
      "\n",
      "Episode: 517, actor_loss: 2.078, critic_loss: 15.987, mean_reward: -232.456, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999622  0.00869432 0.04809428] distance:  1.0011558618858043\n",
      "\n",
      "Episode: 518, actor_loss: 1.966, critic_loss: 15.187, mean_reward: -162.293, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99969615  0.02464981 -0.13168582] distance:  1.0086333109112062\n",
      "\n",
      "Episode: 519, actor_loss: 15.151, critic_loss: 44.569, mean_reward: -274.033, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9405028  -0.33978595 -0.10005031] distance:  1.004992569577499\n",
      "\n",
      "Episode: 520, actor_loss: -3.307, critic_loss: 19.833, mean_reward: -265.074, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.02661315  0.99964581 -3.07878999] distance:  3.237120293920511\n",
      "\n",
      "Episode: 521, actor_loss: -5.730, critic_loss: 19.740, mean_reward: -208.630, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99842326 -0.05613367  5.9967149 ] distance:  6.0795221527115535\n",
      "\n",
      "Episode: 522, actor_loss: -1.469, critic_loss: 21.061, mean_reward: -154.847, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9617974   0.27376223 -0.70090939] distance:  1.2211772898005475\n",
      "\n",
      "Episode: 523, actor_loss: -2.317, critic_loss: 12.081, mean_reward: -290.587, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999811   0.0061479  -0.07341402] distance:  1.0026911876922124\n",
      "\n",
      "Episode: 524, actor_loss: -1.359, critic_loss: 5.167, mean_reward: -188.499, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999939  0.00349392 0.00942876] distance:  1.0000444497401155\n",
      "\n",
      "Episode: 525, actor_loss: 0.054, critic_loss: 9.896, mean_reward: -315.668, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999797 -0.00201272 -0.01914998] distance:  1.0001833439932748\n",
      "\n",
      "Episode: 526, actor_loss: 1.265, critic_loss: 2.443, mean_reward: -125.936, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996011 -0.00893141 -0.07579465] distance:  1.0028683011327715\n",
      "\n",
      "Episode: 527, actor_loss: 2.253, critic_loss: 29.054, mean_reward: -256.853, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99943863 0.03350273 0.14548154] distance:  1.0105270302312537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 528, actor_loss: 2.283, critic_loss: 22.838, mean_reward: -126.524, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996902  0.00787158 -0.16374139] distance:  1.013316950268426\n",
      "\n",
      "Episode: 529, actor_loss: 17.787, critic_loss: 48.561, mean_reward: -154.057, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.03546763 -0.99937083  0.68785717] distance:  1.2137328722640852\n",
      "\n",
      "Episode: 530, actor_loss: -4.229, critic_loss: 18.068, mean_reward: -158.461, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.5902121  -0.80724821  1.32741637] distance:  1.6619368859255028\n",
      "\n",
      "Episode: 531, actor_loss: -5.976, critic_loss: 19.246, mean_reward: -132.702, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99744156 -0.0714866   0.33474043] distance:  1.0545383606007588\n",
      "\n",
      "Episode: 532, actor_loss: 3.013, critic_loss: 18.581, mean_reward: -131.925, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.57549438 -0.81780573  1.36403559] distance:  1.6913287903773502\n",
      "\n",
      "Episode: 533, actor_loss: -7.695, critic_loss: 15.605, mean_reward: -397.272, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99984444  0.01763811 -0.04151759] distance:  1.0008614840273498\n",
      "\n",
      "Episode: 534, actor_loss: 0.780, critic_loss: 38.943, mean_reward: -485.499, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99899916  0.04472891 -0.02694079] distance:  1.0003628373388336\n",
      "\n",
      "Episode: 535, actor_loss: 2.732, critic_loss: 32.861, mean_reward: -400.018, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9997582  0.02198979 0.09510945] distance:  1.004512721789317\n",
      "\n",
      "Episode: 536, actor_loss: 1.208, critic_loss: 10.883, mean_reward: -259.994, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99938189  0.0351544  -0.10509398] distance:  1.005507207929221\n",
      "\n",
      "Episode: 537, actor_loss: 3.940, critic_loss: 73.637, mean_reward: -295.429, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9998721   0.01599314 -0.03462896] distance:  1.0005994026411262\n",
      "\n",
      "Episode: 538, actor_loss: 0.744, critic_loss: 4.500, mean_reward: -597.054, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.25500055 -0.96694091  0.99899638] distance:  1.413504072122619\n",
      "\n",
      "Episode: 539, actor_loss: 16.117, critic_loss: 84.355, mean_reward: -289.780, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.18163978  0.98336514 -0.36927385] distance:  1.0660033644477873\n",
      "\n",
      "Episode: 540, actor_loss: -6.161, critic_loss: 25.508, mean_reward: -344.322, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.53378152  0.84562243 -1.33500403] distance:  1.6680035286620751\n",
      "\n",
      "Episode: 541, actor_loss: -6.774, critic_loss: 39.309, mean_reward: -294.794, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.88477931 -0.46601027 -4.51024668] distance:  4.619775435935934\n",
      "\n",
      "Episode: 542, actor_loss: -1.531, critic_loss: 62.056, mean_reward: -342.922, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.06114054  0.99812917  2.8308491 ] distance:  3.002283568028422\n",
      "\n",
      "Episode: 543, actor_loss: -0.527, critic_loss: 32.621, mean_reward: -417.071, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.0639821  -0.99795105  2.89268869] distance:  3.060661340315492\n",
      "\n",
      "Episode: 544, actor_loss: -5.005, critic_loss: 9.602, mean_reward: -104.733, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.97871225 -0.20523727  5.86464818] distance:  5.949293935023778\n",
      "\n",
      "Episode: 545, actor_loss: 0.871, critic_loss: 16.892, mean_reward: -299.098, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.87847166 0.47779445 0.06114195] distance:  1.0018674255671838\n",
      "\n",
      "Episode: 546, actor_loss: -6.687, critic_loss: 54.312, mean_reward: -324.161, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.58804681  0.8088269   4.69529803] distance:  4.800606582659527\n",
      "\n",
      "Episode: 547, actor_loss: 0.576, critic_loss: 62.116, mean_reward: -134.171, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.41964106 -0.90769013 -1.11711976] distance:  1.4993186977268402\n",
      "\n",
      "Episode: 548, actor_loss: -4.606, critic_loss: 24.520, mean_reward: -235.516, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99981981 -0.01898307  0.05097739] distance:  1.0012985043325158\n",
      "\n",
      "Episode: 549, actor_loss: 22.127, critic_loss: 178.919, mean_reward: -157.984, best_return: -53.935\n",
      "last 50 episode mean reward:  -247.3064658347969\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.43631144  0.89979571 -0.27972632] distance:  1.038386640505803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 550, actor_loss: -6.378, critic_loss: 46.400, mean_reward: -255.937, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.55987226 -0.82857893  1.07675267] distance:  1.469488452733508\n",
      "\n",
      "Episode: 551, actor_loss: -5.493, critic_loss: 29.349, mean_reward: -249.912, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9704569   0.24127455 -5.1256571 ] distance:  5.222294583346672\n",
      "\n",
      "Episode: 552, actor_loss: -3.805, critic_loss: 27.696, mean_reward: -132.053, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99981097  0.01944275 -0.05995247] distance:  1.001795537338203\n",
      "\n",
      "Episode: 553, actor_loss: -2.704, critic_loss: 10.118, mean_reward: -240.361, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99991563 0.0129899  0.10481706] distance:  1.0054783016650124\n",
      "\n",
      "Episode: 554, actor_loss: 1.920, critic_loss: 6.088, mean_reward: -304.451, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999438 0.00335283 0.05557258] distance:  1.001542965358828\n",
      "\n",
      "Episode: 555, actor_loss: 2.749, critic_loss: 9.113, mean_reward: -162.875, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99937787 -0.03526859 -0.24733832] distance:  1.0301340913331396\n",
      "\n",
      "Episode: 556, actor_loss: 2.515, critic_loss: 25.260, mean_reward: -310.290, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99997504 0.00706495 0.12616587] distance:  1.0079274910524656\n",
      "\n",
      "Episode: 557, actor_loss: 2.295, critic_loss: 19.634, mean_reward: -207.560, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99992924 -0.01189618 -0.01860218] distance:  1.000173005544482\n",
      "\n",
      "Episode: 558, actor_loss: 2.296, critic_loss: 11.136, mean_reward: -471.109, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [9.99999953e-01 3.05651107e-04 4.31809556e-02] distance:  1.000931863277469\n",
      "\n",
      "Episode: 559, actor_loss: 20.620, critic_loss: 61.302, mean_reward: -183.270, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.38803306 0.92164545 0.64213293] distance:  1.1884168875160226\n",
      "\n",
      "Episode: 560, actor_loss: -3.270, critic_loss: 47.022, mean_reward: -254.680, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.72220693 -0.69167706  1.64889885] distance:  1.928436518577968\n",
      "\n",
      "Episode: 561, actor_loss: -7.485, critic_loss: 61.547, mean_reward: -229.371, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99931207  0.03708622 -0.06486428] distance:  1.0021014791994962\n",
      "\n",
      "Episode: 562, actor_loss: -0.010, critic_loss: 38.491, mean_reward: -259.693, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9915471  -0.12974722  0.50832146] distance:  1.1217801497382331\n",
      "\n",
      "Episode: 563, actor_loss: -3.545, critic_loss: 17.433, mean_reward: -366.533, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99973484  0.02302697 -0.0482981 ] distance:  1.0011656738517545\n",
      "\n",
      "Episode: 564, actor_loss: 1.016, critic_loss: 20.578, mean_reward: -210.533, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99945767  0.0329296  -0.00143494] distance:  1.0000010295269741\n",
      "\n",
      "Episode: 565, actor_loss: 0.440, critic_loss: 15.161, mean_reward: -203.475, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99960384  0.02814556 -0.0635385 ] distance:  1.0020165370015557\n",
      "\n",
      "Episode: 566, actor_loss: 0.771, critic_loss: 11.612, mean_reward: -328.529, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99135077 -0.13123887 -3.82344336] distance:  3.9520525204748225\n",
      "\n",
      "Episode: 567, actor_loss: 1.227, critic_loss: 3.936, mean_reward: -245.380, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99947813  0.03230277 -0.1211585 ] distance:  1.0073129513798718\n",
      "\n",
      "Episode: 568, actor_loss: 1.527, critic_loss: 11.473, mean_reward: -364.813, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99889087 0.04708532 0.0030928 ] distance:  1.0000047827092662\n",
      "\n",
      "Episode: 569, actor_loss: 23.100, critic_loss: 95.198, mean_reward: -234.981, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.81104608  0.58498227 -0.96943126] distance:  1.3927659423645413\n",
      "\n",
      "Episode: 570, actor_loss: -10.413, critic_loss: 49.809, mean_reward: -285.842, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96076958  0.27734781 -0.46423356] distance:  1.1025029695917858\n",
      "\n",
      "Episode: 571, actor_loss: -4.620, critic_loss: 32.701, mean_reward: -128.793, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99779613  0.06635425 -0.0588941 ] distance:  1.001732756339848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 572, actor_loss: -3.237, critic_loss: 20.043, mean_reward: -299.487, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99892307 0.04639719 0.21565754] distance:  1.0229898214885214\n",
      "\n",
      "Episode: 573, actor_loss: 0.335, critic_loss: 12.062, mean_reward: -159.045, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99601724 0.08916086 0.06653508] distance:  1.002211014065823\n",
      "\n",
      "Episode: 574, actor_loss: -0.896, critic_loss: 5.760, mean_reward: -283.529, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.45121981  0.89241285  0.73850171] distance:  1.2431350599061068\n",
      "\n",
      "Episode: 575, actor_loss: 0.527, critic_loss: 1.901, mean_reward: -343.033, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99858362 0.05320476 0.06193894] distance:  1.001916379689765\n",
      "\n",
      "Episode: 576, actor_loss: 1.730, critic_loss: 29.664, mean_reward: -182.524, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99643553 0.08435781 0.20001324] distance:  1.0198064985033464\n",
      "\n",
      "Episode: 577, actor_loss: 1.744, critic_loss: 34.752, mean_reward: -158.042, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99785033 0.06553407 0.23860167] distance:  1.0280713784240443\n",
      "\n",
      "Episode: 578, actor_loss: 1.203, critic_loss: 32.114, mean_reward: -347.412, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99830286 0.05823571 0.02739259] distance:  1.0003751065833553\n",
      "\n",
      "Episode: 579, actor_loss: 13.890, critic_loss: 45.380, mean_reward: -185.597, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99581116 -0.0914338   0.55746769] distance:  1.144888739108607\n",
      "\n",
      "Episode: 580, actor_loss: -11.668, critic_loss: 44.754, mean_reward: -238.194, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.72106207 -0.69287047  1.6547854 ] distance:  1.9334721943220385\n",
      "\n",
      "Episode: 581, actor_loss: -11.025, critic_loss: 53.136, mean_reward: -134.376, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99953098 -0.03062384 -1.20736517] distance:  1.5677151029743603\n",
      "\n",
      "Episode: 582, actor_loss: -2.848, critic_loss: 23.354, mean_reward: -212.679, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99934801 0.03610475 0.17068279] distance:  1.014461736190263\n",
      "\n",
      "Episode: 583, actor_loss: -0.898, critic_loss: 7.318, mean_reward: -164.483, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999727 0.00233818 0.04525894] distance:  1.0010236619478265\n",
      "\n",
      "Episode: 584, actor_loss: -3.309, critic_loss: 5.382, mean_reward: -311.411, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9983723   0.05703284 -0.07299156] distance:  1.0026603450196498\n",
      "\n",
      "Episode: 585, actor_loss: 1.693, critic_loss: 22.051, mean_reward: -159.653, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99903377  0.0439491  -0.0100151 ] distance:  1.0000501498367131\n",
      "\n",
      "Episode: 586, actor_loss: 2.053, critic_loss: 17.843, mean_reward: -331.900, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99895662  0.04566926 -0.12391072] distance:  1.0076476894034279\n",
      "\n",
      "Episode: 587, actor_loss: 1.384, critic_loss: 23.394, mean_reward: -270.516, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99960185 0.0282159  0.03102026] distance:  1.0004810125381232\n",
      "\n",
      "Episode: 588, actor_loss: 1.870, critic_loss: 25.849, mean_reward: -128.114, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99948991 0.03193621 0.08436462] distance:  1.0035523845291459\n",
      "\n",
      "Episode: 589, actor_loss: 13.501, critic_loss: 41.600, mean_reward: -278.648, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.06636687 -0.99779529  0.68574955] distance:  1.2125396656387861\n",
      "\n",
      "Episode: 590, actor_loss: -11.354, critic_loss: 35.340, mean_reward: -210.480, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.69517609 -0.71883948  3.6838457 ] distance:  3.8171611346874132\n",
      "\n",
      "Episode: 591, actor_loss: -3.460, critic_loss: 28.019, mean_reward: -208.991, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.16611424 -0.98610652  3.62334437] distance:  3.7588062486708167\n",
      "\n",
      "Episode: 592, actor_loss: -3.729, critic_loss: 19.032, mean_reward: -261.428, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99935559  0.03589422 -0.09791859] distance:  1.0047825887167245\n",
      "\n",
      "Episode: 593, actor_loss: -1.307, critic_loss: 4.298, mean_reward: -264.041, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99759166  0.06936055 -0.00455103] distance:  1.0000103558870075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 594, actor_loss: 1.499, critic_loss: 17.102, mean_reward: -157.184, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99944636  0.03327117 -0.03660483] distance:  1.000669732412513\n",
      "\n",
      "Episode: 595, actor_loss: 1.787, critic_loss: 28.940, mean_reward: -411.248, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99983432  0.01820272 -0.00645989] distance:  1.000020864868859\n",
      "\n",
      "Episode: 596, actor_loss: 1.754, critic_loss: 16.085, mean_reward: -259.990, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99897925  0.0451714  -0.08295327] distance:  1.003434723748374\n",
      "\n",
      "Episode: 597, actor_loss: 1.733, critic_loss: 14.780, mean_reward: -132.176, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99941465 0.03421062 0.04530213] distance:  1.0010256155653634\n",
      "\n",
      "Episode: 598, actor_loss: 1.505, critic_loss: 22.933, mean_reward: -268.154, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9992969  0.03749286 0.01385357] distance:  1.0000959561634748\n",
      "\n",
      "Episode: 599, actor_loss: 14.626, critic_loss: 37.103, mean_reward: -443.720, best_return: -53.935\n",
      "last 50 episode mean reward:  -248.72994636039658\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.49365884 -0.86965565 -0.25387353] distance:  1.031722719453801\n",
      "\n",
      "Episode: 600, actor_loss: -8.276, critic_loss: 20.449, mean_reward: -103.827, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.87773083 -0.47915404  2.39392701] distance:  2.5943952111104585\n",
      "\n",
      "Episode: 601, actor_loss: -4.216, critic_loss: 15.191, mean_reward: -330.468, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.48461306  0.87472864 -1.73578506] distance:  2.003234823477217\n",
      "\n",
      "Episode: 602, actor_loss: -1.660, critic_loss: 10.106, mean_reward: -469.730, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999894  -0.00460458  0.31370409] distance:  1.0480506922294808\n",
      "\n",
      "Episode: 603, actor_loss: -1.450, critic_loss: 7.183, mean_reward: -284.102, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99990203 -0.0139975  -0.00658011] distance:  1.0000216486855167\n",
      "\n",
      "Episode: 604, actor_loss: 0.678, critic_loss: 3.721, mean_reward: -758.596, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99986794 -0.01625151 -0.00688814] distance:  1.0000237229628839\n",
      "\n",
      "Episode: 605, actor_loss: 1.948, critic_loss: 27.395, mean_reward: -573.041, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99992946 -0.01187717 -0.07228119] distance:  1.002608882184019\n",
      "\n",
      "Episode: 606, actor_loss: 2.030, critic_loss: 26.538, mean_reward: -332.136, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99981783 0.01908663 0.10148819] distance:  1.0051367336967743\n",
      "\n",
      "Episode: 607, actor_loss: 1.471, critic_loss: 25.836, mean_reward: -77.768, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9994237  0.03394514 0.00660383] distance:  1.0000218050643277\n",
      "\n",
      "Episode: 608, actor_loss: 0.994, critic_loss: 21.231, mean_reward: -309.050, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99928826 0.03772234 0.09139195] distance:  1.0041675600965945\n",
      "\n",
      "Episode: 609, actor_loss: 13.022, critic_loss: 32.902, mean_reward: -496.506, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99276811 -0.1200478  -0.49340061] distance:  1.1150982732570607\n",
      "\n",
      "Episode: 610, actor_loss: -5.619, critic_loss: 31.414, mean_reward: -315.910, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.93732623 -0.34845307  1.89797132] distance:  2.1452960442552063\n",
      "\n",
      "Episode: 611, actor_loss: -5.662, critic_loss: 28.815, mean_reward: -388.308, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.97632127  0.21632563  0.02831194] distance:  1.000400702625658\n",
      "\n",
      "Episode: 612, actor_loss: -2.996, critic_loss: 15.918, mean_reward: -470.298, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999818 -0.00190846  0.02730538] distance:  1.0003727223767769\n",
      "\n",
      "Episode: 613, actor_loss: 0.196, critic_loss: 9.256, mean_reward: -391.187, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99992678  0.01210085 -0.11406291] distance:  1.006484152060204\n",
      "\n",
      "Episode: 614, actor_loss: 1.051, critic_loss: 14.112, mean_reward: -309.527, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997328 -0.00731037  0.10038369] distance:  1.0050258129955483\n",
      "\n",
      "Episode: 615, actor_loss: -0.944, critic_loss: 9.530, mean_reward: -416.245, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999604e-01  8.90158378e-04 -2.00609111e-02] distance:  1.0002011998364495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 616, actor_loss: 0.223, critic_loss: 8.101, mean_reward: -293.566, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99950505 -0.03145878 -0.07161504] distance:  1.0025610772283708\n",
      "\n",
      "Episode: 617, actor_loss: 2.175, critic_loss: 37.825, mean_reward: -320.851, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99984924 -0.01736391  0.03200783] distance:  1.0005121194113333\n",
      "\n",
      "Episode: 618, actor_loss: 1.538, critic_loss: 15.040, mean_reward: -256.794, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999908  0.00135396 -0.13160033] distance:  1.0086221522995618\n",
      "\n",
      "Episode: 619, actor_loss: 14.152, critic_loss: 38.457, mean_reward: -291.623, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.51324055 -0.8582448  -0.33350641] distance:  1.0541472981483515\n",
      "\n",
      "Episode: 620, actor_loss: -8.316, critic_loss: 39.480, mean_reward: -369.079, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.32841896  0.94453215 -3.14447425] distance:  3.299654267013361\n",
      "\n",
      "Episode: 621, actor_loss: -8.465, critic_loss: 41.619, mean_reward: -338.601, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99885462 -0.04784817  0.07798561] distance:  1.003036268012145\n",
      "\n",
      "Episode: 622, actor_loss: -5.050, critic_loss: 27.944, mean_reward: -182.960, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99968849 -0.02495829  0.19302465] distance:  1.0184588928719558\n",
      "\n",
      "Episode: 623, actor_loss: -2.146, critic_loss: 18.546, mean_reward: -207.229, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99993708 0.01121746 0.04376995] distance:  1.0009574457937604\n",
      "\n",
      "Episode: 624, actor_loss: -1.885, critic_loss: 12.188, mean_reward: -372.661, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997869  0.00652877 -0.05833631] distance:  1.0017001173701072\n",
      "\n",
      "Episode: 625, actor_loss: -1.028, critic_loss: 3.486, mean_reward: -292.099, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99911394 0.04208731 0.06133272] distance:  1.0018790857184632\n",
      "\n",
      "Episode: 626, actor_loss: 1.154, critic_loss: 11.076, mean_reward: -259.220, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99767616 -0.06813429  0.21568512] distance:  1.0229956358898347\n",
      "\n",
      "Episode: 627, actor_loss: 2.334, critic_loss: 29.898, mean_reward: -291.819, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998606 -0.00528085 -0.07740126] distance:  1.0029910044072823\n",
      "\n",
      "Episode: 628, actor_loss: 2.370, critic_loss: 13.629, mean_reward: -290.268, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998679 -0.00514006  0.14171058] distance:  1.0099910337416833\n",
      "\n",
      "Episode: 629, actor_loss: 19.672, critic_loss: 50.493, mean_reward: -316.248, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.68305888  0.73036331 -0.70046842] distance:  1.220924244827587\n",
      "\n",
      "Episode: 630, actor_loss: -9.524, critic_loss: 35.725, mean_reward: -126.601, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96314527 -0.26898177  6.87539818] distance:  6.947740645365624\n",
      "\n",
      "Episode: 631, actor_loss: -4.719, critic_loss: 18.086, mean_reward: -162.860, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.41048279 -0.91186835  4.11265903] distance:  4.232489133917733\n",
      "\n",
      "Episode: 632, actor_loss: -3.162, critic_loss: 12.898, mean_reward: -184.835, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99782618 -0.06590081  0.46856797] distance:  1.104335068039392\n",
      "\n",
      "Episode: 633, actor_loss: -1.664, critic_loss: 6.678, mean_reward: -290.458, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998482 -0.00550947  0.06991887] distance:  1.0024413437918427\n",
      "\n",
      "Episode: 634, actor_loss: -1.176, critic_loss: 4.855, mean_reward: -155.027, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9996315  -0.02714514  0.0331839 ] distance:  1.0005504342352707\n",
      "\n",
      "Episode: 635, actor_loss: 1.481, critic_loss: 6.410, mean_reward: -226.809, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997595 -0.00693485 -0.04770554] distance:  1.0011372623990764\n",
      "\n",
      "Episode: 636, actor_loss: 1.765, critic_loss: 21.531, mean_reward: -435.557, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99992006 0.01264428 0.02267203] distance:  1.000256977403859\n",
      "\n",
      "Episode: 637, actor_loss: 0.149, critic_loss: 2.928, mean_reward: -221.965, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988494 -0.01516924 -0.05646851] distance:  1.0015930772006385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 638, actor_loss: 2.624, critic_loss: 41.590, mean_reward: -456.623, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999606 -0.0028075  -0.09426392] distance:  1.0044330178367438\n",
      "\n",
      "Episode: 639, actor_loss: 20.070, critic_loss: 67.231, mean_reward: -518.736, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.66956163 -0.7427565  -0.21844816] distance:  1.0235817502424722\n",
      "\n",
      "Episode: 640, actor_loss: -8.109, critic_loss: 31.967, mean_reward: -696.128, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.64385636 -0.76514638  2.94892489] distance:  3.113865442896421\n",
      "\n",
      "Episode: 641, actor_loss: -5.439, critic_loss: 26.919, mean_reward: -174.849, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99982908  0.01848792  1.65454362] distance:  1.9332652700184685\n",
      "\n",
      "Episode: 642, actor_loss: -6.076, critic_loss: 31.381, mean_reward: -327.487, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.33668962 -0.94161569  5.28651166] distance:  5.380260727929401\n",
      "\n",
      "Episode: 643, actor_loss: -3.379, critic_loss: 19.810, mean_reward: -434.027, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99975897 0.02195476 0.08819078] distance:  1.0038812750098725\n",
      "\n",
      "Episode: 644, actor_loss: -1.746, critic_loss: 11.904, mean_reward: -255.478, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99994863 0.01013613 0.04128814] distance:  1.0008519924701758\n",
      "\n",
      "Episode: 645, actor_loss: -0.509, critic_loss: 4.922, mean_reward: -175.834, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99994818 0.01018042 0.01727559] distance:  1.0001492119161877\n",
      "\n",
      "Episode: 646, actor_loss: -1.124, critic_loss: 5.483, mean_reward: -806.937, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999863  0.00165764 -0.10327338] distance:  1.0053185515127796\n",
      "\n",
      "Episode: 647, actor_loss: 0.820, critic_loss: 10.935, mean_reward: -406.793, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994431  0.01055326 -0.08640807] distance:  1.0037262344536548\n",
      "\n",
      "Episode: 648, actor_loss: 0.320, critic_loss: 13.571, mean_reward: -203.131, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998704  0.0050913  -0.04263322] distance:  1.0009083830428716\n",
      "\n",
      "Episode: 649, actor_loss: 12.436, critic_loss: 164.558, mean_reward: -482.930, best_return: -53.935\n",
      "last 50 episode mean reward:  -337.0556580340472\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99741004 0.07192498 0.59087149] distance:  1.1615201760467864\n",
      "\n",
      "Episode: 650, actor_loss: -3.118, critic_loss: 50.125, mean_reward: -407.857, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.8055126  -0.59257865  2.06875038] distance:  2.297765898513776\n",
      "\n",
      "Episode: 651, actor_loss: -3.701, critic_loss: 30.785, mean_reward: -711.059, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99866963 -0.05156529  0.13007733] distance:  1.0084245686134734\n",
      "\n",
      "Episode: 652, actor_loss: -0.134, critic_loss: 16.364, mean_reward: -460.942, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.82112848 -0.57074339 -1.16757461] distance:  1.537280217686872\n",
      "\n",
      "Episode: 653, actor_loss: -0.604, critic_loss: 10.757, mean_reward: -609.958, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99955911 -0.02969139  0.04308657] distance:  1.0009277958766085\n",
      "\n",
      "Episode: 654, actor_loss: -2.517, critic_loss: 20.551, mean_reward: -594.162, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.89880399 -0.43835076  1.36487337] distance:  1.6920045285823293\n",
      "\n",
      "Episode: 655, actor_loss: 0.574, critic_loss: 13.167, mean_reward: -436.404, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999911 0.00133384 0.18045727] distance:  1.0161519693746808\n",
      "\n",
      "Episode: 656, actor_loss: 0.679, critic_loss: 16.484, mean_reward: -668.884, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999951e-01 -3.13092281e-04 -3.99625245e-02] distance:  1.0007981831337043\n",
      "\n",
      "Episode: 657, actor_loss: 0.508, critic_loss: 7.511, mean_reward: -226.006, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999815  0.00192348 -0.10988678] distance:  1.0060194357943182\n",
      "\n",
      "Episode: 658, actor_loss: 1.980, critic_loss: 40.760, mean_reward: -171.107, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998701 -0.00509793 -0.03729035] distance:  1.0006950434490967\n",
      "\n",
      "Episode: 659, actor_loss: 22.147, critic_loss: 89.196, mean_reward: -556.575, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.80897017 0.5878497  0.35550316] distance:  1.0613116856669895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 660, actor_loss: -7.168, critic_loss: 48.831, mean_reward: -840.317, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.28728988 -0.95784368 -4.12200317] distance:  4.24156929746423\n",
      "\n",
      "Episode: 661, actor_loss: -2.765, critic_loss: 24.055, mean_reward: -371.594, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99943644 0.03356784 0.02656448] distance:  1.0003527734688302\n",
      "\n",
      "Episode: 662, actor_loss: -3.127, critic_loss: 21.265, mean_reward: -437.021, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9480939  -0.31799049  1.19010958] distance:  1.5544648015519593\n",
      "\n",
      "Episode: 663, actor_loss: -1.686, critic_loss: 12.819, mean_reward: -469.684, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997918  0.00645221 -0.03223047] distance:  1.0005192668172753\n",
      "\n",
      "Episode: 664, actor_loss: -2.918, critic_loss: 13.802, mean_reward: -365.143, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.97251186 -0.23285334 -1.89474987] distance:  2.142446519283411\n",
      "\n",
      "Episode: 665, actor_loss: 0.310, critic_loss: 7.097, mean_reward: -428.184, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999726 0.00234028 0.07589585] distance:  1.0028759545246757\n",
      "\n",
      "Episode: 666, actor_loss: 0.252, critic_loss: 6.008, mean_reward: -332.925, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.91841308  0.39562282 -0.8121865 ] distance:  1.2882728400459398\n",
      "\n",
      "Episode: 667, actor_loss: 0.484, critic_loss: 5.678, mean_reward: -367.778, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930497  0.03727699 -0.10425276] distance:  1.0054196331091465\n",
      "\n",
      "Episode: 668, actor_loss: -1.681, critic_loss: 6.105, mean_reward: -149.827, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.67927658  0.73388237 -0.69310437] distance:  1.216714290867028\n",
      "\n",
      "Episode: 669, actor_loss: 8.745, critic_loss: 217.041, mean_reward: -252.570, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.60592244 0.79552372 0.702247  ] distance:  1.2219455184505976\n",
      "\n",
      "Episode: 670, actor_loss: -5.446, critic_loss: 63.811, mean_reward: -432.952, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.20729238 -0.97827904 -0.25221799] distance:  1.0313165931081967\n",
      "\n",
      "Episode: 671, actor_loss: 1.085, critic_loss: 56.376, mean_reward: -309.325, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995532 -0.00945277  0.10027773] distance:  1.0050152354645845\n",
      "\n",
      "Episode: 672, actor_loss: -2.296, critic_loss: 19.988, mean_reward: -155.571, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.56078074  0.82796435 -3.24278736] distance:  3.3934746016466444\n",
      "\n",
      "Episode: 673, actor_loss: -0.905, critic_loss: 17.848, mean_reward: -362.223, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999861 -0.00166644 -0.02028725] distance:  1.0002057650715162\n",
      "\n",
      "Episode: 674, actor_loss: 0.515, critic_loss: 12.178, mean_reward: -279.264, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.25329863 -0.96738814  4.50001449] distance:  4.609786374805586\n",
      "\n",
      "Episode: 675, actor_loss: 2.132, critic_loss: 17.309, mean_reward: -335.417, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998779 -0.00494225  0.05479974] distance:  1.0015003803432052\n",
      "\n",
      "Episode: 676, actor_loss: 0.795, critic_loss: 10.173, mean_reward: -316.849, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997143  0.00755879 -0.14762964] distance:  1.010838518373919\n",
      "\n",
      "Episode: 677, actor_loss: 2.969, critic_loss: 43.424, mean_reward: -315.386, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99898301 0.04508826 0.00409436] distance:  1.0000083818653291\n",
      "\n",
      "Episode: 678, actor_loss: 2.115, critic_loss: 36.476, mean_reward: -426.863, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99952544  0.03080414 -0.05792716] distance:  1.0016763725711744\n",
      "\n",
      "Episode: 679, actor_loss: 19.524, critic_loss: 67.055, mean_reward: -280.873, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.04811078 -0.99884201  0.44421379] distance:  1.0942238746260797\n",
      "\n",
      "Episode: 680, actor_loss: -12.601, critic_loss: 53.182, mean_reward: -336.552, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.32144739  0.94692744 -2.58221672] distance:  2.7690870698858436\n",
      "\n",
      "Episode: 681, actor_loss: -6.689, critic_loss: 20.823, mean_reward: -321.354, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99992    0.01264862 0.16699548] distance:  1.0138478631553736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 682, actor_loss: -1.123, critic_loss: 11.987, mean_reward: -316.506, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99951757 -0.03105834  0.36008246] distance:  1.0628543552439762\n",
      "\n",
      "Episode: 683, actor_loss: -0.664, critic_loss: 5.850, mean_reward: -431.429, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99878149 0.04935105 0.07640872] distance:  1.0029148978078786\n",
      "\n",
      "Episode: 684, actor_loss: -2.547, critic_loss: 3.542, mean_reward: -401.382, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.78500139 -0.619494    2.127369  ] distance:  2.3506805141994995\n",
      "\n",
      "Episode: 685, actor_loss: 1.495, critic_loss: 19.743, mean_reward: -102.722, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99975073 0.02232654 0.08560196] distance:  1.0036571600246238\n",
      "\n",
      "Episode: 686, actor_loss: 1.794, critic_loss: 22.255, mean_reward: -127.078, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996723 -0.00809583  0.0373052 ] distance:  1.0006955970016087\n",
      "\n",
      "Episode: 687, actor_loss: 1.914, critic_loss: 16.126, mean_reward: -253.184, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994056  0.01090282 -0.03048625] distance:  1.00046459781484\n",
      "\n",
      "Episode: 688, actor_loss: 1.922, critic_loss: 18.244, mean_reward: -373.812, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999727 0.00233555 0.17933641] distance:  1.0159535157595203\n",
      "\n",
      "Episode: 689, actor_loss: 14.886, critic_loss: 36.474, mean_reward: -231.849, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.63395451 -0.77337034 -0.36279922] distance:  1.063777832553586\n",
      "\n",
      "Episode: 690, actor_loss: -9.510, critic_loss: 25.580, mean_reward: -259.984, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83719023  0.54691181  0.0614588 ] distance:  1.001886812291863\n",
      "\n",
      "Episode: 691, actor_loss: -7.348, critic_loss: 34.495, mean_reward: -232.618, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.01125061 -0.99993671 -0.51235234] distance:  1.1236124429190377\n",
      "\n",
      "Episode: 692, actor_loss: -7.692, critic_loss: 24.465, mean_reward: -176.831, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.65940586  0.75178715 -3.09616551] distance:  3.2536503952682656\n",
      "\n",
      "Episode: 693, actor_loss: -6.265, critic_loss: 20.995, mean_reward: -180.227, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99971512 -0.02386777 -0.05268009] distance:  1.001386634467032\n",
      "\n",
      "Episode: 694, actor_loss: -1.967, critic_loss: 6.351, mean_reward: -100.299, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99962009 -0.02756219 -0.12533186] distance:  1.0078234346541024\n",
      "\n",
      "Episode: 695, actor_loss: 1.638, critic_loss: 5.684, mean_reward: -416.881, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9997867   0.02065336 -0.01175576] distance:  1.0000690965641597\n",
      "\n",
      "Episode: 696, actor_loss: 2.545, critic_loss: 30.808, mean_reward: -286.294, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99961181 -0.02786102  0.13970008] distance:  1.0097109058232205\n",
      "\n",
      "Episode: 697, actor_loss: 2.702, critic_loss: 17.655, mean_reward: -258.747, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999548 0.00300759 0.08302009] distance:  1.0034402497160653\n",
      "\n",
      "Episode: 698, actor_loss: 2.430, critic_loss: 12.528, mean_reward: -315.210, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999005   0.01410658 -0.05181041] distance:  1.0013412595866387\n",
      "\n",
      "Episode: 699, actor_loss: 21.276, critic_loss: 64.899, mean_reward: -288.155, best_return: -53.935\n",
      "last 50 episode mean reward:  -349.6767011996633\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99989325  0.01461094  0.8663963 ] distance:  1.3231184906801012\n",
      "\n",
      "Episode: 700, actor_loss: -9.643, critic_loss: 32.838, mean_reward: -76.810, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.27489761 -0.96147351  4.11218445] distance:  4.232027998003282\n",
      "\n",
      "Episode: 701, actor_loss: -4.483, critic_loss: 14.946, mean_reward: -344.785, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.97745332  0.21115164  5.37923898] distance:  5.471399453697534\n",
      "\n",
      "Episode: 702, actor_loss: -3.237, critic_loss: 14.156, mean_reward: -334.696, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.17667723  0.98426884 -0.87092938] distance:  1.3260912448252946\n",
      "\n",
      "Episode: 703, actor_loss: -2.651, critic_loss: 8.806, mean_reward: -322.403, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99913496 -0.04158531 -0.04946359] distance:  1.0012225758149036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 704, actor_loss: -0.592, critic_loss: 5.477, mean_reward: -368.203, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9998055  -0.01972194  0.00423544] distance:  1.0000089694321375\n",
      "\n",
      "Episode: 705, actor_loss: 1.806, critic_loss: 10.920, mean_reward: -439.858, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99993906 -0.01103968 -0.02674559] distance:  1.0003575993531584\n",
      "\n",
      "Episode: 706, actor_loss: 1.822, critic_loss: 5.045, mean_reward: -240.999, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99955732 -0.02975157  0.04045463] distance:  1.0008179538952273\n",
      "\n",
      "Episode: 707, actor_loss: 2.278, critic_loss: 25.865, mean_reward: -347.343, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999569 -0.00293587  0.06668108] distance:  1.0022207171662814\n",
      "\n",
      "Episode: 708, actor_loss: 2.399, critic_loss: 22.492, mean_reward: -213.076, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99991246 -0.01323166 -0.10067399] distance:  1.0050548504279204\n",
      "\n",
      "Episode: 709, actor_loss: 19.376, critic_loss: 53.997, mean_reward: -671.200, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.76814532 -0.64027554  0.84728225] distance:  1.3106819668527572\n",
      "\n",
      "Episode: 710, actor_loss: -5.064, critic_loss: 25.069, mean_reward: -339.962, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.29859811 -0.95437894 -0.26739609] distance:  1.035133163952715\n",
      "\n",
      "Episode: 711, actor_loss: -6.049, critic_loss: 20.318, mean_reward: -410.231, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99961365 -0.02779495  0.16149017] distance:  1.0129556138130282\n",
      "\n",
      "Episode: 712, actor_loss: -3.715, critic_loss: 9.794, mean_reward: -341.938, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99989443 -0.01453037 -0.00847011] distance:  1.0000358707578798\n",
      "\n",
      "Episode: 713, actor_loss: -1.141, critic_loss: 1.728, mean_reward: -179.139, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999841 -0.0017857   0.03790672] distance:  1.0007182019462246\n",
      "\n",
      "Episode: 714, actor_loss: 2.344, critic_loss: 12.917, mean_reward: -129.556, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99969662 0.02463056 0.0553174 ] distance:  1.0015288389734658\n",
      "\n",
      "Episode: 715, actor_loss: -0.407, critic_loss: 1.461, mean_reward: -539.343, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.03797451  0.99927871 -1.11809016] distance:  1.5000418681125671\n",
      "\n",
      "Episode: 716, actor_loss: 0.082, critic_loss: 11.757, mean_reward: -692.479, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99984193 -0.01777936  0.04448558] distance:  1.000988994323978\n",
      "\n",
      "Episode: 717, actor_loss: 3.554, critic_loss: 83.437, mean_reward: -748.477, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999847e-01  5.52765561e-04 -4.56762151e-02] distance:  1.0010426147890679\n",
      "\n",
      "Episode: 718, actor_loss: 0.260, critic_loss: 21.146, mean_reward: -400.190, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99909568 -0.04251838 -0.06475188] distance:  1.0020942102880759\n",
      "\n",
      "Episode: 719, actor_loss: 15.570, critic_loss: 104.858, mean_reward: -543.276, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.80717386 -0.59031378  0.09686752] distance:  1.0046807038819956\n",
      "\n",
      "Episode: 720, actor_loss: -14.773, critic_loss: 117.102, mean_reward: -154.513, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.08087136 -0.99672455 -0.40978572] distance:  1.0807054819869544\n",
      "\n",
      "Episode: 721, actor_loss: -6.723, critic_loss: 35.485, mean_reward: -373.075, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9983582  -0.05727912  0.09035798] distance:  1.0040739839257697\n",
      "\n",
      "Episode: 722, actor_loss: -5.099, critic_loss: 25.240, mean_reward: -206.681, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.55883543 -0.82927858  3.21933289] distance:  3.3710687094691356\n",
      "\n",
      "Episode: 723, actor_loss: 1.176, critic_loss: 42.885, mean_reward: -415.919, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.8966003  -0.44284073 -4.46477575] distance:  4.575393155973448\n",
      "\n",
      "Episode: 724, actor_loss: 1.499, critic_loss: 9.246, mean_reward: -360.164, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99983535 -0.01814591  0.04676367] distance:  1.0010928232630931\n",
      "\n",
      "Episode: 725, actor_loss: 0.863, critic_loss: 2.703, mean_reward: -430.930, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9996102  -0.02791861  0.07619026] distance:  1.0028982781656328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 726, actor_loss: 1.841, critic_loss: 37.543, mean_reward: -473.404, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99984344 -0.01769458  0.10914588] distance:  1.0059387771045643\n",
      "\n",
      "Episode: 727, actor_loss: 2.006, critic_loss: 10.326, mean_reward: -292.775, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99986698 -0.01631045 -0.05950339] distance:  1.001768762384006\n",
      "\n",
      "Episode: 728, actor_loss: 2.007, critic_loss: 15.751, mean_reward: -353.210, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99994261 0.01071346 0.13992652] distance:  1.0097422597252044\n",
      "\n",
      "Episode: 729, actor_loss: 15.172, critic_loss: 34.792, mean_reward: -284.105, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99817128  0.06044915 -0.97916886] distance:  1.399561240167884\n",
      "\n",
      "Episode: 730, actor_loss: -11.977, critic_loss: 57.017, mean_reward: -367.754, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99985049 0.01729127 0.00497984] distance:  1.0000123993233445\n",
      "\n",
      "Episode: 731, actor_loss: -8.697, critic_loss: 35.984, mean_reward: -467.510, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988087 -0.01543516  0.19061254] distance:  1.018004488991081\n",
      "\n",
      "Episode: 732, actor_loss: -6.081, critic_loss: 24.860, mean_reward: -442.601, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99750505 -0.07059509  0.12556077] distance:  1.007851926923765\n",
      "\n",
      "Episode: 733, actor_loss: -2.000, critic_loss: 22.746, mean_reward: -425.672, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999373 -0.00354162 -0.08292286] distance:  1.0034322106877067\n",
      "\n",
      "Episode: 734, actor_loss: -0.255, critic_loss: 7.215, mean_reward: -402.747, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997349 -0.00728185 -0.09254061] distance:  1.0042727543246606\n",
      "\n",
      "Episode: 735, actor_loss: -0.752, critic_loss: 5.213, mean_reward: -442.740, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988011  0.01548413 -0.00985205] distance:  1.0000485302695987\n",
      "\n",
      "Episode: 736, actor_loss: 2.139, critic_loss: 30.025, mean_reward: -171.516, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99974014  0.02279604 -0.02769036] distance:  1.0003833045534563\n",
      "\n",
      "Episode: 737, actor_loss: 1.424, critic_loss: 7.070, mean_reward: -344.080, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997978 -0.00635908  0.08930705] distance:  1.0039799547779913\n",
      "\n",
      "Episode: 738, actor_loss: 2.406, critic_loss: 12.353, mean_reward: -410.963, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99966291  0.02596292 -0.01687881] distance:  1.0001424370407936\n",
      "\n",
      "Episode: 739, actor_loss: 24.956, critic_loss: 96.989, mean_reward: -347.795, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.96538292 0.26083677 0.98719404] distance:  1.4051875549483943\n",
      "\n",
      "Episode: 740, actor_loss: -10.535, critic_loss: 64.008, mean_reward: -574.713, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9098818   0.41486759  6.42803037] distance:  6.5053496822414845\n",
      "\n",
      "Episode: 741, actor_loss: -3.828, critic_loss: 63.500, mean_reward: -485.878, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.72722741 0.6863966  2.19790184] distance:  2.414699257770076\n",
      "\n",
      "Episode: 742, actor_loss: -5.243, critic_loss: 43.042, mean_reward: -680.943, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9991159   0.04204076 -0.01426581] distance:  1.000101751520255\n",
      "\n",
      "Episode: 743, actor_loss: -2.626, critic_loss: 28.697, mean_reward: -555.344, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99946606  0.03267393 -0.04735159] distance:  1.001120459057132\n",
      "\n",
      "Episode: 744, actor_loss: -1.041, critic_loss: 28.259, mean_reward: -654.460, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99982121 0.01890881 0.00778175] distance:  1.0000302773426433\n",
      "\n",
      "Episode: 745, actor_loss: 0.543, critic_loss: 48.415, mean_reward: -482.793, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99866668  0.05162235 -0.01068645] distance:  1.0000570984677197\n",
      "\n",
      "Episode: 746, actor_loss: 1.751, critic_loss: 8.808, mean_reward: -735.243, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99983109 0.01837887 0.00716138] distance:  1.0000256423430876\n",
      "\n",
      "Episode: 747, actor_loss: 1.140, critic_loss: 6.752, mean_reward: -756.350, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.06625076 0.99780301 4.71403792] distance:  4.8189369650360865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 748, actor_loss: 0.382, critic_loss: 5.357, mean_reward: -799.917, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99997119 0.00759048 0.04517138] distance:  1.0010197069007127\n",
      "\n",
      "Episode: 749, actor_loss: 12.970, critic_loss: 95.678, mean_reward: -785.391, best_return: -53.935\n",
      "last 50 episode mean reward:  -427.2629772467306\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.57119487 -0.82081448 -0.61328403] distance:  1.1730802605240633\n",
      "\n",
      "Episode: 750, actor_loss: -11.661, critic_loss: 44.985, mean_reward: -1139.336, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99814239 0.06092435 0.05203176] distance:  1.001352737313451\n",
      "\n",
      "Episode: 751, actor_loss: -2.851, critic_loss: 30.746, mean_reward: -214.568, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.03075939  0.99952682 -1.745927  ] distance:  2.0120291001389132\n",
      "\n",
      "Episode: 752, actor_loss: -2.664, critic_loss: 76.569, mean_reward: -506.679, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.54750215 0.83680427 3.59578028] distance:  3.732242732037443\n",
      "\n",
      "Episode: 753, actor_loss: 0.957, critic_loss: 27.242, mean_reward: -754.243, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9967221   0.08090151 -0.05487629] distance:  1.0015045718168558\n",
      "\n",
      "Episode: 754, actor_loss: 1.265, critic_loss: 6.295, mean_reward: -805.199, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.91417315 -0.40532389  4.9331383 ] distance:  5.033473306671824\n",
      "\n",
      "Episode: 755, actor_loss: 0.801, critic_loss: 4.594, mean_reward: -731.054, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.32396799 -0.94606804  6.91767808] distance:  6.989582960700726\n",
      "\n",
      "Episode: 756, actor_loss: 1.068, critic_loss: 4.239, mean_reward: -588.132, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.09878105 -0.99510919  7.19325245] distance:  7.262429397155931\n",
      "\n",
      "Episode: 757, actor_loss: 1.170, critic_loss: 3.862, mean_reward: -498.660, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99589566  0.09050881  7.77884928] distance:  7.8428627518928895\n",
      "\n",
      "Episode: 758, actor_loss: 1.781, critic_loss: 3.776, mean_reward: -375.981, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99902835  0.0440721  -0.05090222] distance:  1.0012946797267515\n",
      "\n",
      "Episode: 759, actor_loss: 12.776, critic_loss: 115.949, mean_reward: -181.432, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96838367  0.24946557 -0.98403911] distance:  1.4029729063355463\n",
      "\n",
      "Episode: 760, actor_loss: -10.886, critic_loss: 37.498, mean_reward: -363.279, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.43032557 -0.90267375  3.79479356] distance:  3.924341752339443\n",
      "\n",
      "Episode: 761, actor_loss: -3.762, critic_loss: 13.427, mean_reward: -183.556, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99898479 -0.04504876  0.15693868] distance:  1.012239966310333\n",
      "\n",
      "Episode: 762, actor_loss: -1.930, critic_loss: 6.667, mean_reward: -293.086, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999126  0.00417998 -0.01616805] distance:  1.0001306943560686\n",
      "\n",
      "Episode: 763, actor_loss: 2.280, critic_loss: 19.669, mean_reward: -344.936, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [9.99999564e-01 9.33530297e-04 7.29740377e-02] distance:  1.002659069763039\n",
      "\n",
      "Episode: 764, actor_loss: 2.494, critic_loss: 22.104, mean_reward: -187.281, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99987788 -0.01562766 -0.1374951 ] distance:  1.009408194527808\n",
      "\n",
      "Episode: 765, actor_loss: 2.348, critic_loss: 16.392, mean_reward: -241.838, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999981  -0.00194751 -0.00745546] distance:  1.0000277915470839\n",
      "\n",
      "Episode: 766, actor_loss: 2.149, critic_loss: 11.952, mean_reward: -210.761, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994632  0.0103616  -0.09597698] distance:  1.00459523230665\n",
      "\n",
      "Episode: 767, actor_loss: 2.124, critic_loss: 21.524, mean_reward: -222.762, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999523  0.00976688 0.01709639] distance:  1.0001461326384868\n",
      "\n",
      "Episode: 768, actor_loss: 2.080, critic_loss: 8.648, mean_reward: -224.597, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997997  0.00632999 -0.00915585] distance:  1.0000419139250614\n",
      "\n",
      "Episode: 769, actor_loss: 17.451, critic_loss: 46.429, mean_reward: -210.024, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9517303  -0.30693557  0.738304  ] distance:  1.2430176158906525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 770, actor_loss: -9.960, critic_loss: 26.385, mean_reward: -191.246, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.70172878 -0.71244418  2.45983164] distance:  2.655328920592932\n",
      "\n",
      "Episode: 771, actor_loss: -4.352, critic_loss: 14.445, mean_reward: -317.711, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99650107 -0.08357999  0.16604908] distance:  1.0136924069177025\n",
      "\n",
      "Episode: 772, actor_loss: -3.350, critic_loss: 7.581, mean_reward: -126.861, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999393  0.00348319 -0.02442115] distance:  1.000298151780452\n",
      "\n",
      "Episode: 773, actor_loss: 1.794, critic_loss: 16.312, mean_reward: -378.678, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.88368793 -0.46807654  1.39718898] distance:  1.7181784111466372\n",
      "\n",
      "Episode: 774, actor_loss: 0.368, critic_loss: 8.662, mean_reward: -244.639, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999559 0.00296935 0.07883422] distance:  1.00310260385048\n",
      "\n",
      "Episode: 775, actor_loss: 0.524, critic_loss: 2.001, mean_reward: -264.503, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99975847 -0.02197715 -0.00695545] distance:  1.0000241888524448\n",
      "\n",
      "Episode: 776, actor_loss: 1.451, critic_loss: 5.771, mean_reward: -182.618, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999999e-01  4.49602132e-05 -1.12877157e-01] distance:  1.006350462130499\n",
      "\n",
      "Episode: 777, actor_loss: 2.064, critic_loss: 41.783, mean_reward: -258.011, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996923 -0.0078447   0.09237539] distance:  1.0042575428659646\n",
      "\n",
      "Episode: 778, actor_loss: 1.958, critic_loss: 10.507, mean_reward: -292.915, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [9.99999795e-01 6.40280914e-04 4.85779640e-02] distance:  1.0011792140221907\n",
      "\n",
      "Episode: 779, actor_loss: 17.686, critic_loss: 43.473, mean_reward: -235.129, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.89196446 -0.45210553 -0.70491498] distance:  1.22348074298355\n",
      "\n",
      "Episode: 780, actor_loss: -6.324, critic_loss: 25.877, mean_reward: -133.663, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.40439812 -0.91458305  4.46535553] distance:  4.575958918241115\n",
      "\n",
      "Episode: 781, actor_loss: -2.690, critic_loss: 24.854, mean_reward: -285.268, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99991204 -0.01326315  0.10037148] distance:  1.0050245939752418\n",
      "\n",
      "Episode: 782, actor_loss: -4.309, critic_loss: 12.623, mean_reward: -266.198, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99960349  0.0281577  -0.05994658] distance:  1.0017951848622195\n",
      "\n",
      "Episode: 783, actor_loss: -0.188, critic_loss: 5.108, mean_reward: -260.295, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9987052   0.05087162 -0.05657772] distance:  1.00159924066615\n",
      "\n",
      "Episode: 784, actor_loss: -2.958, critic_loss: 6.107, mean_reward: -235.896, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995286  0.0097095  -0.04232429] distance:  1.0008952718215371\n",
      "\n",
      "Episode: 785, actor_loss: 1.274, critic_loss: 19.373, mean_reward: -233.767, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.999117   0.04201445 0.0717242 ] distance:  1.0025688805137947\n",
      "\n",
      "Episode: 786, actor_loss: 2.248, critic_loss: 29.586, mean_reward: -249.246, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99962658  0.02732573 -0.07562821] distance:  1.002855735644867\n",
      "\n",
      "Episode: 787, actor_loss: 1.846, critic_loss: 30.137, mean_reward: -108.354, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99949665 0.03172443 0.01932766] distance:  1.000186761763184\n",
      "\n",
      "Episode: 788, actor_loss: 1.443, critic_loss: 25.888, mean_reward: -300.750, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994428  0.01055633 -0.03741492] distance:  1.000699693429055\n",
      "\n",
      "Episode: 789, actor_loss: 15.420, critic_loss: 51.646, mean_reward: -212.776, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.88403121  0.46742788 -0.13455235] distance:  1.0090115630794618\n",
      "\n",
      "Episode: 790, actor_loss: -7.392, critic_loss: 29.798, mean_reward: -156.762, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99989538 0.01446458 0.12532582] distance:  1.0078226835741186\n",
      "\n",
      "Episode: 791, actor_loss: -4.382, critic_loss: 13.245, mean_reward: -298.422, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.31853632  0.94791066  3.36728225] distance:  3.512632877003475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 792, actor_loss: -2.511, critic_loss: 15.519, mean_reward: -187.102, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99916049 -0.04096731  0.39990367] distance:  1.0769971878926796\n",
      "\n",
      "Episode: 793, actor_loss: 1.759, critic_loss: 38.562, mean_reward: -285.614, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99812308  0.0612398  -0.04924714] distance:  1.001211905995554\n",
      "\n",
      "Episode: 794, actor_loss: 1.754, critic_loss: 26.429, mean_reward: -323.120, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99979229  0.02038076 -0.05840086] distance:  1.0017038784683903\n",
      "\n",
      "Episode: 795, actor_loss: 1.491, critic_loss: 20.240, mean_reward: -310.093, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99979353  0.02031989 -0.02276223] distance:  1.0002590258983541\n",
      "\n",
      "Episode: 796, actor_loss: 1.402, critic_loss: 24.737, mean_reward: -149.349, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998965  0.00454978 -0.05055336] distance:  1.0012770058962206\n",
      "\n",
      "Episode: 797, actor_loss: 0.968, critic_loss: 12.825, mean_reward: -181.413, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99992641 0.01213134 0.0440653 ] distance:  1.0009704046101338\n",
      "\n",
      "Episode: 798, actor_loss: 1.081, critic_loss: 19.349, mean_reward: -182.889, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99992637  0.01213522 -0.04087776] distance:  1.000835146951133\n",
      "\n",
      "Episode: 799, actor_loss: 9.993, critic_loss: 21.323, mean_reward: -192.783, best_return: -53.935\n",
      "last 50 episode mean reward:  -306.46952958899146\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.88231938 -0.47065116 -0.87163567] distance:  1.326555217259962\n",
      "\n",
      "Episode: 800, actor_loss: -5.942, critic_loss: 20.152, mean_reward: -202.040, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.50462774 -0.86333704  2.81701083] distance:  2.989239038536591\n",
      "\n",
      "Episode: 801, actor_loss: -4.383, critic_loss: 23.302, mean_reward: -201.024, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99883783 -0.04819748  0.35297181] distance:  1.060466453931865\n",
      "\n",
      "Episode: 802, actor_loss: -3.392, critic_loss: 12.785, mean_reward: -274.737, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999762  0.00689978 0.02174404] distance:  1.0002363737520277\n",
      "\n",
      "Episode: 803, actor_loss: -1.383, critic_loss: 5.165, mean_reward: -162.642, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9997271  0.02336069 0.00474023] distance:  1.0000112348390766\n",
      "\n",
      "Episode: 804, actor_loss: -0.958, critic_loss: 1.042, mean_reward: -139.922, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999976   0.00219096 -0.00539129] distance:  1.0000145328907626\n",
      "\n",
      "Episode: 805, actor_loss: 1.256, critic_loss: 20.337, mean_reward: -187.006, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998117  0.00613726 -0.06779669] distance:  1.0022955610249542\n",
      "\n",
      "Episode: 806, actor_loss: 1.846, critic_loss: 14.691, mean_reward: -306.108, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99991395 0.01311879 0.02390328] distance:  1.000285642569231\n",
      "\n",
      "Episode: 807, actor_loss: 1.722, critic_loss: 11.873, mean_reward: -153.909, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999565e-01  9.32922643e-04 -6.43682571e-02] distance:  1.002069494853384\n",
      "\n",
      "Episode: 808, actor_loss: 1.709, critic_loss: 10.941, mean_reward: -185.217, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999963  -0.00272103  0.07080251] distance:  1.0025033642642085\n",
      "\n",
      "Episode: 809, actor_loss: 12.288, critic_loss: 28.071, mean_reward: -197.368, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.91884397 -0.39462104  0.57514409] distance:  1.1535990294225162\n",
      "\n",
      "Episode: 810, actor_loss: -7.431, critic_loss: 25.645, mean_reward: -195.003, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83259295  0.55388536 -2.53397979] distance:  2.724161080853773\n",
      "\n",
      "Episode: 811, actor_loss: -4.600, critic_loss: 23.017, mean_reward: -104.649, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99973979 -0.0228112   0.10106981] distance:  1.00509457632675\n",
      "\n",
      "Episode: 812, actor_loss: -7.982, critic_loss: 32.234, mean_reward: -291.979, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99990618 0.01369821 0.02758789] distance:  1.0003804733838308\n",
      "\n",
      "Episode: 813, actor_loss: 0.557, critic_loss: 19.236, mean_reward: -281.611, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99966947 0.02570885 0.03235432] distance:  1.0005232641107453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 814, actor_loss: 1.930, critic_loss: 24.410, mean_reward: -187.706, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9998612   0.01666091 -0.00208117] distance:  1.0000021656216447\n",
      "\n",
      "Episode: 815, actor_loss: 1.760, critic_loss: 27.322, mean_reward: -183.268, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99944964  0.03317251 -0.0174629 ] distance:  1.0001524648968922\n",
      "\n",
      "Episode: 816, actor_loss: 2.188, critic_loss: 20.416, mean_reward: -184.703, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999229   0.0124175  -0.04751319] distance:  1.001128115107203\n",
      "\n",
      "Episode: 817, actor_loss: 1.611, critic_loss: 25.723, mean_reward: -286.462, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9998398  0.01789878 0.05286615] distance:  1.0013964396292272\n",
      "\n",
      "Episode: 818, actor_loss: 1.655, critic_loss: 19.318, mean_reward: -198.296, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997139  0.00756489 -0.09112178] distance:  1.0041430070706765\n",
      "\n",
      "Episode: 819, actor_loss: 13.606, critic_loss: 35.912, mean_reward: -226.665, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.01448943  0.99989502 -0.58094592] distance:  1.1565025565090206\n",
      "\n",
      "Episode: 820, actor_loss: -10.708, critic_loss: 28.509, mean_reward: -158.884, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.47373859  0.88066552 -1.93353105] distance:  2.1768193130991533\n",
      "\n",
      "Episode: 821, actor_loss: -6.625, critic_loss: 21.347, mean_reward: -216.482, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99797607 -0.06359058  0.12814221] distance:  1.0081767826538686\n",
      "\n",
      "Episode: 822, actor_loss: -0.877, critic_loss: 12.435, mean_reward: -270.122, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99997909 0.00646652 0.00575898] distance:  1.0000165827969518\n",
      "\n",
      "Episode: 823, actor_loss: 1.336, critic_loss: 11.976, mean_reward: -137.486, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995892  0.00906408 -0.00775386] distance:  1.0000300607230026\n",
      "\n",
      "Episode: 824, actor_loss: -0.011, critic_loss: 1.943, mean_reward: -220.282, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998876 -0.00474135 -0.05061504] distance:  1.0012801218693133\n",
      "\n",
      "Episode: 825, actor_loss: 2.304, critic_loss: 40.397, mean_reward: -157.638, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999974  -0.00227879  0.06073116] distance:  1.001842439771598\n",
      "\n",
      "Episode: 826, actor_loss: 0.986, critic_loss: 27.164, mean_reward: -443.539, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99171889  0.12842761 -0.0196491 ] distance:  1.0001930249669155\n",
      "\n",
      "Episode: 827, actor_loss: 1.101, critic_loss: 33.207, mean_reward: -764.110, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99120678 0.13232205 0.02158174] distance:  1.0002328585994384\n",
      "\n",
      "Episode: 828, actor_loss: 1.658, critic_loss: 28.494, mean_reward: -353.668, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99458849 0.10389292 0.01016822] distance:  1.0000516950264287\n",
      "\n",
      "Episode: 829, actor_loss: 12.621, critic_loss: 46.275, mean_reward: -710.504, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.77490902 -0.63207278  0.00264872] distance:  1.0000035078564087\n",
      "\n",
      "Episode: 830, actor_loss: -8.409, critic_loss: 44.404, mean_reward: -406.351, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.2788825   0.96032523  0.19436105] distance:  1.0187130195997702\n",
      "\n",
      "Episode: 831, actor_loss: -7.579, critic_loss: 90.117, mean_reward: -354.955, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.39174727  0.92007286  3.96445614] distance:  4.088632103951629\n",
      "\n",
      "Episode: 832, actor_loss: -5.761, critic_loss: 38.604, mean_reward: -473.869, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99425086  0.10707579 -0.02848912] distance:  1.0004057326162652\n",
      "\n",
      "Episode: 833, actor_loss: 1.693, critic_loss: 109.705, mean_reward: -532.054, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99300873 0.11804095 0.00735026] distance:  1.0000270128144886\n",
      "\n",
      "Episode: 834, actor_loss: -1.442, critic_loss: 10.513, mean_reward: -269.721, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99660907  0.08228223 -0.11827292] distance:  1.0069699513817767\n",
      "\n",
      "Episode: 835, actor_loss: 1.700, critic_loss: 34.124, mean_reward: -77.939, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99806179  0.06223071 -0.05107944] distance:  1.0013037047987734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 836, actor_loss: 2.603, critic_loss: 14.003, mean_reward: -188.310, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99629335 0.08602073 0.02601243] distance:  1.0003382661505327\n",
      "\n",
      "Episode: 837, actor_loss: -1.171, critic_loss: 6.884, mean_reward: -474.341, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99767867 0.06809746 2.29919202] distance:  2.5072462833209093\n",
      "\n",
      "Episode: 838, actor_loss: 2.797, critic_loss: 41.420, mean_reward: -640.739, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99726363 0.07392737 0.00912204] distance:  1.0000416049462386\n",
      "\n",
      "Episode: 839, actor_loss: 7.526, critic_loss: 156.326, mean_reward: -726.138, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.41604116  0.90934578  0.25462504] distance:  1.0319078981376097\n",
      "\n",
      "Episode: 840, actor_loss: -11.437, critic_loss: 40.563, mean_reward: -366.282, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.19098897  0.98159218 -1.06027458] distance:  1.4574574355528107\n",
      "\n",
      "Episode: 841, actor_loss: -7.477, critic_loss: 63.027, mean_reward: -399.753, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99822743  0.05951462 -0.00721471] distance:  1.0000260256944165\n",
      "\n",
      "Episode: 842, actor_loss: -1.207, critic_loss: 10.353, mean_reward: -501.265, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99805506 0.06233851 0.00312395] distance:  1.0000048795302066\n",
      "\n",
      "Episode: 843, actor_loss: 1.822, critic_loss: 14.469, mean_reward: -316.362, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99694172 0.07814865 0.02752214] distance:  1.0003786623276654\n",
      "\n",
      "Episode: 844, actor_loss: 2.426, critic_loss: 36.528, mean_reward: -755.295, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99883786  0.04819681 -0.0366177 ] distance:  1.0006702033217112\n",
      "\n",
      "Episode: 845, actor_loss: 1.925, critic_loss: 29.250, mean_reward: -360.143, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99873809 0.05022183 0.07522814] distance:  1.0028256447446522\n",
      "\n",
      "Episode: 846, actor_loss: 2.006, critic_loss: 25.471, mean_reward: -524.528, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99853593 0.05409248 0.08103893] distance:  1.0032782801592486\n",
      "\n",
      "Episode: 847, actor_loss: 1.564, critic_loss: 23.222, mean_reward: -236.415, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99940089 0.03461007 0.04684768] distance:  1.001096751117235\n",
      "\n",
      "Episode: 848, actor_loss: 1.583, critic_loss: 34.479, mean_reward: -515.544, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9994469   0.03325509 -0.0487136 ] distance:  1.001185804136002\n",
      "\n",
      "Episode: 849, actor_loss: 14.833, critic_loss: 59.888, mean_reward: -405.043, best_return: -53.935\n",
      "last 50 episode mean reward:  -322.16152214660127\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.50758034  0.86160443 -0.53744901] distance:  1.1352759283280527\n",
      "\n",
      "Episode: 850, actor_loss: -11.091, critic_loss: 36.695, mean_reward: -356.746, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.30800598  0.95138442 -4.54402319] distance:  4.65275689825864\n",
      "\n",
      "Episode: 851, actor_loss: -2.504, critic_loss: 34.006, mean_reward: -348.227, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99971438  0.02389898 -0.05783086] distance:  1.0016708081056427\n",
      "\n",
      "Episode: 852, actor_loss: -3.143, critic_loss: 7.963, mean_reward: -319.359, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.999698    0.02457452 -0.02296949] distance:  1.000263763980847\n",
      "\n",
      "Episode: 853, actor_loss: 2.154, critic_loss: 40.760, mean_reward: -923.903, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99979774 0.02011166 0.09417055] distance:  1.0044242592266739\n",
      "\n",
      "Episode: 854, actor_loss: 0.629, critic_loss: 14.940, mean_reward: -128.606, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99990383 0.01386862 0.01673957] distance:  1.0001400967573213\n",
      "\n",
      "Episode: 855, actor_loss: 1.695, critic_loss: 54.978, mean_reward: -760.342, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999944   0.00334562 -0.13806392] distance:  1.0094858319390334\n",
      "\n",
      "Episode: 856, actor_loss: 1.119, critic_loss: 16.431, mean_reward: -283.138, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999824 0.00187416 0.01845788] distance:  1.0001703321535853\n",
      "\n",
      "Episode: 857, actor_loss: 1.265, critic_loss: 35.119, mean_reward: -205.809, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99997827 0.00659283 0.01885963] distance:  1.0001778269823105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 858, actor_loss: -0.328, critic_loss: 23.407, mean_reward: -315.832, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997829  0.00659007 -0.0471737 ] distance:  1.0011120605012032\n",
      "\n",
      "Episode: 859, actor_loss: 17.086, critic_loss: 45.634, mean_reward: -154.938, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.70075388  0.71340311 -0.99688059] distance:  1.4120095322552642\n",
      "\n",
      "Episode: 860, actor_loss: -9.433, critic_loss: 97.021, mean_reward: -205.105, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.50861888 -0.86099177  6.41295678] distance:  6.490455658901958\n",
      "\n",
      "Episode: 861, actor_loss: -9.018, critic_loss: 64.911, mean_reward: -208.354, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.77794576 -0.62833144  6.18760832] distance:  6.267894117002676\n",
      "\n",
      "Episode: 862, actor_loss: -1.149, critic_loss: 9.698, mean_reward: -205.212, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99966065 -0.0260495   0.2588921 ] distance:  1.0329690803175344\n",
      "\n",
      "Episode: 863, actor_loss: 1.340, critic_loss: 18.813, mean_reward: -462.389, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99943242  0.03368744 -0.03592504] distance:  1.0006450962513984\n",
      "\n",
      "Episode: 864, actor_loss: 1.439, critic_loss: 13.676, mean_reward: -239.732, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996925  0.00784201 -0.00752502] distance:  1.000028312570065\n",
      "\n",
      "Episode: 865, actor_loss: 1.088, critic_loss: 9.005, mean_reward: -176.626, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99990584 -0.01372236 -0.00143966] distance:  1.0000010363145626\n",
      "\n",
      "Episode: 866, actor_loss: 1.310, critic_loss: 14.262, mean_reward: -181.627, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999607 -0.00280518  0.08781049] distance:  1.0038479380530272\n",
      "\n",
      "Episode: 867, actor_loss: 0.972, critic_loss: 12.728, mean_reward: -104.057, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99996676  0.00815309 -0.0145474 ] distance:  1.000105807849486\n",
      "\n",
      "Episode: 868, actor_loss: 0.954, critic_loss: 10.423, mean_reward: -175.517, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99969222 0.02480863 0.02933418] distance:  1.0004301544614684\n",
      "\n",
      "Episode: 869, actor_loss: 8.689, critic_loss: 16.554, mean_reward: -261.549, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.03876015 -0.99924854  0.72591259] distance:  1.2356978130274006\n",
      "\n",
      "Episode: 870, actor_loss: -9.172, critic_loss: 65.268, mean_reward: -132.555, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.63049077  0.77619675  4.74875998] distance:  4.8529085465419985\n",
      "\n",
      "Episode: 871, actor_loss: -4.788, critic_loss: 32.757, mean_reward: -178.808, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.26018896  0.96555772 -2.59869071] distance:  2.78445567045074\n",
      "\n",
      "Episode: 872, actor_loss: -2.143, critic_loss: 37.496, mean_reward: -263.067, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.97589757 -0.21822909  1.02644473] distance:  1.4330348170799836\n",
      "\n",
      "Episode: 873, actor_loss: 0.101, critic_loss: 10.037, mean_reward: -326.470, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99989667  0.01437557 -0.02856287] distance:  1.0004078356057704\n",
      "\n",
      "Episode: 874, actor_loss: 1.614, critic_loss: 18.597, mean_reward: -181.355, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998802  0.00489397 -0.02666356] distance:  1.0003554095195455\n",
      "\n",
      "Episode: 875, actor_loss: 1.552, critic_loss: 14.758, mean_reward: -133.395, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99988469 0.01518576 0.07374961] distance:  1.0027158144635944\n",
      "\n",
      "Episode: 876, actor_loss: 1.568, critic_loss: 18.249, mean_reward: -322.285, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999922 0.0012507  0.01809495] distance:  1.0001637002624335\n",
      "\n",
      "Episode: 877, actor_loss: 1.377, critic_loss: 12.008, mean_reward: -129.335, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999752e-01 -7.04752817e-04  8.74403663e-02] distance:  1.0038156293130673\n",
      "\n",
      "Episode: 878, actor_loss: 1.273, critic_loss: 13.848, mean_reward: -208.892, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99999771 0.0021384  0.03115089] distance:  1.0004850713131006\n",
      "\n",
      "Episode: 879, actor_loss: 11.010, critic_loss: 24.257, mean_reward: -274.522, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99883089 -0.04834094 -0.89243089] distance:  1.3403107474165221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 880, actor_loss: -8.615, critic_loss: 34.128, mean_reward: -156.482, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.97939394 -0.2019592  -5.30796638] distance:  5.401343081917432\n",
      "\n",
      "Episode: 881, actor_loss: -3.268, critic_loss: 20.994, mean_reward: -129.226, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.999474   -0.03243032  0.09548934] distance:  1.0045487615318942\n",
      "\n",
      "Episode: 882, actor_loss: -2.174, critic_loss: 3.713, mean_reward: -156.480, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99980816 -0.01958691  0.00520995] distance:  1.0000135716751193\n",
      "\n",
      "Episode: 883, actor_loss: 0.305, critic_loss: 13.362, mean_reward: -105.993, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988358 -0.01525832 -0.0015388 ] distance:  1.0000011839447485\n",
      "\n",
      "Episode: 884, actor_loss: 1.540, critic_loss: 28.190, mean_reward: -203.821, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99988037 -0.01546773  0.01966069] distance:  1.0001932527066144\n",
      "\n",
      "Episode: 885, actor_loss: 1.519, critic_loss: 18.324, mean_reward: -211.451, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999624 -0.00274145 -0.03142012] distance:  1.0004934901994778\n",
      "\n",
      "Episode: 886, actor_loss: 1.455, critic_loss: 13.775, mean_reward: -210.666, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99993747 -0.01118274 -0.02877961] distance:  1.0004140471904737\n",
      "\n",
      "Episode: 887, actor_loss: 1.510, critic_loss: 10.920, mean_reward: -237.468, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998676 -0.00514603  0.01592301] distance:  1.0001267630119974\n",
      "\n",
      "Episode: 888, actor_loss: 1.214, critic_loss: 10.534, mean_reward: -178.138, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99988874 0.01491679 0.05913531] distance:  1.0017469663853622\n",
      "\n",
      "Episode: 889, actor_loss: 11.689, critic_loss: 21.547, mean_reward: -238.016, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.82178782 -0.56979363 -0.55560602] distance:  1.1439834155126467\n",
      "\n",
      "Episode: 890, actor_loss: -10.021, critic_loss: 36.614, mean_reward: -181.811, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.77455629  0.63250498  2.08633946] distance:  2.3136145597785203\n",
      "\n",
      "Episode: 891, actor_loss: -5.189, critic_loss: 19.216, mean_reward: -212.459, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99961061  0.02790404 -0.12903859] distance:  1.0082911071384861\n",
      "\n",
      "Episode: 892, actor_loss: -2.856, critic_loss: 5.557, mean_reward: -133.735, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999398   0.01097248 -0.06822288] distance:  1.0023244787812264\n",
      "\n",
      "Episode: 893, actor_loss: 1.399, critic_loss: 31.246, mean_reward: -130.645, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999982e-01  1.89368378e-04 -4.18729447e-02] distance:  1.00087628780984\n",
      "\n",
      "Episode: 894, actor_loss: 1.703, critic_loss: 11.413, mean_reward: -181.884, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99999990e-01 -1.41303227e-04  2.34646890e-02] distance:  1.0002752579310226\n",
      "\n",
      "Episode: 895, actor_loss: 1.533, critic_loss: 17.187, mean_reward: -204.370, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997676 -0.00681819  0.03108617] distance:  1.0004830584162738\n",
      "\n",
      "Episode: 896, actor_loss: 1.307, critic_loss: 12.363, mean_reward: -182.889, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99994922 -0.01007763 -0.00346588] distance:  1.000006006146196\n",
      "\n",
      "Episode: 897, actor_loss: 1.022, critic_loss: 16.300, mean_reward: -160.173, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999753   0.00702854 -0.01544872] distance:  1.000119324292223\n",
      "\n",
      "Episode: 898, actor_loss: 1.068, critic_loss: 25.532, mean_reward: -185.917, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99993587  0.01132513 -0.04184639] distance:  1.000875177146863\n",
      "\n",
      "Episode: 899, actor_loss: 9.915, critic_loss: 20.307, mean_reward: -155.194, best_return: -53.935\n",
      "last 50 episode mean reward:  -234.49141671214005\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.33704351  0.94148907  0.18126885] distance:  1.0162964114443573\n",
      "\n",
      "Episode: 900, actor_loss: -8.103, critic_loss: 22.814, mean_reward: -209.602, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997684  0.00680614 -0.07128346] distance:  1.002537446580543\n",
      "\n",
      "Episode: 901, actor_loss: -3.526, critic_loss: 32.834, mean_reward: -232.766, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.47170485 -0.8817565   5.13884511] distance:  5.2352391555073705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 902, actor_loss: -3.432, critic_loss: 9.610, mean_reward: -211.595, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.97430869  0.22521674 -1.20955489] distance:  1.5694021261508855\n",
      "\n",
      "Episode: 903, actor_loss: 1.005, critic_loss: 14.594, mean_reward: -129.802, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99584052 -0.09111346  0.29504459] distance:  1.0426175294037023\n",
      "\n",
      "Episode: 904, actor_loss: 1.892, critic_loss: 49.744, mean_reward: -229.146, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9999981  -0.0019502  -0.03530983] distance:  1.0006231977875788\n",
      "\n",
      "Episode: 905, actor_loss: 1.547, critic_loss: 7.838, mean_reward: -180.682, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99993721 0.0112064  0.02155145] distance:  1.0002322056318085\n",
      "\n",
      "Episode: 906, actor_loss: 1.694, critic_loss: 8.925, mean_reward: -187.365, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995594  0.00938697 -0.04689283] distance:  1.0010988648891233\n",
      "\n",
      "Episode: 907, actor_loss: 1.572, critic_loss: 15.073, mean_reward: -157.629, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995435 -0.00955493 -0.01299384] distance:  1.0000844163166622\n",
      "\n",
      "Episode: 908, actor_loss: 1.316, critic_loss: 9.124, mean_reward: -150.524, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998682  0.00513322 -0.0351685 ] distance:  1.0006182204355354\n",
      "\n",
      "Episode: 909, actor_loss: 11.668, critic_loss: 25.133, mean_reward: -154.046, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98752531 -0.15746035  0.5993326 ] distance:  1.1658471464788862\n",
      "\n",
      "Episode: 910, actor_loss: -9.775, critic_loss: 24.548, mean_reward: -157.988, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.10751981  0.99420294 -2.41207658] distance:  2.6111517410207057\n",
      "\n",
      "Episode: 911, actor_loss: -3.538, critic_loss: 29.707, mean_reward: -132.557, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.39179015 -0.9200546  -2.2084552 ] distance:  2.424309049390409\n",
      "\n",
      "Episode: 912, actor_loss: -2.488, critic_loss: 8.394, mean_reward: -184.286, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99994042 0.01091608 0.05726113] distance:  1.0016380768470807\n",
      "\n",
      "Episode: 913, actor_loss: 1.205, critic_loss: 13.017, mean_reward: -241.960, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99959321  0.0285205  -0.1487922 ] distance:  1.0110089607155228\n",
      "\n",
      "Episode: 914, actor_loss: 1.561, critic_loss: 18.336, mean_reward: -158.142, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99999862 -0.00166171  0.02911184] distance:  1.0004236598388618\n",
      "\n",
      "Episode: 915, actor_loss: 1.709, critic_loss: 8.335, mean_reward: -159.214, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99998422 0.00561713 0.04316887] distance:  1.000931341777255\n",
      "\n",
      "Episode: 916, actor_loss: 1.386, critic_loss: 12.570, mean_reward: -105.058, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99997068 -0.00765798 -0.01577023] distance:  1.0001243423921857\n",
      "\n",
      "Episode: 917, actor_loss: 1.315, critic_loss: 12.891, mean_reward: -224.721, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998091  0.00617886 -0.00560128] distance:  1.00001568705701\n",
      "\n",
      "Episode: 918, actor_loss: 1.087, critic_loss: 6.334, mean_reward: -194.178, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99998363 -0.00572238  0.00189603] distance:  1.0000017974577575\n",
      "\n",
      "Episode: 919, actor_loss: 10.171, critic_loss: 26.600, mean_reward: -181.311, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.40904616 -0.91251369 -0.31061432] distance:  1.0471300094210976\n",
      "\n",
      "Episode: 920, actor_loss: -8.371, critic_loss: 32.227, mean_reward: -156.971, best_return: -53.935\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.21147534  0.97738333 -1.97188412] distance:  2.210956127986354\n",
      "\n",
      "Episode: 921, actor_loss: -4.670, critic_loss: 16.408, mean_reward: -53.044, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99998042 0.00625777 0.01881448] distance:  1.0001769766249888\n",
      "\n",
      "Episode: 922, actor_loss: 0.316, critic_loss: 16.372, mean_reward: -284.507, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99995633  0.00934575 -0.0487394 ] distance:  1.0011870599068025\n",
      "\n",
      "Episode: 923, actor_loss: 1.677, critic_loss: 24.101, mean_reward: -160.177, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99984432  0.0176447  -0.00591365] distance:  1.0000174854994484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 924, actor_loss: 3.333, critic_loss: 22.761, mean_reward: -292.455, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99975218  0.02226169 -0.04794632] distance:  1.0011487651889994\n",
      "\n",
      "Episode: 925, actor_loss: 1.322, critic_loss: 16.626, mean_reward: -160.652, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99946203 0.03279712 0.04257905] distance:  1.0009060774490999\n",
      "\n",
      "Episode: 926, actor_loss: 1.064, critic_loss: 28.393, mean_reward: -307.717, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99986874 0.01620208 0.01520538] distance:  1.0001155950728493\n",
      "\n",
      "Episode: 927, actor_loss: 1.006, critic_loss: 16.949, mean_reward: -155.045, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99989976  0.01415875 -0.01555892] distance:  1.0001210326823986\n",
      "\n",
      "Episode: 928, actor_loss: 1.133, critic_loss: 17.837, mean_reward: -422.415, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999255  0.01220613 0.00789056] distance:  1.0000311299670743\n",
      "\n",
      "Episode: 929, actor_loss: 9.531, critic_loss: 34.899, mean_reward: -342.211, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.97434588  0.22505576 -0.89011591] distance:  1.3387704581862527\n",
      "\n",
      "Episode: 930, actor_loss: -6.506, critic_loss: 20.252, mean_reward: -179.496, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.2160639   0.97637922  1.10143347] distance:  1.4876678656376303\n",
      "\n",
      "Episode: 931, actor_loss: -3.900, critic_loss: 16.765, mean_reward: -234.594, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99982408 0.01875678 0.01119575] distance:  1.0000626704490327\n",
      "\n",
      "Episode: 932, actor_loss: -2.084, critic_loss: 8.298, mean_reward: -75.308, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.59649358  0.80261785 -5.97271047] distance:  6.055845966897969\n",
      "\n",
      "Episode: 933, actor_loss: 0.479, critic_loss: 2.554, mean_reward: -265.404, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9984262  -0.05608147  0.27027906] distance:  1.0358816385333125\n",
      "\n",
      "Episode: 934, actor_loss: 1.619, critic_loss: 40.843, mean_reward: -432.590, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99973438  0.02304703 -0.03495439] distance:  1.0006107180965385\n",
      "\n",
      "Episode: 935, actor_loss: 1.741, critic_loss: 40.251, mean_reward: -193.641, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 9.99895048e-01  1.44876539e-02 -8.78024064e-05] distance:  1.0000000038546313\n",
      "\n",
      "Episode: 936, actor_loss: 1.337, critic_loss: 13.193, mean_reward: -409.875, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99993545 0.01136199 0.00875147] distance:  1.0000382933571155\n",
      "\n",
      "Episode: 937, actor_loss: 1.372, critic_loss: 10.330, mean_reward: -389.602, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.9999246  0.01227964 0.01892821] distance:  1.0001791226078873\n",
      "\n",
      "Episode: 938, actor_loss: 1.086, critic_loss: 12.109, mean_reward: -135.473, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99997084 0.00763684 0.00366399] distance:  1.0000067123732992\n",
      "\n",
      "Episode: 939, actor_loss: 10.882, critic_loss: 28.356, mean_reward: -130.253, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.92717495 -0.37462862  0.55181019] distance:  1.1421446872524879\n",
      "\n",
      "Episode: 940, actor_loss: -10.088, critic_loss: 43.948, mean_reward: -334.106, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.07153472 -0.99743811  4.82283891] distance:  4.925421318417416\n",
      "\n",
      "Episode: 941, actor_loss: -3.952, critic_loss: 26.784, mean_reward: -155.712, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.05879558  0.99827004 -2.63592133] distance:  2.8192341594787127\n",
      "\n",
      "Episode: 942, actor_loss: -2.476, critic_loss: 19.872, mean_reward: -358.222, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99376897 0.11145958 0.03575019] distance:  1.0006388339928194\n",
      "\n",
      "Episode: 943, actor_loss: 1.209, critic_loss: 45.088, mean_reward: -596.707, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.8926649  -0.45072095 -1.09514319] distance:  1.4830167231324924\n",
      "\n",
      "Episode: 944, actor_loss: 1.151, critic_loss: 16.101, mean_reward: -211.902, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.98979633 0.14248939 0.00901502] distance:  1.0000406344573707\n",
      "\n",
      "Episode: 945, actor_loss: 0.778, critic_loss: 6.800, mean_reward: -347.734, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98995277  0.14139839 -0.03535897] distance:  1.0006249329482015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 946, actor_loss: 0.862, critic_loss: 23.947, mean_reward: -335.478, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98863416  0.15034129 -0.02311544] distance:  1.0002671260852212\n",
      "\n",
      "Episode: 947, actor_loss: -0.163, critic_loss: 40.994, mean_reward: -420.777, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98786384  0.15532236 -0.0135208 ] distance:  1.0000914017828684\n",
      "\n",
      "Episode: 948, actor_loss: 2.575, critic_loss: 111.058, mean_reward: -129.369, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98807175  0.1539942  -0.05580454] distance:  1.001555863062564\n",
      "\n",
      "Episode: 949, actor_loss: 12.846, critic_loss: 52.276, mean_reward: -126.984, best_return: -53.044\n",
      "last 50 episode mean reward:  -227.61986572196136\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99825427 -0.05906284  0.71593206] distance:  1.2298612610764605\n",
      "\n",
      "Episode: 950, actor_loss: -7.138, critic_loss: 79.985, mean_reward: -504.356, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.17629915  0.98433663 -5.22805603] distance:  5.322834757141137\n",
      "\n",
      "Episode: 951, actor_loss: -4.585, critic_loss: 63.599, mean_reward: -187.173, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.65736021 -0.75357651  0.54223607] distance:  1.1375499770683297\n",
      "\n",
      "Episode: 952, actor_loss: -2.987, critic_loss: 20.255, mean_reward: -375.917, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99445672 0.10514667 0.01192099] distance:  1.0000710524685121\n",
      "\n",
      "Episode: 953, actor_loss: 1.251, critic_loss: 22.023, mean_reward: -507.977, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99136156 0.13115735 0.02301524] distance:  1.0002648154933391\n",
      "\n",
      "Episode: 954, actor_loss: 2.262, critic_loss: 29.121, mean_reward: -643.384, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99370212 0.11205403 0.03892896] distance:  1.0007574451919656\n",
      "\n",
      "Episode: 955, actor_loss: 1.418, critic_loss: 44.116, mean_reward: -386.426, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99289335 0.11900758 0.02660467] distance:  1.0003538415328084\n",
      "\n",
      "Episode: 956, actor_loss: 1.611, critic_loss: 45.392, mean_reward: -594.948, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.99310544 0.11722449 0.03242015] distance:  1.0005253949218469\n",
      "\n",
      "Episode: 957, actor_loss: 1.609, critic_loss: 37.793, mean_reward: -78.076, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99314594  0.11688087 -0.02192692] distance:  1.0002403660238683\n",
      "\n",
      "Episode: 958, actor_loss: 2.058, critic_loss: 44.225, mean_reward: -578.741, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99520903  0.09777012 -0.01683874] distance:  1.0001417615850006\n",
      "\n",
      "Episode: 959, actor_loss: 12.752, critic_loss: 58.029, mean_reward: -104.814, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.39371941 -0.91923067 -0.91690946] distance:  1.3567324576531565\n",
      "\n",
      "Episode: 960, actor_loss: -10.045, critic_loss: 35.310, mean_reward: -646.629, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.3323396   0.94315979 -6.51750096] distance:  6.593771206400032\n",
      "\n",
      "Episode: 961, actor_loss: -1.586, critic_loss: 48.987, mean_reward: -580.911, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.36481778 -0.93107894 -4.97543718] distance:  5.0749359711915965\n",
      "\n",
      "Episode: 962, actor_loss: -2.662, critic_loss: 12.128, mean_reward: -610.882, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.91087213 -0.41268869 -4.03831547] distance:  4.16028746835694\n",
      "\n",
      "Episode: 963, actor_loss: 1.246, critic_loss: 8.066, mean_reward: -317.158, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.63620477 -0.77152025 -4.475652  ] distance:  4.58600706594569\n",
      "\n",
      "Episode: 964, actor_loss: 0.547, critic_loss: 5.426, mean_reward: -649.464, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.63014559  0.776477   -5.31282217] distance:  5.406114999288228\n",
      "\n",
      "Episode: 965, actor_loss: 1.397, critic_loss: 4.775, mean_reward: -389.251, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99970286 -0.02437593 -0.01442814] distance:  1.0001040801301784\n",
      "\n",
      "Episode: 966, actor_loss: 1.095, critic_loss: 6.681, mean_reward: -150.082, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99902151 -0.04422705 -0.00129842] distance:  1.0000008429514669\n",
      "\n",
      "Episode: 967, actor_loss: 0.139, critic_loss: 3.771, mean_reward: -369.695, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.52900732 -0.84861726 -6.63665179] distance:  6.711568142862602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 968, actor_loss: 0.611, critic_loss: 3.840, mean_reward: -1085.970, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930313 -0.03732641 -0.04871599] distance:  1.0011859204175644\n",
      "\n",
      "Episode: 969, actor_loss: 7.061, critic_loss: 213.702, mean_reward: -359.233, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.36929621 -0.92931174  0.8511774 ] distance:  1.3132033204658224\n",
      "\n",
      "Episode: 970, actor_loss: -4.765, critic_loss: 31.532, mean_reward: -1149.677, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.93900755  0.34389651  2.05320154] distance:  2.2837768213820366\n",
      "\n",
      "Episode: 971, actor_loss: 0.447, critic_loss: 27.602, mean_reward: -409.519, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.95003671 -0.3121382   0.26865233] distance:  1.0354583886595456\n",
      "\n",
      "Episode: 972, actor_loss: 0.722, critic_loss: 30.535, mean_reward: -250.578, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.95958106 -0.28143239 -1.56774715] distance:  1.8595244350592808\n",
      "\n",
      "Episode: 973, actor_loss: 1.112, critic_loss: 16.017, mean_reward: -620.424, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.63576164  0.77188544 -7.24701527] distance:  7.315683854705151\n",
      "\n",
      "Episode: 974, actor_loss: -0.955, critic_loss: 9.945, mean_reward: -357.364, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96463868 -0.26357581  0.01366053] distance:  1.0000933006268695\n",
      "\n",
      "Episode: 975, actor_loss: -1.988, critic_loss: 7.114, mean_reward: -1177.151, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.07348502 -0.99729632 -6.51541158] distance:  6.591706007635938\n",
      "\n",
      "Episode: 976, actor_loss: -2.322, critic_loss: 5.875, mean_reward: -602.392, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.87724917  0.48003531 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 977, actor_loss: -2.041, critic_loss: 6.412, mean_reward: -132.678, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.71708361  0.69698716 -6.31567797] distance:  6.394355967630903\n",
      "\n",
      "Episode: 978, actor_loss: -2.057, critic_loss: 4.765, mean_reward: -201.382, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96358863 -0.26738914 -0.01129272] distance:  1.0000637606809277\n",
      "\n",
      "Episode: 979, actor_loss: -12.037, critic_loss: 172.445, mean_reward: -843.258, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.05013698  0.99874235  0.42327704] distance:  1.0858929269359894\n",
      "\n",
      "Episode: 980, actor_loss: -1.443, critic_loss: 20.362, mean_reward: -408.493, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.78852142 -0.61500729  6.2187262 ] distance:  6.298615366664001\n",
      "\n",
      "Episode: 981, actor_loss: -2.216, critic_loss: 29.386, mean_reward: -393.632, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.42124871 -0.90694516  5.40602593] distance:  5.497737388248569\n",
      "\n",
      "Episode: 982, actor_loss: -0.523, critic_loss: 21.364, mean_reward: -1311.438, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.88067129  0.47372785 -4.82914238] distance:  4.931593674126667\n",
      "\n",
      "Episode: 983, actor_loss: 0.494, critic_loss: 12.345, mean_reward: -1011.767, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.0226959  -0.99974241 -6.31878786] distance:  6.397427606622463\n",
      "\n",
      "Episode: 984, actor_loss: -0.980, critic_loss: 1.622, mean_reward: -1058.912, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.87826232  0.47817914 -6.11220833] distance:  6.193471618338668\n",
      "\n",
      "Episode: 985, actor_loss: -1.312, critic_loss: 1.338, mean_reward: -1074.114, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96862048  0.2485445  -6.06762149] distance:  6.149474006375906\n",
      "\n",
      "Episode: 986, actor_loss: -1.396, critic_loss: 1.182, mean_reward: -550.992, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.94738588  0.32009374 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 987, actor_loss: -1.255, critic_loss: 1.090, mean_reward: -529.181, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.89711574 -0.44179559 -5.56391856] distance:  5.653069050324531\n",
      "\n",
      "Episode: 988, actor_loss: -0.979, critic_loss: 1.318, mean_reward: -1058.522, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.01912143  0.99981717 -7.37028678] distance:  7.437817367866994\n",
      "\n",
      "Episode: 989, actor_loss: -4.054, critic_loss: 4.523, mean_reward: -1062.037, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.91618717 -0.40075063 -0.47102098] distance:  1.1053781083384813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 990, actor_loss: -0.365, critic_loss: 37.570, mean_reward: -1290.248, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.68738886  0.72628958 -2.78645528] distance:  2.9604616243825856\n",
      "\n",
      "Episode: 991, actor_loss: -0.793, critic_loss: 16.987, mean_reward: -1342.957, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.70617491 -0.70803743 -1.3123541 ] distance:  1.6499312945285696\n",
      "\n",
      "Episode: 992, actor_loss: 1.681, critic_loss: 11.080, mean_reward: -1120.454, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9032083   0.42920249 -4.94986888] distance:  5.049871477049249\n",
      "\n",
      "Episode: 993, actor_loss: 0.427, critic_loss: 7.129, mean_reward: -1329.467, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.75477166  0.65598761 -6.26658154] distance:  6.345868273424118\n",
      "\n",
      "Episode: 994, actor_loss: -0.727, critic_loss: 4.969, mean_reward: -604.156, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.26195688  0.96507958 -6.98165472] distance:  7.05290739108543\n",
      "\n",
      "Episode: 995, actor_loss: 0.119, critic_loss: 1.717, mean_reward: -1151.373, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.31242539 -0.9499423  -6.09191798] distance:  6.173448358046935\n",
      "\n",
      "Episode: 996, actor_loss: 0.131, critic_loss: 1.765, mean_reward: -1388.036, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.59842689 -0.80117742 -5.73993355] distance:  5.826391430682978\n",
      "\n",
      "Episode: 997, actor_loss: 0.091, critic_loss: 1.250, mean_reward: -779.799, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9682507  -0.24998117 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 998, actor_loss: 0.082, critic_loss: 1.168, mean_reward: -1364.217, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.35852532 -0.93352    -6.96781839] distance:  7.039211118662736\n",
      "\n",
      "Episode: 999, actor_loss: -3.324, critic_loss: 6.287, mean_reward: -857.112, best_return: -53.044\n",
      "last 50 episode mean reward:  -691.0484649695193\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.54037934 -0.84142151 -0.80389246] distance:  1.283060050156793\n",
      "\n",
      "Episode: 1000, actor_loss: -3.317, critic_loss: 28.497, mean_reward: -1346.396, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.48972036  0.87187956 -5.24069749] distance:  5.33525165037993\n",
      "\n",
      "Episode: 1001, actor_loss: 0.048, critic_loss: 17.033, mean_reward: -1139.960, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.01278971 -0.99991821  0.69030582] distance:  1.2151222703123836\n",
      "\n",
      "Episode: 1002, actor_loss: 3.785, critic_loss: 11.777, mean_reward: -904.346, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.95443277 -0.298426   -4.67019061] distance:  4.7760527937919095\n",
      "\n",
      "Episode: 1003, actor_loss: 3.166, critic_loss: 11.858, mean_reward: -1146.879, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99911739  0.04200531 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1004, actor_loss: -0.165, critic_loss: 2.536, mean_reward: -1464.564, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.50519894 -0.86300292 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1005, actor_loss: -1.231, critic_loss: 0.996, mean_reward: -836.501, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83134001  0.55576415 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1006, actor_loss: -1.038, critic_loss: 0.918, mean_reward: -1458.148, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 4.26213576e-03  9.99990917e-01 -7.34369580e+00] distance:  7.411468680516429\n",
      "\n",
      "Episode: 1007, actor_loss: -0.664, critic_loss: 0.935, mean_reward: -857.435, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.63018241  0.77644712 -6.47455057] distance:  6.551320866040246\n",
      "\n",
      "Episode: 1008, actor_loss: -0.149, critic_loss: 1.388, mean_reward: -1458.671, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9123153  -0.40948845 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1009, actor_loss: -4.256, critic_loss: 5.738, mean_reward: -1159.818, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.45810522 -0.88889797 -0.13896411] distance:  1.0096093421848507\n",
      "\n",
      "Episode: 1010, actor_loss: -0.003, critic_loss: 17.823, mean_reward: -885.431, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.39674244 -0.91792997  2.70362352] distance:  2.8826342321553233\n",
      "\n",
      "Episode: 1011, actor_loss: 1.609, critic_loss: 21.736, mean_reward: -878.567, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.12175069  0.99256071 -5.25177157] distance:  5.346129872656406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1012, actor_loss: 2.887, critic_loss: 12.706, mean_reward: -1493.220, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.01116573  0.99993766 -6.93577239] distance:  7.007491609098637\n",
      "\n",
      "Episode: 1013, actor_loss: 0.231, critic_loss: 3.147, mean_reward: -1192.853, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95272925  0.30382064 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1014, actor_loss: -1.001, critic_loss: 0.907, mean_reward: -872.131, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.77508475  0.63185728 -6.24563464] distance:  6.325183952031607\n",
      "\n",
      "Episode: 1015, actor_loss: -1.251, critic_loss: 0.755, mean_reward: -1478.517, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95260331  0.30421527 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1016, actor_loss: -0.862, critic_loss: 0.875, mean_reward: -1183.837, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.03888095 -0.99924385 -7.37010207] distance:  7.437634339465859\n",
      "\n",
      "Episode: 1017, actor_loss: -0.406, critic_loss: 1.100, mean_reward: -1446.609, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.90038118 -0.43510198 -6.31428085] distance:  6.392976037117468\n",
      "\n",
      "Episode: 1018, actor_loss: -0.011, critic_loss: 1.102, mean_reward: -900.465, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.20457189 -0.97885154 -7.68739521] distance:  7.752163897234684\n",
      "\n",
      "Episode: 1019, actor_loss: -4.323, critic_loss: 6.554, mean_reward: -869.617, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.47954228 -0.87751878  0.29321979] distance:  1.0421026070675659\n",
      "\n",
      "Episode: 1020, actor_loss: -0.394, critic_loss: 15.907, mean_reward: -850.768, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.35571787 -0.93459338  2.83216007] distance:  3.00351971358419\n",
      "\n",
      "Episode: 1021, actor_loss: -2.444, critic_loss: 12.923, mean_reward: -1467.195, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.13071403 -0.99142011  0.96500294] distance:  1.3896872585915023\n",
      "\n",
      "Episode: 1022, actor_loss: 2.169, critic_loss: 14.457, mean_reward: -610.526, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.86533414 -0.50119539 -6.32683134] distance:  6.405372339087673\n",
      "\n",
      "Episode: 1023, actor_loss: 2.430, critic_loss: 17.356, mean_reward: -1478.038, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98331115 -0.1819318  -6.26714445] distance:  6.346424153636601\n",
      "\n",
      "Episode: 1024, actor_loss: -1.160, critic_loss: 5.958, mean_reward: -1226.834, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9938377  -0.11084504 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1025, actor_loss: -1.496, critic_loss: 3.772, mean_reward: -938.117, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.57282159  0.81968007 -6.54289136] distance:  6.618869043434432\n",
      "\n",
      "Episode: 1026, actor_loss: -0.673, critic_loss: 3.733, mean_reward: -1500.139, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.81405105 -0.58079333 -6.82964713] distance:  6.902469115689776\n",
      "\n",
      "Episode: 1027, actor_loss: 0.307, critic_loss: 4.751, mean_reward: -1232.099, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.72159573 -0.69231467 -7.06523536] distance:  7.135653484701499\n",
      "\n",
      "Episode: 1028, actor_loss: -0.285, critic_loss: 4.136, mean_reward: -1526.657, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96981253 -0.2438517  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1029, actor_loss: -4.500, critic_loss: 9.258, mean_reward: -1216.413, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.17006961 -0.98543205  0.72822677] distance:  1.2370587024937423\n",
      "\n",
      "Episode: 1030, actor_loss: 0.330, critic_loss: 18.840, mean_reward: -908.632, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98595723  0.166998   -5.59820251] distance:  5.686815570408704\n",
      "\n",
      "Episode: 1031, actor_loss: 0.847, critic_loss: 11.401, mean_reward: -1508.385, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.77087026 -0.63699219 -4.85044408] distance:  4.952454723253232\n",
      "\n",
      "Episode: 1032, actor_loss: 2.533, critic_loss: 6.969, mean_reward: -1532.290, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99249208  0.12230892 -6.06774838] distance:  6.149599208068143\n",
      "\n",
      "Episode: 1033, actor_loss: 0.091, critic_loss: 3.982, mean_reward: -1518.279, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.98565378 -0.16877981 -8.        ] distance:  8.06225774829855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1034, actor_loss: -1.487, critic_loss: 1.195, mean_reward: -1211.374, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.94275748 -0.33347914 -6.48523662] distance:  6.561881893111877\n",
      "\n",
      "Episode: 1035, actor_loss: -1.417, critic_loss: 1.030, mean_reward: -1493.700, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.30707838  0.95168423 -7.68025107] distance:  7.745079499010081\n",
      "\n",
      "Episode: 1036, actor_loss: -0.773, critic_loss: 1.767, mean_reward: -1180.976, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.74563784 -0.66635142 -7.09058514] distance:  7.1607539878354265\n",
      "\n",
      "Episode: 1037, actor_loss: -0.253, critic_loss: 1.886, mean_reward: -1591.203, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.57041785  0.82135466 -7.89591644] distance:  7.95898840120142\n",
      "\n",
      "Episode: 1038, actor_loss: -0.356, critic_loss: 1.596, mean_reward: -1543.395, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.54674267 -0.8373007  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1039, actor_loss: -5.618, critic_loss: 8.101, mean_reward: -1186.415, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.36605047 -0.930595   -0.2223059 ] distance:  1.0244119835010201\n",
      "\n",
      "Episode: 1040, actor_loss: 0.661, critic_loss: 8.944, mean_reward: -1238.332, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.592923   -0.80525916 -1.8844005 ] distance:  2.1332991483046406\n",
      "\n",
      "Episode: 1041, actor_loss: 0.068, critic_loss: 6.333, mean_reward: -1478.273, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.11376459 -0.99350773  0.21683636] distance:  1.023238977855677\n",
      "\n",
      "Episode: 1042, actor_loss: 1.655, critic_loss: 4.251, mean_reward: -1226.232, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.23907322 -0.97100154 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1043, actor_loss: -0.109, critic_loss: 2.528, mean_reward: -356.385, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.70587775  0.70833368 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1044, actor_loss: -0.914, critic_loss: 0.418, mean_reward: -993.669, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.93862013  0.34495253 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1045, actor_loss: -0.543, critic_loss: 0.519, mean_reward: -1012.467, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99389556  0.11032506 -6.07318973] distance:  6.154968196853721\n",
      "\n",
      "Episode: 1046, actor_loss: -0.444, critic_loss: 0.387, mean_reward: -1279.465, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.34043706  0.94026731 -7.72478035] distance:  7.789238179244866\n",
      "\n",
      "Episode: 1047, actor_loss: -0.605, critic_loss: 0.261, mean_reward: -935.231, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.70442907  0.70977439 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1048, actor_loss: -0.801, critic_loss: 0.173, mean_reward: -1021.741, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.1488681   0.98885706 -7.12176144] distance:  7.191626104562877\n",
      "\n",
      "Episode: 1049, actor_loss: -3.977, critic_loss: 4.704, mean_reward: -985.225, best_return: -53.044\n",
      "last 50 episode mean reward:  -1184.4483606497922\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.73907191  0.67362654 -0.78132578] distance:  1.2690429349673846\n",
      "\n",
      "Episode: 1050, actor_loss: 1.250, critic_loss: 6.819, mean_reward: -1513.547, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.98885036 -0.14891263 -7.43321086] distance:  7.5001749134448366\n",
      "\n",
      "Episode: 1051, actor_loss: 0.409, critic_loss: 5.064, mean_reward: -1467.191, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.60003997 -0.79997002 -7.46011974] distance:  7.526844390462838\n",
      "\n",
      "Episode: 1052, actor_loss: 0.455, critic_loss: 5.382, mean_reward: -1512.775, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.7883018   0.61528878 -4.38526245] distance:  4.497835787807658\n",
      "\n",
      "Episode: 1053, actor_loss: -0.315, critic_loss: 2.720, mean_reward: -1289.175, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98723152 -0.15929197 -6.24421835] distance:  6.32378547838942\n",
      "\n",
      "Episode: 1054, actor_loss: -0.380, critic_loss: 2.577, mean_reward: -1531.800, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99257569  0.12162851 -6.06803565] distance:  6.149882657782512\n",
      "\n",
      "Episode: 1055, actor_loss: -0.077, critic_loss: 2.644, mean_reward: -1514.666, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.66951724 -0.74279652 -8.        ] distance:  8.06225774829855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1056, actor_loss: -0.952, critic_loss: 1.666, mean_reward: -1511.662, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.81143205 -0.58444677 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1057, actor_loss: -1.381, critic_loss: 1.590, mean_reward: -1535.672, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.74136551 -0.67110147 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1058, actor_loss: -0.869, critic_loss: 2.061, mean_reward: -1534.126, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98200876  0.18883537 -6.05341768] distance:  6.135459688260797\n",
      "\n",
      "Episode: 1059, actor_loss: -3.805, critic_loss: 5.432, mean_reward: -1536.616, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.6372685  0.77064185 0.7305669 ] distance:  1.2384377248302851\n",
      "\n",
      "Episode: 1060, actor_loss: 1.163, critic_loss: 3.178, mean_reward: -1550.463, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.6856612  -0.72792082 -1.33131963] distance:  1.6650561451751111\n",
      "\n",
      "Episode: 1061, actor_loss: 0.640, critic_loss: 3.095, mean_reward: -1228.758, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.33231127 -0.94316977 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1062, actor_loss: 1.206, critic_loss: 2.934, mean_reward: -1262.851, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.15523695  0.98787726 -7.50083979] distance:  7.567205398678386\n",
      "\n",
      "Episode: 1063, actor_loss: -0.443, critic_loss: 3.266, mean_reward: -1523.794, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.60422979 -0.79681011 -7.44885632] distance:  7.515680975472859\n",
      "\n",
      "Episode: 1064, actor_loss: -0.892, critic_loss: 1.862, mean_reward: -1527.799, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.53899811  0.84230698 -6.58046632] distance:  6.656015096492188\n",
      "\n",
      "Episode: 1065, actor_loss: -0.011, critic_loss: 2.391, mean_reward: -1527.625, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99760634  0.06914903 -6.09773139] distance:  6.179185066743655\n",
      "\n",
      "Episode: 1066, actor_loss: 0.181, critic_loss: 2.412, mean_reward: -1240.162, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.10845203 -0.99410168 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1067, actor_loss: -0.491, critic_loss: 1.650, mean_reward: -1506.702, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.13847127  0.99036645 -7.48325821] distance:  7.549778367477952\n",
      "\n",
      "Episode: 1068, actor_loss: -1.175, critic_loss: 1.719, mean_reward: -1485.793, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.28783504  0.95768001 -7.65543251] distance:  7.720469347999399\n",
      "\n",
      "Episode: 1069, actor_loss: -3.196, critic_loss: 5.152, mean_reward: -1508.323, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.74533568 0.66668937 0.5892822 ] distance:  1.1607124999765797\n",
      "\n",
      "Episode: 1070, actor_loss: 4.238, critic_loss: 6.297, mean_reward: -887.243, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.33572449 -0.94196023  2.46161712] distance:  2.656983032199363\n",
      "\n",
      "Episode: 1071, actor_loss: -0.428, critic_loss: 5.559, mean_reward: -785.649, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96632015 -0.2573429  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1072, actor_loss: 1.687, critic_loss: 7.081, mean_reward: -491.348, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.83559559  0.54934508 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1073, actor_loss: -0.795, critic_loss: 1.846, mean_reward: -1045.553, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.83689579  0.54736225 -6.19197245] distance:  6.272202389251837\n",
      "\n",
      "Episode: 1074, actor_loss: -0.677, critic_loss: 1.308, mean_reward: -936.755, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.55723319  0.83035605 -7.88555758] distance:  7.948711743103576\n",
      "\n",
      "Episode: 1075, actor_loss: 0.393, critic_loss: 1.783, mean_reward: -419.759, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98347051  0.18106838 -6.05358737] distance:  6.135627115140363\n",
      "\n",
      "Episode: 1076, actor_loss: 0.025, critic_loss: 1.495, mean_reward: -657.706, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.58352767 -0.81209326 -7.50461157] distance:  7.570944113499237\n",
      "\n",
      "Episode: 1077, actor_loss: -0.667, critic_loss: 1.023, mean_reward: -459.947, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.13179895  0.99127647 -7.47638028] distance:  7.5429610927808195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1078, actor_loss: -0.756, critic_loss: 1.134, mean_reward: -754.375, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.6236552   0.78169955 -6.48887224] distance:  6.5654750704176905\n",
      "\n",
      "Episode: 1079, actor_loss: -1.881, critic_loss: 3.674, mean_reward: -958.723, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99930155 -0.03736869 -0.23167491] distance:  1.026485880143688\n",
      "\n",
      "Episode: 1080, actor_loss: 1.468, critic_loss: 5.561, mean_reward: -711.317, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.19306554  0.98118586 -1.33636555] distance:  1.6690934295759627\n",
      "\n",
      "Episode: 1081, actor_loss: 1.146, critic_loss: 8.806, mean_reward: -466.307, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.32634365 -0.9452512   2.4144987 ] distance:  2.613389360420285\n",
      "\n",
      "Episode: 1082, actor_loss: 3.680, critic_loss: 6.778, mean_reward: -671.740, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.84692069 -0.53171924 -6.66685517] distance:  6.74143588643515\n",
      "\n",
      "Episode: 1083, actor_loss: -0.333, critic_loss: 1.852, mean_reward: -1447.712, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.37715511 -0.92615011 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1084, actor_loss: -1.704, critic_loss: 1.601, mean_reward: -1573.757, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.64515788  0.76404928 -6.4429107 ] distance:  6.520053551345991\n",
      "\n",
      "Episode: 1085, actor_loss: -1.262, critic_loss: 1.788, mean_reward: -1454.329, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.92917579  0.36963813 -6.07173769] distance:  6.153535450416489\n",
      "\n",
      "Episode: 1086, actor_loss: -0.371, critic_loss: 2.289, mean_reward: -1087.317, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.87471121  0.48464451 -6.11759621] distance:  6.198788866667874\n",
      "\n",
      "Episode: 1087, actor_loss: 0.015, critic_loss: 2.568, mean_reward: -1392.840, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.39168464 -0.92009953 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1088, actor_loss: -0.723, critic_loss: 1.680, mean_reward: -1388.899, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.35146895 -0.93619954 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1089, actor_loss: -4.592, critic_loss: 6.917, mean_reward: -1243.506, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.74894093 -0.66263677  0.95819526] distance:  1.3849686484225507\n",
      "\n",
      "Episode: 1090, actor_loss: 1.022, critic_loss: 6.715, mean_reward: -1217.597, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.45854689 -0.88867021  2.14900875] distance:  2.370282393780285\n",
      "\n",
      "Episode: 1091, actor_loss: 2.237, critic_loss: 7.098, mean_reward: -1268.257, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9934184  -0.11454207 -6.20779662] distance:  6.2878246508767015\n",
      "\n",
      "Episode: 1092, actor_loss: 1.295, critic_loss: 2.406, mean_reward: -1328.428, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.94130268  0.33756372 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1093, actor_loss: -0.289, critic_loss: 0.787, mean_reward: -1489.702, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95909168 -0.28309564 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1094, actor_loss: -0.892, critic_loss: 0.198, mean_reward: -1248.308, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.54987134  0.83524937 -6.56783208] distance:  6.643524538575903\n",
      "\n",
      "Episode: 1095, actor_loss: -0.652, critic_loss: 0.293, mean_reward: -1405.094, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.24337552  0.96993214 -7.00261954] distance:  7.073661032940384\n",
      "\n",
      "Episode: 1096, actor_loss: -0.633, critic_loss: 0.272, mean_reward: -1382.112, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.77902289  0.62699549 -6.24204755] distance:  6.321642003872902\n",
      "\n",
      "Episode: 1097, actor_loss: -0.601, critic_loss: 0.194, mean_reward: -1241.878, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.25059049 -0.96809318 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1098, actor_loss: -0.705, critic_loss: 0.122, mean_reward: -1362.277, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.04033333  0.99918628 -7.28389527] distance:  7.352219408892313\n",
      "\n",
      "Episode: 1099, actor_loss: -4.672, critic_loss: 5.076, mean_reward: -1465.129, best_return: -53.044\n",
      "last 50 episode mean reward:  -1231.0612341001088\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.58439594  0.81146866 -0.15976711] distance:  1.0126823434250811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1100, actor_loss: 0.271, critic_loss: 4.893, mean_reward: -1339.263, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.26120593 -0.9652831   3.59956862] distance:  3.7358926966459345\n",
      "\n",
      "Episode: 1101, actor_loss: 3.384, critic_loss: 7.484, mean_reward: -1222.280, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.61774667 -0.78637717 -1.36463277] distance:  1.6918104521938073\n",
      "\n",
      "Episode: 1102, actor_loss: 0.842, critic_loss: 1.425, mean_reward: -1471.650, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.36439355  0.93124505 -7.75679757] distance:  7.8209915353120385\n",
      "\n",
      "Episode: 1103, actor_loss: -0.240, critic_loss: 0.972, mean_reward: -1449.130, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.69956487  0.71456909 -6.34275711] distance:  6.4211033159208375\n",
      "\n",
      "Episode: 1104, actor_loss: -0.926, critic_loss: 0.532, mean_reward: -1352.809, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.51198183  0.85899628 -7.85166888] distance:  7.915093447265148\n",
      "\n",
      "Episode: 1105, actor_loss: -1.126, critic_loss: 0.594, mean_reward: -1392.208, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.04379384 -0.99904059 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1106, actor_loss: -0.658, critic_loss: 0.770, mean_reward: -1407.259, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.32687139  0.94506883 -7.70643872] distance:  7.771048689958352\n",
      "\n",
      "Episode: 1107, actor_loss: -0.123, critic_loss: 0.993, mean_reward: -1438.634, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.87920494  0.47644378 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1108, actor_loss: -0.205, critic_loss: 0.939, mean_reward: -1353.705, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.96771109 -0.25206198 -6.35502775] distance:  6.433224522604903\n",
      "\n",
      "Episode: 1109, actor_loss: -3.776, critic_loss: 5.945, mean_reward: -1484.919, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.91732443  0.39814055 -0.20003929] distance:  1.0198116080083528\n",
      "\n",
      "Episode: 1110, actor_loss: 0.005, critic_loss: 3.606, mean_reward: -1551.349, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.23389537 -0.97226177  0.63513078] distance:  1.1846480937812645\n",
      "\n",
      "Episode: 1111, actor_loss: 2.095, critic_loss: 4.610, mean_reward: -1488.345, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.90926327  0.41622147 -7.88185449] distance:  7.945038086313638\n",
      "\n",
      "Episode: 1112, actor_loss: 0.457, critic_loss: 2.282, mean_reward: -1434.761, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.59542229 -0.80341291 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1113, actor_loss: -0.504, critic_loss: 0.433, mean_reward: -1432.245, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.87699536  0.48049885 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1114, actor_loss: -0.612, critic_loss: 0.417, mean_reward: -1327.945, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.87120127 -0.49092601 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1115, actor_loss: -0.753, critic_loss: 0.269, mean_reward: -1308.226, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.94418673  0.32941072 -6.07650556] distance:  6.158239992287877\n",
      "\n",
      "Episode: 1116, actor_loss: -0.748, critic_loss: 0.160, mean_reward: -1468.925, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.05462368  0.99850701 -7.40168541] distance:  7.468932111658422\n",
      "\n",
      "Episode: 1117, actor_loss: -0.699, critic_loss: 0.225, mean_reward: -1455.369, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99876732  0.04963718 -6.11230448] distance:  6.193566504065046\n",
      "\n",
      "Episode: 1118, actor_loss: -0.484, critic_loss: 0.292, mean_reward: -1419.257, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.74851347  0.66311959 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1119, actor_loss: -4.030, critic_loss: 5.362, mean_reward: -1205.377, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.45697018  0.88948201  0.15343888] distance:  1.011703262515751\n",
      "\n",
      "Episode: 1120, actor_loss: -0.128, critic_loss: 4.694, mean_reward: -1437.114, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.29644853 -0.95504883  2.46612035] distance:  2.6611556817951887\n",
      "\n",
      "Episode: 1121, actor_loss: 2.019, critic_loss: 7.057, mean_reward: -1463.594, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9912494  -0.13200239 -6.2205268 ] distance:  6.300393135180797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1122, actor_loss: 0.939, critic_loss: 1.389, mean_reward: -1336.500, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.4362471   0.89982691 -7.80020423] distance:  7.864043875232031\n",
      "\n",
      "Episode: 1123, actor_loss: -0.381, critic_loss: 0.951, mean_reward: -1241.024, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.36512313  0.93095924 -6.84894176] distance:  6.921560753244717\n",
      "\n",
      "Episode: 1124, actor_loss: -1.025, critic_loss: 0.850, mean_reward: -1230.696, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.91870452  0.39494558 -6.07366786] distance:  6.155439973363184\n",
      "\n",
      "Episode: 1125, actor_loss: -0.405, critic_loss: 1.054, mean_reward: -1384.021, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.22807609  0.97364331 -7.58228039] distance:  7.6479393281240915\n",
      "\n",
      "Episode: 1126, actor_loss: -0.042, critic_loss: 1.180, mean_reward: -1338.408, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.2419076  -0.97029929 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1127, actor_loss: -0.442, critic_loss: 0.887, mean_reward: -1438.059, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.16752088  0.98586853 -7.5139949 ] distance:  7.580245331985386\n",
      "\n",
      "Episode: 1128, actor_loss: -0.816, critic_loss: 0.655, mean_reward: -1190.546, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99602151 -0.08911313 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1129, actor_loss: -3.690, critic_loss: 4.016, mean_reward: -1333.183, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99286734  0.11922436 -0.32572107] distance:  1.0517101374716264\n",
      "\n",
      "Episode: 1130, actor_loss: 1.414, critic_loss: 6.030, mean_reward: -1395.689, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.08068905 -0.99673932 -0.25689449] distance:  1.032470232033789\n",
      "\n",
      "Episode: 1131, actor_loss: 3.237, critic_loss: 3.594, mean_reward: -1452.553, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95241395 -0.30480758 -7.01910105] distance:  7.0899773979787035\n",
      "\n",
      "Episode: 1132, actor_loss: 1.482, critic_loss: 1.069, mean_reward: -1528.317, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.57271927 -0.81975157 -7.53380119] distance:  7.599878973854311\n",
      "\n",
      "Episode: 1133, actor_loss: -0.203, critic_loss: 0.754, mean_reward: -1528.976, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.71020596 -0.70399396 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1134, actor_loss: -0.810, critic_loss: 0.584, mean_reward: -1520.394, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.55424201 -0.83235557 -7.58377584] distance:  7.649421937628172\n",
      "\n",
      "Episode: 1135, actor_loss: -0.950, critic_loss: 0.344, mean_reward: -1487.740, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.45442671 -0.89078413 -7.84613087] distance:  7.909599837407845\n",
      "\n",
      "Episode: 1136, actor_loss: -0.981, critic_loss: 0.393, mean_reward: -1518.479, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.89608228 -0.44388799 -6.61470371] distance:  6.689865852688603\n",
      "\n",
      "Episode: 1137, actor_loss: -0.869, critic_loss: 0.407, mean_reward: -1551.473, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.75168888  0.65951788 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1138, actor_loss: -0.435, critic_loss: 0.603, mean_reward: -1511.677, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.16526403  0.98624936 -7.09960757] distance:  7.1696881149498655\n",
      "\n",
      "Episode: 1139, actor_loss: -5.267, critic_loss: 6.041, mean_reward: -1464.666, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99994444  0.010541   -0.05764054] distance:  1.0016598381474509\n",
      "\n",
      "Episode: 1140, actor_loss: 0.960, critic_loss: 3.460, mean_reward: -1548.364, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.43752496 -0.89920627  3.38150569] distance:  3.526270087613524\n",
      "\n",
      "Episode: 1141, actor_loss: 0.558, critic_loss: 4.880, mean_reward: -1540.674, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.16029998 -0.98706834 -0.01625975] distance:  1.0001321810149537\n",
      "\n",
      "Episode: 1142, actor_loss: 1.568, critic_loss: 3.468, mean_reward: -1540.139, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.11234539  0.99366922 -7.17332598] distance:  7.242693252534853\n",
      "\n",
      "Episode: 1143, actor_loss: -0.150, critic_loss: 1.070, mean_reward: -1506.986, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.51995392 -0.85419431 -7.67658829] distance:  7.74144739990541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1144, actor_loss: -0.125, critic_loss: 0.183, mean_reward: -1493.081, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.89869969  0.43856456 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1145, actor_loss: -0.430, critic_loss: 0.283, mean_reward: -1558.057, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.72940151  0.68408584 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1146, actor_loss: -0.547, critic_loss: 0.211, mean_reward: -1522.938, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.9215057   0.38836482 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1147, actor_loss: -0.565, critic_loss: 0.150, mean_reward: -1499.406, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.02055416 -0.99978874 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1148, actor_loss: -0.544, critic_loss: 0.156, mean_reward: -1507.584, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.76917795  0.63903465 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1149, actor_loss: -3.306, critic_loss: 4.629, mean_reward: -1537.612, best_return: -53.044\n",
      "last 50 episode mean reward:  -1432.232164809588\n",
      "\n",
      "\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [0.81669936 0.57706338 0.92944051] distance:  1.3652324553310178\n",
      "\n",
      "Episode: 1150, actor_loss: 2.465, critic_loss: 2.406, mean_reward: -1508.715, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.2923749   0.95630378 -7.65756476] distance:  7.722583639012995\n",
      "\n",
      "Episode: 1151, actor_loss: -0.380, critic_loss: 3.837, mean_reward: -1571.004, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95108281  0.30893606 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1152, actor_loss: 0.773, critic_loss: 5.550, mean_reward: -1526.175, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.29017482  0.95697365 -6.95139109] distance:  7.022950809589335\n",
      "\n",
      "Episode: 1153, actor_loss: -0.045, critic_loss: 2.706, mean_reward: -1538.322, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.56039041  0.82822859 -7.88801745] distance:  7.951152076956343\n",
      "\n",
      "Episode: 1154, actor_loss: 0.275, critic_loss: 2.197, mean_reward: -1501.249, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.12874293  0.991678   -7.14979771] distance:  7.219391059431819\n",
      "\n",
      "Episode: 1155, actor_loss: 0.744, critic_loss: 2.358, mean_reward: -1526.440, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.04882707  0.99880725 -7.39643231] distance:  7.463726346439271\n",
      "\n",
      "Episode: 1156, actor_loss: -0.241, critic_loss: 1.964, mean_reward: -1562.679, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.95399539  0.29982126 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1157, actor_loss: -0.850, critic_loss: 1.800, mean_reward: -1488.528, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.52965118 -0.84821556 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1158, actor_loss: -0.617, critic_loss: 1.874, mean_reward: -1514.786, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.83786885  0.54587159 -6.18969087] distance:  6.269950007443302\n",
      "\n",
      "Episode: 1159, actor_loss: -1.084, critic_loss: 4.391, mean_reward: -1553.317, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.88914675 -0.45762218  0.50044121] distance:  1.1182313718245322\n",
      "\n",
      "Episode: 1160, actor_loss: 1.786, critic_loss: 4.439, mean_reward: -1520.374, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.10574022  0.99439379 -0.41543734] distance:  1.0828611082391604\n",
      "\n",
      "Episode: 1161, actor_loss: 1.633, critic_loss: 5.827, mean_reward: -1549.022, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.74050299  0.67205306 -2.44056089] distance:  2.6374869614166627\n",
      "\n",
      "Episode: 1162, actor_loss: 1.046, critic_loss: 3.391, mean_reward: -1472.269, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.90353533  0.42851361 -6.08282596] distance:  6.164476588679011\n",
      "\n",
      "Episode: 1163, actor_loss: -0.611, critic_loss: 0.554, mean_reward: -1471.575, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.81987818  0.57253801 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1164, actor_loss: -0.524, critic_loss: 0.386, mean_reward: -1493.356, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99264225  0.12108413 -6.0682674 ] distance:  6.150111317172735\n",
      "\n",
      "Episode: 1165, actor_loss: -0.422, critic_loss: 0.319, mean_reward: -1476.507, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.92597799 -0.3775775  -8.        ] distance:  8.06225774829855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1166, actor_loss: -0.364, critic_loss: 0.311, mean_reward: -1483.383, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.52778018 -0.849381   -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1167, actor_loss: -0.558, critic_loss: 0.285, mean_reward: -1501.199, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.56511725  0.8250106  -7.89172449] distance:  7.954829689742157\n",
      "\n",
      "Episode: 1168, actor_loss: -0.435, critic_loss: 0.338, mean_reward: -1549.359, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.12639418  0.9919801  -7.15313017] distance:  7.222691411204936\n",
      "\n",
      "Episode: 1169, actor_loss: -3.091, critic_loss: 3.414, mean_reward: -1527.851, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.99355621 -0.11334044 -0.06659725] distance:  1.0022151432380928\n",
      "\n",
      "Episode: 1170, actor_loss: 2.512, critic_loss: 5.873, mean_reward: -1497.079, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.32703495 -0.94501224  0.82302542] distance:  1.2951335224892817\n",
      "\n",
      "Episode: 1171, actor_loss: 0.289, critic_loss: 3.606, mean_reward: -1515.160, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.98964637  0.14352725 -6.06014663] distance:  6.1420987568303484\n",
      "\n",
      "Episode: 1172, actor_loss: 0.505, critic_loss: 1.642, mean_reward: -1503.313, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.90140452  0.43297794 -6.08465636] distance:  6.166282753087095\n",
      "\n",
      "Episode: 1173, actor_loss: 0.232, critic_loss: 2.171, mean_reward: -1524.100, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.99772648 -0.06739338 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1174, actor_loss: -0.167, critic_loss: 1.403, mean_reward: -1499.544, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.03099538  0.99951953 -7.29909395] distance:  7.3672771413053\n",
      "\n",
      "Episode: 1175, actor_loss: -1.086, critic_loss: 1.300, mean_reward: -1487.579, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.12126118  0.99262064 -7.46565518] distance:  7.532330803952958\n",
      "\n",
      "Episode: 1176, actor_loss: -0.765, critic_loss: 1.548, mean_reward: -1526.141, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.98746782  0.1578205  -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1177, actor_loss: 0.282, critic_loss: 1.694, mean_reward: -1538.078, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.42559825 -0.90491222 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1178, actor_loss: 0.436, critic_loss: 1.720, mean_reward: -1524.549, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.75762692 -0.65268786 -7.06494715] distance:  7.135368122404981\n",
      "\n",
      "Episode: 1179, actor_loss: -2.672, critic_loss: 5.791, mean_reward: -1544.727, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6600\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.17529718 -0.98451557  0.37176826] distance:  1.066870019356428\n",
      "\n",
      "Episode: 1180, actor_loss: 1.608, critic_loss: 6.409, mean_reward: -1533.304, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  520\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.94415221  0.32950965 -4.84269503] distance:  4.944865529664923\n",
      "\n",
      "Episode: 1181, actor_loss: 1.532, critic_loss: 4.689, mean_reward: -1510.688, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  840\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.90255161  0.4305817  -6.0836552 ] distance:  6.165294848205816\n",
      "\n",
      "Episode: 1182, actor_loss: 1.193, critic_loss: 1.536, mean_reward: -1506.878, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1160\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96020106 -0.27930974 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1183, actor_loss: -0.134, critic_loss: 0.674, mean_reward: -1558.186, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1480\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.47942129  0.87758488 -7.82877722] distance:  7.892385743350627\n",
      "\n",
      "Episode: 1184, actor_loss: -0.155, critic_loss: 0.552, mean_reward: -1541.954, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  1800\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.55615916 -0.83107581 -7.57858774] distance:  7.64427838796809\n",
      "\n",
      "Episode: 1185, actor_loss: -0.460, critic_loss: 0.328, mean_reward: -1533.692, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2120\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.70220936 -0.71197052 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1186, actor_loss: -0.720, critic_loss: 0.449, mean_reward: -1481.089, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2440\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.22716108  0.9738572  -7.58120544] distance:  7.646873608220451\n",
      "\n",
      "Episode: 1187, actor_loss: -0.344, critic_loss: 0.419, mean_reward: -1558.327, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  2760\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.64446823  0.76463109 -6.44433016] distance:  6.521456221932763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1188, actor_loss: -0.013, critic_loss: 0.493, mean_reward: -1497.328, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3080\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.48068218  0.87689489 -7.82964161] distance:  7.893243166040748\n",
      "\n",
      "Episode: 1189, actor_loss: -2.050, critic_loss: 4.224, mean_reward: -1539.293, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3400\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.53427172 -0.8453128  -0.03340721] distance:  1.0005578653660556\n",
      "\n",
      "Episode: 1190, actor_loss: 1.314, critic_loss: 3.540, mean_reward: -1522.936, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  3720\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.71367033  0.70048174 -1.6351675 ] distance:  1.9167088331834052\n",
      "\n",
      "Episode: 1191, actor_loss: 0.659, critic_loss: 3.464, mean_reward: -1540.541, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4040\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.16874886  0.98565908 -5.43406338] distance:  5.525309473504833\n",
      "\n",
      "Episode: 1192, actor_loss: 1.512, critic_loss: 1.690, mean_reward: -1485.106, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4360\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.96524558 -0.26134453 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1193, actor_loss: 0.265, critic_loss: 0.383, mean_reward: -1546.901, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  4680\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.58499226 -0.81103887 -7.5006597 ] distance:  7.5670268860200425\n",
      "\n",
      "Episode: 1194, actor_loss: -0.261, critic_loss: 0.236, mean_reward: -1540.356, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5000\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.76021667 -0.64966962 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1195, actor_loss: -0.390, critic_loss: 0.124, mean_reward: -1530.933, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5320\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.25294767 -0.96747996 -8.        ] distance:  8.06225774829855\n",
      "\n",
      "Episode: 1196, actor_loss: -0.341, critic_loss: 0.145, mean_reward: -1510.608, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5640\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.9768652   0.21385599 -6.0558865 ] distance:  6.137895513630244\n",
      "\n",
      "Episode: 1197, actor_loss: -0.148, critic_loss: 0.178, mean_reward: -1496.821, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  5960\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [ 0.76875011 -0.63954927 -7.03992184] distance:  7.110590661063496\n",
      "\n",
      "Episode: 1198, actor_loss: 0.070, critic_loss: 0.244, mean_reward: -1516.454, best_return: -53.044\n",
      "generate goal using supervised model based...\n",
      "len after concat dataset:  6280\n",
      "len filtered concat dataset:  0\n",
      "mb subgoal:  [ 0.85775274 -0.51626986 -0.15860233]  distance:  1.0136217405331878\n",
      "sample final subgoal:  [-0.46734264  0.88407628 -7.82058444] distance:  7.884259065937687\n",
      "\n",
      "Episode: 1199, actor_loss: -1.588, critic_loss: 3.370, mean_reward: -1496.609, best_return: -53.044\n",
      "last 50 episode mean reward:  -1518.887651989245\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "# training hindsight ppo model hyperparameter\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20 # 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "max_frames       = 24000# 50000\n",
    "frame_idx        = 0\n",
    "episode_count    = 0\n",
    "best_return      = -9999\n",
    "desired_goal     = np.asarray([0,0,0])\n",
    "\n",
    "# options hyperparameter\n",
    "use_modelbased   = True\n",
    "early_stop       = False\n",
    "multi_goal       = True\n",
    "\n",
    "# statistical variable\n",
    "test_rewards          = []\n",
    "mean_actor_loss_list  = []\n",
    "mean_critic_loss_list = []\n",
    "\n",
    "# model based parameter\n",
    "rand_num_traj     = 1\n",
    "mb_train_iter     = 60\n",
    "init_goal_dist    = 7.0\n",
    "end_goal_dist     = 0.01\n",
    "decay_rate        = 1000 / math.log(700)\n",
    "goal_network_lr   = 3e-4\n",
    "all_mb_loss       = []\n",
    "all_goal_mse_eval = []\n",
    "mb_distance_list  = []\n",
    "sf_distance_list  = []\n",
    "\n",
    "# network model\n",
    "model            = ActorCritic(2*num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer        = optim.Adam(model.parameters(), lr = lr)\n",
    "goal_model       = ModelBasedGoalNetwork(num_inputs+num_outputs, num_inputs, hidden_size).to(device)\n",
    "goal_optimizer   = optim.Adam(goal_model.parameters(), lr = goal_network_lr)\n",
    "\n",
    "random_dataset   = generate_random_trajectories(rand_num_traj)\n",
    "temp_rl_dataset  = []\n",
    "rl_dataset       = []\n",
    "state = envs.reset()\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    \n",
    "    # sample state from previous episode\n",
    "    if multi_goal:\n",
    "        if frame_idx == 0: \n",
    "            goal = initial_subgoal\n",
    "        else: \n",
    "            if use_modelbased:\n",
    "                min_dist = init_goal_dist * math.exp((-1)*episode_count/decay_rate)\n",
    "                goal = model_based_goal_generator(random_dataset, rl_dataset, rand_num_traj, mb_train_iter, \n",
    "                                                    min_dist,goal_model, goal_optimizer)\n",
    "                if len(goal) > 1:\n",
    "                    goal = goal.cpu().detach().numpy()\n",
    "                    temp = goal\n",
    "                else: \n",
    "                    goal = temp\n",
    "                    \n",
    "                if episode_count != 0:\n",
    "                    evaluate_goals(state, num_envs, goal, desired_goal)\n",
    "            else: \n",
    "                if len(state) > 1:\n",
    "                    goal = state[random.randint(0, num_envs - 1)]\n",
    "                else:\n",
    "                    goal = state[0]\n",
    "    else:\n",
    "        goal = desired_goal\n",
    "        \n",
    "    log_probs         = []\n",
    "    log_probs_desired = []\n",
    "    values            = []\n",
    "    states            = []\n",
    "    states_goals      = []\n",
    "    next_states       = []\n",
    "    actions           = []\n",
    "    actions_desired   = []\n",
    "    rewards           = []\n",
    "    dones             = []\n",
    "    masks             = []\n",
    "    entropy = 0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        state_goals = []\n",
    "        state_desired_goals = []\n",
    "        next_state_goals = []\n",
    "        \n",
    "        # append state with subgoal and desired goal\n",
    "        for s in state: \n",
    "            state_goal = np.concatenate((s,goal),0)\n",
    "            state_goals.append((state_goal))\n",
    "            state_desired_goal = np.concatenate((s, desired_goal), 0)\n",
    "            state_desired_goals.append((state_desired_goal))\n",
    "            \n",
    "        state_goals = np.array(state_goals)\n",
    "        state_goals = torch.FloatTensor(state_goals).to(device)\n",
    "        state_desired_goals = np.array(state_desired_goals)\n",
    "        state_desired_goals = torch.FloatTensor(state_desired_goals).to(device)\n",
    "        \n",
    "        # for subgoal\n",
    "        dist, value = model(state_goals)\n",
    "        action = dist.sample() \n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        # for desired goal\n",
    "        dist_desired, value_desired = model(state_desired_goals)\n",
    "        action_desired = dist_desired.sample()\n",
    "        \n",
    "        # append next state with sub goal\n",
    "        for n_s in next_state: \n",
    "            next_state_goal = np.concatenate((n_s, goal), 0)\n",
    "            next_state_desired_goal = np.concatenate((n_s, desired_goal), 0)\n",
    "            next_state_goals.append((next_state_goal)) \n",
    "        next_state_goals = np.array(next_state_goals)\n",
    "        \n",
    "        # for subgoal\n",
    "        log_prob = dist.log_prob(action)\n",
    "        # for desired goal\n",
    "        log_prob_desired = dist_desired.log_prob(action_desired)\n",
    "        \n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        # normalized reward\n",
    "        reward = (reward - np.mean(reward))/(np.std(reward) + 1e-5)\n",
    "        \n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        states_goals.append(state_goals)\n",
    "        actions.append(action)\n",
    "        actions_desired.append(action_desired)\n",
    "        log_probs.append(log_prob)\n",
    "        log_probs_desired.append(log_prob_desired)\n",
    "        dones.append(done)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        values.append(value)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % num_steps == 0:\n",
    "            test_reward = np.mean([test_env(model, desired_goal) for _ in range(5)])\n",
    "            test_rewards.append(test_reward)\n",
    "            if test_reward >= best_return:\n",
    "                best_return = test_reward\n",
    "            # plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "                \n",
    "    temp_rl_dataset.append([states, next_states, rewards, dones, actions])\n",
    "    \n",
    "    # forget older data (reduce bias)\n",
    "    if episode_count % 20 == 0:\n",
    "        rl_dataset = []\n",
    "    rl_dataset = flatten_rl_dataset(temp_rl_dataset, rl_dataset)\n",
    "    \n",
    "    next_state_goals = torch.FloatTensor(next_state_goals).to(device)\n",
    "    _, next_value = model(next_state_goals)\n",
    "\n",
    "    old_logprobs     = log_probs \n",
    "    current_logprobs = log_probs_desired\n",
    "    \n",
    "    # print ('old_logprobs: ', log_probs)\n",
    "    # print ('current_logprobs: ', current_logprobs)\n",
    "#     returns        = hindsight_gae(rewards, old_logprobs, current_logprobs, masks, values)\n",
    "    returns        = compute_gae (next_value, rewards, masks, values)\n",
    "                          \n",
    "    returns        = torch.cat(returns).detach()\n",
    "    log_probs      = torch.cat(log_probs).detach()\n",
    "    values         = torch.cat(values).detach()\n",
    "    states_goals   = torch.cat(states_goals)\n",
    "    actions        = torch.cat(actions)\n",
    "    advantage      = returns - values\n",
    "\n",
    "    ppo_update(ppo_epochs, mini_batch_size, states_goals, actions, log_probs, returns, advantage, episode_count, \n",
    "               test_reward, best_return)\n",
    "    \n",
    "    if frame_idx % (num_steps * 50) == 0:\n",
    "        lower_bound = int((frame_idx - (num_steps * 50)) / num_steps)\n",
    "        upper_bound = int(frame_idx / num_steps)\n",
    "        last_fifty_episode_mean_reward = np.mean(test_rewards[lower_bound:upper_bound])\n",
    "        print ('last 50 episode mean reward: ', last_fifty_episode_mean_reward)\n",
    "        print ('\\n')\n",
    "    \n",
    "    episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Testing Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Test Reward Plot/mb_test_rewards2', 'wb') as fp1:\n",
    "    pickle.dump(test_rewards, fp1)\n",
    "with open('./Loss Plot/mb_mean_actor_loss2', 'wb') as fp2:\n",
    "    pickle.dump(mean_actor_loss_list, fp2)\n",
    "with open('./Loss Plot/mb_mean_critic_loss2', 'wb') as fp3:\n",
    "    pickle.dump(mean_critic_loss_list, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Goal Plot/mb_goal4', 'wb') as fp1:\n",
    "    pickle.dump(mb_distance_list, fp1)\n",
    "\n",
    "with open('./Goal Plot/sf_goal4', 'wb') as fp1:\n",
    "    pickle.dump(sf_distance_list, fp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save and Load Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'model_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './Model/'+model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_model = torch.load('./Model/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_test_rewards = []\n",
    "# for i in range(5): \n",
    "# #     env = gym.wrappers.Monitor(env, 'test_video'+str(i), video_callable=lambda episode_id: True)\n",
    "#     expert_test_reward = test_env(expert_model, [0, 0, 0], False)\n",
    "#     expert_test_rewards.append(expert_test_reward)\n",
    "#     print ('test {0}, total_reward from '+model_name+' load model: {1}'.format(i+1, expert_test_reward))\n",
    "\n",
    "# # print ('mean expert test reward: ', np.mean(expert_test_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weixiang",
   "language": "python",
   "name": "weixiang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
