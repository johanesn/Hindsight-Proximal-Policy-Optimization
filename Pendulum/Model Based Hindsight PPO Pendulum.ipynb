{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Beta\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed_number = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_number)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed_number)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed_number)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value\n",
    "\n",
    "class ModelBasedGoalNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(ModelBasedGoalNetwork, self).__init__()\n",
    "        \n",
    "        self.goalnetwork = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        subgoal = self.goalnetwork(x)\n",
    "        return subgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, goal, vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_goal = np.concatenate((state,goal),0)\n",
    "        state_goal = torch.FloatTensor(state_goal).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state_goal)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hindsight GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, lamda=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lamda * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importance Hindsight GAE </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_gae(rewards, current_logprobs, desired_logprobs, masks, values, gamma = 0.995, lamda = 0):\n",
    "    lambda_ret = 1\n",
    "    hindsight_gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in range(len(rewards)):\n",
    "        temp = 0\n",
    "        is_weight_ratio = 1\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            ratio = (current_logprobs[step_] - desired_logprobs[step_]).exp() \n",
    "            clipped_ratio = lambda_ret * torch.clamp(ratio, max = 1)\n",
    "            is_weight_ratio = is_weight_ratio * clipped_ratio\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            temp = temp + ((gamma ** (step_+1)) * rewards[step_] - (gamma ** (step_)) * rewards[step_])  \n",
    "        temp = temp - (gamma ** (step + 1)) * rewards[step]\n",
    "        \n",
    "        delta = rewards[step] + is_weight_ratio * temp\n",
    "        hindsight_gae = delta + gamma * lamda * masks[step] * hindsight_gae\n",
    "        returns.insert(0, hindsight_gae + values[step])\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Compute Return </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma = 0.995):\n",
    "    returns = 0\n",
    "    returns_list = []\n",
    "    for step in range(len(rewards)):\n",
    "        returns = returns + (gamma ** i) * rewards[step] \n",
    "        returns_list.insert(0,returns)\n",
    "    return returns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :],actions[rand_ids,:],log_probs[rand_ids,:],returns[rand_ids,:],advantage[rand_ids,:]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, \n",
    "               advantages, episode_count, test_reward, best_return, clip_param=0.2):\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    clip = 5\n",
    "    for ppo_epoch in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, \n",
    "                                                                         actions, log_probs, returns, advantages):\n",
    "            model.zero_grad()\n",
    "            dist, value = model(state)\n",
    "\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            # MSE Loss\n",
    "            critic_loss = (return_ - value).pow(2).mean() \n",
    "            \n",
    "            # Huber Loss\n",
    "            # critic_loss = nn.functional.smooth_l1_loss(value, return_)\n",
    "            \n",
    "            actor_loss_list.append(actor_loss.data.cpu().numpy().item(0))\n",
    "            critic_loss_list.append(critic_loss.data.cpu().numpy().item(0))\n",
    "            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.0001 * entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradient to prevent gradient exploding\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "    \n",
    "    mean_actor_loss = np.mean(actor_loss_list)\n",
    "    mean_critic_loss = np.mean(critic_loss_list)\n",
    "    \n",
    "    mean_actor_loss_list.append(mean_actor_loss)\n",
    "    mean_critic_loss_list.append(mean_critic_loss)\n",
    "    \n",
    "    assert ~np.isnan(mean_critic_loss), \"Assert error: critic loss has nan value.\" \n",
    "    assert ~np.isinf(mean_critic_loss), \"Assert error: critic loss has inf value.\"\n",
    "\n",
    "    print ('\\nEpisode: {0}, actor_loss: {1:.3f}, critic_loss: {2:.3f}, mean_reward: {3:.3f}, best_return: {4:.3f}'\n",
    "           .format(episode_count, mean_actor_loss, mean_critic_loss, test_reward, best_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env(i):\n",
    "    def _thunk():\n",
    "        \n",
    "        env = gym.make(env_name)\n",
    "        env.seed(i+seed_number)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env(i) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Goal Distribution Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 50\n",
    "reward = 0\n",
    "done = False\n",
    "initial_subgoals = []\n",
    "        \n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "#     print (state)\n",
    "    done_count = 0\n",
    "    while True:\n",
    "        action = agent.act(state, reward, done)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    initial_subgoals.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial subgoal sampled is:  [ 0.11320659 -0.99357147 -0.14532486]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed_number)\n",
    "initial_subgoal = initial_subgoals[random.randint(0, len(initial_subgoals)-1)]\n",
    "print ('Initial subgoal sampled is: ', initial_subgoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Based Goal Generator </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_trajectories(num_traj):\n",
    "    print ('generating random trajectories...')\n",
    "    dataset_random = []\n",
    "\n",
    "    game_rewards = []\n",
    "    for n in range(num_traj):\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            sampled_action = env.action_space.sample()\n",
    "            new_obs, reward, done, _ = env.step(sampled_action)\n",
    "\n",
    "            dataset_random.append([obs, new_obs, reward, done, sampled_action])\n",
    "\n",
    "            obs = new_obs\n",
    "            game_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # print some stats\n",
    "    print('mean rand_dataset reward:',np.round(np.sum(game_rewards)/num_traj,2), \n",
    "          'max rand_dataset reward:', np.round(np.max(game_rewards),2), np.round(len(game_rewards)/num_traj))\n",
    "\n",
    "    return dataset_random\n",
    "\n",
    "def flatten_rl_dataset(temp, rl_dataset):\n",
    "    for i in range(num_steps):\n",
    "        for j in range(num_envs):\n",
    "            rl_dataset.append([temp[0][0][i][j],temp[0][1][i][j],temp[0][2][i][j].cpu().numpy(),\n",
    "                               temp[0][3][i][j].astype('bool'),temp[0][4][i][j].cpu().numpy()])\n",
    "    return rl_dataset\n",
    "\n",
    "def filtered_prep(dataset, min_dist):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        curr_dist = np.linalg.norm(dataset[i][0]-desired_goal)\n",
    "        if (curr_dist > (min_dist - 0.2)) and (curr_dist < (min_dist + 0.2)):\n",
    "            new_dataset.append(dataset[i])\n",
    "    return np.array(new_dataset)\n",
    "                    \n",
    "def MSELoss(y_truth, y_pred):\n",
    "    y_truth = torch.FloatTensor(np.array(y_truth)).to(device)\n",
    "    return F.mse_loss(y_pred.view(-1).float(), y_truth.view(-1))\n",
    "\n",
    "\n",
    "def goal_network_update(dataset, train_iter, goal_model, goal_optimizer):\n",
    "    # split dataset to 80% training and 20% validation\n",
    "    len_data = len(dataset)\n",
    "    \n",
    "    d_train  = dataset[:int(len_data * 0.8)]\n",
    "    d_valid  = dataset[int(len_data * 0.8):-1]\n",
    "    \n",
    "    state_action_input = np.concatenate((dataset[-1][0],dataset[-1][4]),0)\n",
    "    sff      = np.arange(len(d_train))\n",
    "    np.random.shuffle(sff)\n",
    "    d_train  = d_train[sff]\n",
    "    \n",
    "    # training dataset\n",
    "    x_train  = np.array([np.concatenate([s,a]) for s,_,_,_,a in d_train]) \n",
    "    y_train  = np.array([ns for _,ns,_,_,_ in d_train])\n",
    "    \n",
    "    # validation dataset\n",
    "    x_valid  = np.array([np.concatenate([s,a]) for s,_,_,_,a in d_valid])\n",
    "    y_valid  = np.array([ns for _,ns,_,_,_ in d_valid])\n",
    "\n",
    "    losses_goal = []\n",
    "    mse_valid = []\n",
    "    # go through max_model_iter supervised iterations\n",
    "    for i in range(train_iter):\n",
    "        x_train += np.random.normal(loc = 0, scale=0.001, size = x_train.shape)\n",
    "\n",
    "        goal_optimizer.zero_grad()\n",
    "        pred_goal = goal_model((torch.tensor(x_train)).type(torch.FloatTensor).to(device))\n",
    "        goal_loss = MSELoss(y_train, pred_goal)\n",
    "        # print ('goal_loss: ', goal_loss.cpu().detach().numpy())\n",
    "        losses_goal.append(goal_loss.cpu().detach().numpy())\n",
    "        goal_loss.backward()\n",
    "        goal_optimizer.step()\n",
    "\n",
    "        # iteratively evaluate\n",
    "        if i % 5 == 0:\n",
    "            goal_model.eval()\n",
    "            pred_goal = goal_model((torch.tensor(x_valid)).type(torch.FloatTensor).to(device))\n",
    "            goal_model.train(True)\n",
    "            valid_goal_loss = MSELoss(y_valid, pred_goal)\n",
    "            mse_valid.append(valid_goal_loss)\n",
    "            # print ('evaluation iteration: ',i, ' ,valid_goal_loss: ', valid_goal_loss.cpu().detach().numpy())\n",
    "    # final evaluation\n",
    "    goal_model.eval()\n",
    "    pred_goal = goal_model((torch.tensor(x_valid)).type(torch.FloatTensor).to(device))\n",
    "    goal_model.train(True)\n",
    "    valid_goal_loss = MSELoss(y_valid, pred_goal)\n",
    "    mse_valid.append(valid_goal_loss)\n",
    "    # print ('end of training validation goal loss: ', valid_goal_loss.cpu().detach().numpy())\n",
    "    return losses_goal, mse_valid, state_action_input\n",
    "\n",
    "def model_based_goal_generator(rand_dataset, rl_dataset, rand_num_traj, mb_train_iter, min_dist, \n",
    "                               goal_model, goal_optimizer):\n",
    "    print ('generate goal using supervised model based...')\n",
    "\n",
    "    \n",
    "    if len(rl_dataset) > 0:\n",
    "        concat_dataset    = np.concatenate([rand_dataset, rl_dataset], axis=0)\n",
    "    else:\n",
    "        concat_dataset    = np.array(rl_dataset)\n",
    "    print ('len after concat dataset: ', len(concat_dataset))\n",
    "\n",
    "    filtered_dataset  = filtered_prep (concat_dataset, min_dist)\n",
    "    print ('len filtered concat dataset: ', len(filtered_dataset))\n",
    "    # if filtered dataset empty\n",
    "    if len(filtered_dataset) > 0:\n",
    "        mb_loss, mse_eval, state_action_input = goal_network_update(filtered_dataset, mb_train_iter, \n",
    "                                                                goal_model, goal_optimizer)\n",
    "        all_mb_loss.append(mb_loss)\n",
    "        all_goal_mse_eval.append(mse_eval)\n",
    "\n",
    "        model_based_goal = goal_model(torch.FloatTensor(state_action_input).to(device))\n",
    "\n",
    "        return model_based_goal\n",
    "    else:\n",
    "        return [0]\n",
    "def evaluate_goals(state, num_envs, goal, desired_goal):\n",
    "    sf_subgoal = state[random.randint(0, num_envs - 1)]\n",
    "    mb_distance = np.linalg.norm(goal - desired_goal)\n",
    "    sf_distance = np.linalg.norm(sf_subgoal - desired_goal) \n",
    "    print ('mb subgoal: ', goal, ' distance: ', mb_distance)\n",
    "    print ('sample final subgoal: ', sf_subgoal, 'distance: ', sf_distance)\n",
    "    mb_distance_list.append(mb_distance)\n",
    "    sf_distance_list.append(sf_distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "# training hindsight ppo model hyperparameter\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20 # 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "max_frames       = 24000# 50000\n",
    "frame_idx        = 0\n",
    "episode_count    = 0\n",
    "best_return      = -9999\n",
    "desired_goal     = np.asarray([0,0,0])\n",
    "\n",
    "# options hyperparameter\n",
    "use_modelbased   = True\n",
    "early_stop       = False\n",
    "multi_goal       = True\n",
    "\n",
    "# statistical variable\n",
    "test_rewards          = []\n",
    "mean_actor_loss_list  = []\n",
    "mean_critic_loss_list = []\n",
    "\n",
    "# model based parameter\n",
    "rand_num_traj     = 1\n",
    "mb_train_iter     = 60\n",
    "init_goal_dist    = 7.0\n",
    "end_goal_dist     = 0.01\n",
    "decay_rate        = 1000 / math.log(700)\n",
    "goal_network_lr   = 3e-4\n",
    "all_mb_loss       = []\n",
    "all_goal_mse_eval = []\n",
    "mb_distance_list  = []\n",
    "sf_distance_list  = []\n",
    "\n",
    "# network model\n",
    "model            = ActorCritic(2*num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer        = optim.Adam(model.parameters(), lr = lr)\n",
    "goal_model       = ModelBasedGoalNetwork(num_inputs+num_outputs, num_inputs, hidden_size).to(device)\n",
    "goal_optimizer   = optim.Adam(goal_model.parameters(), lr = goal_network_lr)\n",
    "\n",
    "random_dataset   = generate_random_trajectories(rand_num_traj)\n",
    "temp_rl_dataset  = []\n",
    "rl_dataset       = []\n",
    "state = envs.reset()\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    \n",
    "    # sample state from previous episode\n",
    "    if multi_goal:\n",
    "        if frame_idx == 0: \n",
    "            goal = initial_subgoal\n",
    "        else: \n",
    "            if use_modelbased:\n",
    "                min_dist = init_goal_dist * math.exp((-1)*episode_count/decay_rate)\n",
    "                goal = model_based_goal_generator(random_dataset, rl_dataset, rand_num_traj, mb_train_iter, \n",
    "                                                    min_dist,goal_model, goal_optimizer)\n",
    "                if len(goal) > 1:\n",
    "                    goal = goal.cpu().detach().numpy()\n",
    "                    temp = goal\n",
    "                else: \n",
    "                    goal = temp\n",
    "                    \n",
    "                if episode_count != 0:\n",
    "                    evaluate_goals(state, num_envs, goal, desired_goal)\n",
    "            else: \n",
    "                if len(state) > 1:\n",
    "                    goal = state[random.randint(0, num_envs - 1)]\n",
    "                else:\n",
    "                    goal = state[0]\n",
    "    else:\n",
    "        goal = desired_goal\n",
    "        \n",
    "    log_probs         = []\n",
    "    log_probs_desired = []\n",
    "    values            = []\n",
    "    states            = []\n",
    "    states_goals      = []\n",
    "    next_states       = []\n",
    "    actions           = []\n",
    "    actions_desired   = []\n",
    "    rewards           = []\n",
    "    dones             = []\n",
    "    masks             = []\n",
    "    entropy = 0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        state_goals = []\n",
    "        state_desired_goals = []\n",
    "        next_state_goals = []\n",
    "        \n",
    "        # append state with subgoal and desired goal\n",
    "        for s in state: \n",
    "            state_goal = np.concatenate((s,goal),0)\n",
    "            state_goals.append((state_goal))\n",
    "            state_desired_goal = np.concatenate((s, desired_goal), 0)\n",
    "            state_desired_goals.append((state_desired_goal))\n",
    "            \n",
    "        state_goals = np.array(state_goals)\n",
    "        state_goals = torch.FloatTensor(state_goals).to(device)\n",
    "        state_desired_goals = np.array(state_desired_goals)\n",
    "        state_desired_goals = torch.FloatTensor(state_desired_goals).to(device)\n",
    "        \n",
    "        # for subgoal\n",
    "        dist, value = model(state_goals)\n",
    "        action = dist.sample() \n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        # for desired goal\n",
    "        dist_desired, value_desired = model(state_desired_goals)\n",
    "        action_desired = dist_desired.sample()\n",
    "        \n",
    "        # append next state with sub goal\n",
    "        for n_s in next_state: \n",
    "            next_state_goal = np.concatenate((n_s, goal), 0)\n",
    "            next_state_desired_goal = np.concatenate((n_s, desired_goal), 0)\n",
    "            next_state_goals.append((next_state_goal)) \n",
    "        next_state_goals = np.array(next_state_goals)\n",
    "        \n",
    "        # for subgoal\n",
    "        log_prob = dist.log_prob(action)\n",
    "        # for desired goal\n",
    "        log_prob_desired = dist_desired.log_prob(action_desired)\n",
    "        \n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        # normalized reward\n",
    "        reward = (reward - np.mean(reward))/(np.std(reward) + 1e-5)\n",
    "        \n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        states_goals.append(state_goals)\n",
    "        actions.append(action)\n",
    "        actions_desired.append(action_desired)\n",
    "        log_probs.append(log_prob)\n",
    "        log_probs_desired.append(log_prob_desired)\n",
    "        dones.append(done)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        values.append(value)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % num_steps == 0:\n",
    "            test_reward = np.mean([test_env(model, desired_goal) for _ in range(5)])\n",
    "            test_rewards.append(test_reward)\n",
    "            if test_reward >= best_return:\n",
    "                best_return = test_reward\n",
    "            # plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "                \n",
    "    temp_rl_dataset.append([states, next_states, rewards, dones, actions])\n",
    "    \n",
    "    # forget older data (reduce bias)\n",
    "    if episode_count % 20 == 0:\n",
    "        rl_dataset = []\n",
    "    rl_dataset = flatten_rl_dataset(temp_rl_dataset, rl_dataset)\n",
    "    \n",
    "    next_state_goals = torch.FloatTensor(next_state_goals).to(device)\n",
    "    _, next_value = model(next_state_goals)\n",
    "\n",
    "    old_logprobs     = log_probs \n",
    "    current_logprobs = log_probs_desired\n",
    "    \n",
    "    # print ('old_logprobs: ', log_probs)\n",
    "    # print ('current_logprobs: ', current_logprobs)\n",
    "    returns        = hindsight_gae(rewards, old_logprobs, current_logprobs, masks, values)\n",
    "#     returns        = compute_gae (next_value, rewards, masks, values)\n",
    "                          \n",
    "    returns        = torch.cat(returns).detach()\n",
    "    log_probs      = torch.cat(log_probs).detach()\n",
    "    values         = torch.cat(values).detach()\n",
    "    states_goals   = torch.cat(states_goals)\n",
    "    actions        = torch.cat(actions)\n",
    "    advantage      = returns - values\n",
    "\n",
    "    ppo_update(ppo_epochs, mini_batch_size, states_goals, actions, log_probs, returns, advantage, episode_count, \n",
    "               test_reward, best_return)\n",
    "    \n",
    "    if frame_idx % (num_steps * 50) == 0:\n",
    "        lower_bound = int((frame_idx - (num_steps * 50)) / num_steps)\n",
    "        upper_bound = int(frame_idx / num_steps)\n",
    "        last_fifty_episode_mean_reward = np.mean(test_rewards[lower_bound:upper_bound])\n",
    "        print ('last 50 episode mean reward: ', last_fifty_episode_mean_reward)\n",
    "        print ('\\n')\n",
    "    \n",
    "    episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Testing Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Test Reward Plot/mb_test_rewards2', 'wb') as fp1:\n",
    "    pickle.dump(test_rewards, fp1)\n",
    "with open('./Loss Plot/mb_mean_actor_loss2', 'wb') as fp2:\n",
    "    pickle.dump(mean_actor_loss_list, fp2)\n",
    "with open('./Loss Plot/mb_mean_critic_loss2', 'wb') as fp3:\n",
    "    pickle.dump(mean_critic_loss_list, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Goal Plot/mb_goal4', 'wb') as fp1:\n",
    "    pickle.dump(mb_distance_list, fp1)\n",
    "\n",
    "with open('./Goal Plot/sf_goal4', 'wb') as fp1:\n",
    "    pickle.dump(sf_distance_list, fp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save and Load Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'model_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './Model/'+model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_model = torch.load('./Model/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_test_rewards = []\n",
    "# for i in range(5): \n",
    "# #     env = gym.wrappers.Monitor(env, 'test_video'+str(i), video_callable=lambda episode_id: True)\n",
    "#     expert_test_reward = test_env(expert_model, [0, 0, 0], False)\n",
    "#     expert_test_rewards.append(expert_test_reward)\n",
    "#     print ('test {0}, total_reward from '+model_name+' load model: {1}'.format(i+1, expert_test_reward))\n",
    "\n",
    "# # print ('mean expert test reward: ', np.mean(expert_test_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weixiang",
   "language": "python",
   "name": "weixiang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
