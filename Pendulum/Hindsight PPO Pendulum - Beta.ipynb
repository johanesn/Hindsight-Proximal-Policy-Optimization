{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal,Beta\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_number)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed_number)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed_number)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "#         nn.init.xavier_uniform_(m.weight)\n",
    "#         nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2 * num_outputs),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        alpha_beta = self.actor(x)\n",
    "        \n",
    "        alpha = alpha_beta[:,0]+1\n",
    "        beta = alpha_beta[:,1]+1\n",
    "\n",
    "        alpha = alpha.reshape(len(alpha),1)\n",
    "        beta = beta.reshape(len(beta),1)\n",
    "        dist = Beta (alpha, beta)\n",
    "        return dist, value, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, goal, vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_goal = np.concatenate((state,goal),0)\n",
    "        state_goal = torch.FloatTensor(state_goal).unsqueeze(0).to(device)\n",
    "        dist, _, a_, b_ = model(state_goal)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic Hindsight GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, lamda=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lamda * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hindsight GAE (Importance Sampling) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_gae(rewards, current_logprobs, desired_logprobs, masks, values, gamma = 0.995, lamda = 0):\n",
    "    lambda_ret = 1\n",
    "    hindsight_gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in range(len(rewards)):\n",
    "        temp = 0\n",
    "        is_weight_ratio = 1\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            ratio = (current_logprobs[step_] - desired_logprobs[step_]).exp() \n",
    "            clipped_ratio = lambda_ret * torch.clamp(ratio, max = 1)\n",
    "            is_weight_ratio = is_weight_ratio * clipped_ratio\n",
    "        for step_ in range(step, len(rewards)):\n",
    "            temp = temp + ((gamma ** (step_+1)) * rewards[step_] - (gamma ** (step_)) * rewards[step_])  \n",
    "        temp = temp - (gamma ** (step + 1)) * rewards[step]\n",
    "        \n",
    "        delta = rewards[step] + is_weight_ratio * temp\n",
    "        hindsight_gae = delta + gamma * lamda * masks[step] * hindsight_gae\n",
    "        returns.insert(0, hindsight_gae + values[step])\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Compute Return </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma = 0.995):\n",
    "    returns = 0\n",
    "    returns_list = []\n",
    "    for step in range(len(rewards)):\n",
    "        returns = returns + (gamma ** i) * rewards[step] \n",
    "        returns_list.insert(0,returns)\n",
    "    return returns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">PPO Paper</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, episode_count, test_reward, best_return, clip_param=0.2):\n",
    "    actor_loss_list = []\n",
    "    critic_loss_list = []\n",
    "    clipped = False\n",
    "    for ppo_epoch in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            \n",
    "            dist, value, a_, b_ = model(state)\n",
    "\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob((action+2)/4)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # MSE Loss\n",
    "            critic_loss = (return_ - value).pow(2).mean() \n",
    "            \n",
    "            # Huber Loss\n",
    "#             critic_loss = nn.functional.smooth_l1_loss(value, return_)\n",
    "            \n",
    "            actor_loss_list.append(actor_loss.data.cpu().numpy().item(0))\n",
    "            critic_loss_list.append(critic_loss.data.cpu().numpy().item(0))\n",
    "            \n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    mean_actor_loss = np.mean(actor_loss_list)\n",
    "    mean_critic_loss = np.mean(critic_loss_list)\n",
    "    \n",
    "    mean_actor_loss_list.append(mean_actor_loss)\n",
    "    mean_critic_loss_list.append(mean_critic_loss)\n",
    "    \n",
    "    assert ~np.isnan(mean_critic_loss), \"Assert error: critic loss has nan value.\" \n",
    "    assert ~np.isinf(mean_critic_loss), \"Assert error: critic loss has inf value.\"\n",
    "    \n",
    "    print ('episode: {0}, actor_loss: {1:.3f}, critic_loss: {2:.3f}, mean_reward: {3:.3f}, best_return: {4:.3f}'\n",
    "           .format(episode_count, mean_actor_loss, mean_critic_loss, test_reward, best_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2018]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env(i):\n",
    "    def _thunk():\n",
    "        \n",
    "        env = gym.make(env_name)\n",
    "        env.seed(i+seed_number)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env(i) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Goal Distribution Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 50\n",
    "reward = 0\n",
    "done = False\n",
    "initial_subgoals = []\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "#     print (state)\n",
    "    done_count = 0\n",
    "    while True:\n",
    "        action = agent.act(state, reward, done)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    initial_subgoals.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial subgoal sampled is:  [-0.77119042 -0.63660453  2.76435863]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed_number)\n",
    "initial_subgoal = initial_subgoals[random.randint(0, len(initial_subgoals)-1)]\n",
    "print ('Initial subgoal sampled is: ', initial_subgoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1.0 alpha+beta:  3.2134826\n",
      "episode: 1, actor_loss: -0.073, critic_loss: 12.613, mean_reward: -1354.996, best_return: -1354.996\n",
      "episode:  2.0 alpha+beta:  3.0538774\n",
      "episode: 2, actor_loss: 0.170, critic_loss: 22.956, mean_reward: -1269.329, best_return: -1269.329\n",
      "episode:  3.0 alpha+beta:  2.9401727\n",
      "episode: 3, actor_loss: 0.076, critic_loss: 18.169, mean_reward: -1655.359, best_return: -1269.329\n",
      "episode:  4.0 alpha+beta:  3.2444334\n",
      "episode: 4, actor_loss: -0.509, critic_loss: 15.816, mean_reward: -1319.891, best_return: -1269.329\n",
      "episode:  5.0 alpha+beta:  3.1576867\n",
      "episode: 5, actor_loss: 0.094, critic_loss: 9.920, mean_reward: -1351.808, best_return: -1269.329\n",
      "episode:  6.0 alpha+beta:  3.7418642\n",
      "episode: 6, actor_loss: -0.407, critic_loss: 11.636, mean_reward: -1096.549, best_return: -1096.549\n",
      "episode:  7.0 alpha+beta:  3.064237\n",
      "episode: 7, actor_loss: -0.147, critic_loss: 7.088, mean_reward: -1361.872, best_return: -1096.549\n",
      "episode:  8.0 alpha+beta:  3.5806646\n",
      "episode: 8, actor_loss: 0.445, critic_loss: 5.448, mean_reward: -1302.976, best_return: -1096.549\n",
      "episode:  9.0 alpha+beta:  3.713439\n",
      "episode: 9, actor_loss: 1.175, critic_loss: 6.481, mean_reward: -1377.555, best_return: -1096.549\n",
      "episode:  10.0 alpha+beta:  3.4817715\n",
      "episode: 10, actor_loss: 1.882, critic_loss: 9.822, mean_reward: -1452.319, best_return: -1096.549\n",
      "episode:  11.0 alpha+beta:  3.6057265\n",
      "episode: 11, actor_loss: -1.099, critic_loss: 8.135, mean_reward: -1331.887, best_return: -1096.549\n",
      "episode:  12.0 alpha+beta:  3.3146996\n",
      "episode: 12, actor_loss: -0.346, critic_loss: 7.673, mean_reward: -1104.552, best_return: -1096.549\n",
      "episode:  13.0 alpha+beta:  3.5211897\n",
      "episode: 13, actor_loss: -0.448, critic_loss: 9.595, mean_reward: -1413.909, best_return: -1096.549\n",
      "episode:  14.0 alpha+beta:  3.9280221\n",
      "episode: 14, actor_loss: -1.275, critic_loss: 8.961, mean_reward: -1354.032, best_return: -1096.549\n",
      "episode:  15.0 alpha+beta:  4.3414736\n",
      "episode: 15, actor_loss: -0.450, critic_loss: 12.357, mean_reward: -1467.060, best_return: -1096.549\n",
      "episode:  16.0 alpha+beta:  3.661045\n",
      "episode: 16, actor_loss: -0.337, critic_loss: 13.695, mean_reward: -1180.014, best_return: -1096.549\n",
      "episode:  17.0 alpha+beta:  4.2603474\n",
      "episode: 17, actor_loss: -0.299, critic_loss: 30.469, mean_reward: -1152.773, best_return: -1096.549\n",
      "episode:  18.0 alpha+beta:  4.8646584\n",
      "episode: 18, actor_loss: 0.698, critic_loss: 15.511, mean_reward: -1224.299, best_return: -1096.549\n",
      "episode:  19.0 alpha+beta:  4.9042144\n",
      "episode: 19, actor_loss: 0.039, critic_loss: 12.683, mean_reward: -1301.372, best_return: -1096.549\n",
      "episode:  20.0 alpha+beta:  5.5569253\n",
      "episode: 20, actor_loss: 13.023, critic_loss: 32.029, mean_reward: -1226.257, best_return: -1096.549\n",
      "episode:  21.0 alpha+beta:  3.7700002\n",
      "episode: 21, actor_loss: -2.616, critic_loss: 18.151, mean_reward: -1208.926, best_return: -1096.549\n",
      "episode:  22.0 alpha+beta:  4.3592567\n",
      "episode: 22, actor_loss: -1.360, critic_loss: 26.130, mean_reward: -1209.779, best_return: -1096.549\n",
      "episode:  23.0 alpha+beta:  4.551154\n",
      "episode: 23, actor_loss: 0.863, critic_loss: 21.422, mean_reward: -1171.831, best_return: -1096.549\n",
      "episode:  24.0 alpha+beta:  5.7015104\n",
      "episode: 24, actor_loss: 0.672, critic_loss: 8.453, mean_reward: -1243.474, best_return: -1096.549\n",
      "episode:  25.0 alpha+beta:  4.980608\n",
      "episode: 25, actor_loss: 1.188, critic_loss: 6.062, mean_reward: -1253.010, best_return: -1096.549\n",
      "episode:  26.0 alpha+beta:  4.938491\n",
      "episode: 26, actor_loss: 0.551, critic_loss: 2.706, mean_reward: -1209.560, best_return: -1096.549\n",
      "episode:  27.0 alpha+beta:  4.429069\n",
      "episode: 27, actor_loss: 1.002, critic_loss: 1.983, mean_reward: -1203.360, best_return: -1096.549\n",
      "episode:  28.0 alpha+beta:  3.6775563\n",
      "episode: 28, actor_loss: 0.926, critic_loss: 2.630, mean_reward: -1251.879, best_return: -1096.549\n",
      "episode:  29.0 alpha+beta:  2.8393784\n",
      "episode: 29, actor_loss: 0.692, critic_loss: 1.825, mean_reward: -1213.602, best_return: -1096.549\n",
      "episode:  30.0 alpha+beta:  4.730952\n",
      "episode: 30, actor_loss: 7.663, critic_loss: 19.205, mean_reward: -1282.876, best_return: -1096.549\n",
      "episode:  31.0 alpha+beta:  3.8699415\n",
      "episode: 31, actor_loss: -1.212, critic_loss: 13.811, mean_reward: -1292.155, best_return: -1096.549\n",
      "episode:  32.0 alpha+beta:  4.348501\n",
      "episode: 32, actor_loss: -0.795, critic_loss: 15.336, mean_reward: -1257.736, best_return: -1096.549\n",
      "episode:  33.0 alpha+beta:  4.4346\n",
      "episode: 33, actor_loss: -0.833, critic_loss: 8.984, mean_reward: -1411.996, best_return: -1096.549\n",
      "episode:  34.0 alpha+beta:  4.9379663\n",
      "episode: 34, actor_loss: -0.764, critic_loss: 8.671, mean_reward: -1185.564, best_return: -1096.549\n",
      "episode:  35.0 alpha+beta:  4.7956295\n",
      "episode: 35, actor_loss: -0.234, critic_loss: 10.833, mean_reward: -1213.692, best_return: -1096.549\n",
      "episode:  36.0 alpha+beta:  4.8472786\n",
      "episode: 36, actor_loss: -0.290, critic_loss: 5.484, mean_reward: -1303.496, best_return: -1096.549\n",
      "episode:  37.0 alpha+beta:  3.0662355\n",
      "episode: 37, actor_loss: 0.008, critic_loss: 7.242, mean_reward: -1328.075, best_return: -1096.549\n",
      "episode:  38.0 alpha+beta:  5.0155854\n",
      "episode: 38, actor_loss: -0.391, critic_loss: 2.409, mean_reward: -1223.774, best_return: -1096.549\n",
      "episode:  39.0 alpha+beta:  2.9244692\n",
      "episode: 39, actor_loss: 0.404, critic_loss: 2.435, mean_reward: -1213.339, best_return: -1096.549\n",
      "episode:  40.0 alpha+beta:  5.3239927\n",
      "episode: 40, actor_loss: 3.587, critic_loss: 5.216, mean_reward: -1323.428, best_return: -1096.549\n",
      "episode:  41.0 alpha+beta:  4.2392387\n",
      "episode: 41, actor_loss: -1.223, critic_loss: 10.154, mean_reward: -1302.154, best_return: -1096.549\n",
      "episode:  42.0 alpha+beta:  4.4949107\n",
      "episode: 42, actor_loss: 0.862, critic_loss: 12.499, mean_reward: -1172.138, best_return: -1096.549\n",
      "episode:  43.0 alpha+beta:  6.3342476\n",
      "episode: 43, actor_loss: -0.272, critic_loss: 11.080, mean_reward: -1354.791, best_return: -1096.549\n",
      "episode:  44.0 alpha+beta:  6.6748824\n",
      "episode: 44, actor_loss: 0.174, critic_loss: 6.987, mean_reward: -1296.941, best_return: -1096.549\n",
      "episode:  45.0 alpha+beta:  3.4531858\n",
      "episode: 45, actor_loss: 0.222, critic_loss: 5.079, mean_reward: -1378.525, best_return: -1096.549\n",
      "episode:  46.0 alpha+beta:  5.750822\n",
      "episode: 46, actor_loss: 0.404, critic_loss: 2.618, mean_reward: -1349.213, best_return: -1096.549\n",
      "episode:  47.0 alpha+beta:  7.5987215\n",
      "episode: 47, actor_loss: 0.279, critic_loss: 0.584, mean_reward: -1241.251, best_return: -1096.549\n",
      "episode:  48.0 alpha+beta:  3.088848\n",
      "episode: 48, actor_loss: -0.327, critic_loss: 1.431, mean_reward: -1337.030, best_return: -1096.549\n",
      "episode:  49.0 alpha+beta:  4.8563275\n",
      "episode: 49, actor_loss: -0.122, critic_loss: 1.198, mean_reward: -1377.357, best_return: -1096.549\n",
      "episode:  50.0 alpha+beta:  2.762895\n",
      "episode: 50, actor_loss: 0.455, critic_loss: 5.985, mean_reward: -1267.703, best_return: -1096.549\n",
      "last 50 episode mean reward:  -1287.5492753997614\n",
      "\n",
      "\n",
      "episode:  51.0 alpha+beta:  4.259066\n",
      "episode: 51, actor_loss: -1.534, critic_loss: 10.057, mean_reward: -1150.400, best_return: -1096.549\n",
      "episode:  52.0 alpha+beta:  4.5181723\n",
      "episode: 52, actor_loss: -0.547, critic_loss: 11.432, mean_reward: -1276.011, best_return: -1096.549\n",
      "episode:  53.0 alpha+beta:  3.7082808\n",
      "episode: 53, actor_loss: -1.250, critic_loss: 5.597, mean_reward: -1239.618, best_return: -1096.549\n",
      "episode:  54.0 alpha+beta:  6.4063377\n",
      "episode: 54, actor_loss: -0.724, critic_loss: 6.135, mean_reward: -1233.768, best_return: -1096.549\n",
      "episode:  55.0 alpha+beta:  2.4443283\n",
      "episode: 55, actor_loss: -1.297, critic_loss: 5.222, mean_reward: -1279.067, best_return: -1096.549\n",
      "episode:  56.0 alpha+beta:  5.2641177\n",
      "episode: 56, actor_loss: -0.757, critic_loss: 6.483, mean_reward: -1387.268, best_return: -1096.549\n",
      "episode:  57.0 alpha+beta:  2.2599092\n",
      "episode: 57, actor_loss: 0.201, critic_loss: 7.070, mean_reward: -1161.971, best_return: -1096.549\n",
      "episode:  58.0 alpha+beta:  2.699609\n",
      "episode: 58, actor_loss: 0.716, critic_loss: 5.274, mean_reward: -1160.555, best_return: -1096.549\n",
      "episode:  59.0 alpha+beta:  2.2720814\n",
      "episode: 59, actor_loss: 1.661, critic_loss: 6.367, mean_reward: -1212.268, best_return: -1096.549\n",
      "episode:  60.0 alpha+beta:  4.881087\n",
      "episode: 60, actor_loss: 1.770, critic_loss: 7.476, mean_reward: -1254.117, best_return: -1096.549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  61.0 alpha+beta:  3.8155599\n",
      "episode: 61, actor_loss: -0.280, critic_loss: 10.442, mean_reward: -1257.975, best_return: -1096.549\n",
      "episode:  62.0 alpha+beta:  2.4021168\n",
      "episode: 62, actor_loss: 0.247, critic_loss: 9.422, mean_reward: -1248.774, best_return: -1096.549\n",
      "episode:  63.0 alpha+beta:  2.7664819\n",
      "episode: 63, actor_loss: -0.073, critic_loss: 5.863, mean_reward: -1173.933, best_return: -1096.549\n",
      "episode:  64.0 alpha+beta:  6.62481\n",
      "episode: 64, actor_loss: 0.182, critic_loss: 8.264, mean_reward: -1233.909, best_return: -1096.549\n",
      "episode:  65.0 alpha+beta:  5.795319\n",
      "episode: 65, actor_loss: -0.092, critic_loss: 4.191, mean_reward: -1320.610, best_return: -1096.549\n",
      "episode:  66.0 alpha+beta:  4.919242\n",
      "episode: 66, actor_loss: -0.129, critic_loss: 6.127, mean_reward: -1280.957, best_return: -1096.549\n",
      "episode:  67.0 alpha+beta:  4.6414375\n",
      "episode: 67, actor_loss: 0.026, critic_loss: 7.135, mean_reward: -1279.466, best_return: -1096.549\n",
      "episode:  68.0 alpha+beta:  2.6362376\n",
      "episode: 68, actor_loss: -1.374, critic_loss: 3.550, mean_reward: -1276.040, best_return: -1096.549\n",
      "episode:  69.0 alpha+beta:  4.1290317\n",
      "episode: 69, actor_loss: -0.136, critic_loss: 5.163, mean_reward: -1196.458, best_return: -1096.549\n",
      "episode:  70.0 alpha+beta:  5.9395504\n",
      "episode: 70, actor_loss: 6.420, critic_loss: 9.141, mean_reward: -1173.565, best_return: -1096.549\n",
      "episode:  71.0 alpha+beta:  3.8372722\n",
      "episode: 71, actor_loss: -1.244, critic_loss: 11.191, mean_reward: -1196.971, best_return: -1096.549\n",
      "episode:  72.0 alpha+beta:  2.7770333\n",
      "episode: 72, actor_loss: 0.482, critic_loss: 7.417, mean_reward: -1254.522, best_return: -1096.549\n",
      "episode:  73.0 alpha+beta:  3.7402265\n",
      "episode: 73, actor_loss: 0.644, critic_loss: 15.201, mean_reward: -1241.305, best_return: -1096.549\n",
      "episode:  74.0 alpha+beta:  3.2236285\n",
      "episode: 74, actor_loss: -0.697, critic_loss: 12.180, mean_reward: -1237.775, best_return: -1096.549\n",
      "episode:  75.0 alpha+beta:  4.87794\n",
      "episode: 75, actor_loss: 0.939, critic_loss: 5.549, mean_reward: -1192.952, best_return: -1096.549\n",
      "episode:  76.0 alpha+beta:  3.4798698\n",
      "episode: 76, actor_loss: 0.961, critic_loss: 6.202, mean_reward: -1434.944, best_return: -1096.549\n",
      "episode:  77.0 alpha+beta:  3.4027357\n",
      "episode: 77, actor_loss: 0.619, critic_loss: 6.352, mean_reward: -1022.947, best_return: -1022.947\n",
      "episode:  78.0 alpha+beta:  2.971314\n",
      "episode: 78, actor_loss: 0.366, critic_loss: 7.563, mean_reward: -1151.801, best_return: -1022.947\n",
      "episode:  79.0 alpha+beta:  3.3295457\n",
      "episode: 79, actor_loss: 0.907, critic_loss: 9.463, mean_reward: -1164.431, best_return: -1022.947\n",
      "episode:  80.0 alpha+beta:  2.9796393\n",
      "episode: 80, actor_loss: 2.872, critic_loss: 12.277, mean_reward: -1136.410, best_return: -1022.947\n",
      "episode:  81.0 alpha+beta:  3.4244137\n",
      "episode: 81, actor_loss: -1.688, critic_loss: 6.893, mean_reward: -1259.139, best_return: -1022.947\n",
      "episode:  82.0 alpha+beta:  3.7486672\n",
      "episode: 82, actor_loss: -0.240, critic_loss: 3.733, mean_reward: -1258.753, best_return: -1022.947\n",
      "episode:  83.0 alpha+beta:  3.6126623\n",
      "episode: 83, actor_loss: 0.408, critic_loss: 6.396, mean_reward: -1415.169, best_return: -1022.947\n",
      "episode:  84.0 alpha+beta:  3.3371844\n",
      "episode: 84, actor_loss: 0.019, critic_loss: 7.480, mean_reward: -1169.202, best_return: -1022.947\n",
      "episode:  85.0 alpha+beta:  3.79992\n",
      "episode: 85, actor_loss: 0.808, critic_loss: 10.276, mean_reward: -1396.135, best_return: -1022.947\n",
      "episode:  86.0 alpha+beta:  4.009559\n",
      "episode: 86, actor_loss: 0.079, critic_loss: 10.357, mean_reward: -1202.564, best_return: -1022.947\n",
      "episode:  87.0 alpha+beta:  3.426148\n",
      "episode: 87, actor_loss: -0.895, critic_loss: 6.246, mean_reward: -1122.877, best_return: -1022.947\n",
      "episode:  88.0 alpha+beta:  3.1101341\n",
      "episode: 88, actor_loss: -0.152, critic_loss: 6.383, mean_reward: -1242.973, best_return: -1022.947\n",
      "episode:  89.0 alpha+beta:  4.7264805\n",
      "episode: 89, actor_loss: 0.398, critic_loss: 3.731, mean_reward: -1152.887, best_return: -1022.947\n",
      "episode:  90.0 alpha+beta:  2.7464287\n",
      "episode: 90, actor_loss: 0.021, critic_loss: 8.224, mean_reward: -1184.163, best_return: -1022.947\n",
      "episode:  91.0 alpha+beta:  3.2775218\n",
      "episode: 91, actor_loss: -0.556, critic_loss: 13.405, mean_reward: -1185.733, best_return: -1022.947\n",
      "episode:  92.0 alpha+beta:  3.6836493\n",
      "episode: 92, actor_loss: -0.447, critic_loss: 10.623, mean_reward: -1116.610, best_return: -1022.947\n",
      "episode:  93.0 alpha+beta:  3.6613722\n",
      "episode: 93, actor_loss: 0.478, critic_loss: 7.108, mean_reward: -1278.098, best_return: -1022.947\n",
      "episode:  94.0 alpha+beta:  3.978336\n",
      "episode: 94, actor_loss: 0.281, critic_loss: 9.195, mean_reward: -1128.080, best_return: -1022.947\n",
      "episode:  95.0 alpha+beta:  4.0330915\n",
      "episode: 95, actor_loss: 0.265, critic_loss: 10.547, mean_reward: -1239.696, best_return: -1022.947\n",
      "episode:  96.0 alpha+beta:  4.158351\n",
      "episode: 96, actor_loss: 1.571, critic_loss: 9.395, mean_reward: -1142.810, best_return: -1022.947\n",
      "episode:  97.0 alpha+beta:  3.633945\n",
      "episode: 97, actor_loss: -0.390, critic_loss: 3.139, mean_reward: -1201.700, best_return: -1022.947\n",
      "episode:  98.0 alpha+beta:  3.764017\n",
      "episode: 98, actor_loss: 1.616, critic_loss: 2.942, mean_reward: -1264.027, best_return: -1022.947\n",
      "episode:  99.0 alpha+beta:  3.852685\n",
      "episode: 99, actor_loss: 0.132, critic_loss: 6.955, mean_reward: -1241.868, best_return: -1022.947\n",
      "episode:  100.0 alpha+beta:  3.3062668\n",
      "episode: 100, actor_loss: 2.392, critic_loss: 7.742, mean_reward: -1292.983, best_return: -1022.947\n",
      "last 50 episode mean reward:  -1226.5250343436303\n",
      "\n",
      "\n",
      "episode:  101.0 alpha+beta:  3.408527\n",
      "episode: 101, actor_loss: -2.645, critic_loss: 10.194, mean_reward: -1058.808, best_return: -1022.947\n",
      "episode:  102.0 alpha+beta:  2.8564906\n",
      "episode: 102, actor_loss: -0.735, critic_loss: 8.860, mean_reward: -1111.423, best_return: -1022.947\n",
      "episode:  103.0 alpha+beta:  3.4942079\n",
      "episode: 103, actor_loss: 1.733, critic_loss: 9.849, mean_reward: -1269.907, best_return: -1022.947\n",
      "episode:  104.0 alpha+beta:  4.1845217\n",
      "episode: 104, actor_loss: 1.730, critic_loss: 9.424, mean_reward: -1216.710, best_return: -1022.947\n",
      "episode:  105.0 alpha+beta:  3.0071893\n",
      "episode: 105, actor_loss: 1.305, critic_loss: 7.416, mean_reward: -1122.635, best_return: -1022.947\n",
      "episode:  106.0 alpha+beta:  3.0608459\n",
      "episode: 106, actor_loss: -0.009, critic_loss: 5.947, mean_reward: -1234.118, best_return: -1022.947\n",
      "episode:  107.0 alpha+beta:  3.3202467\n",
      "episode: 107, actor_loss: 1.737, critic_loss: 12.740, mean_reward: -1249.128, best_return: -1022.947\n",
      "episode:  108.0 alpha+beta:  3.6785402\n",
      "episode: 108, actor_loss: -0.445, critic_loss: 7.505, mean_reward: -1179.047, best_return: -1022.947\n",
      "episode:  109.0 alpha+beta:  3.281436\n",
      "episode: 109, actor_loss: -0.597, critic_loss: 9.134, mean_reward: -1039.513, best_return: -1022.947\n",
      "episode:  110.0 alpha+beta:  3.9575553\n",
      "episode: 110, actor_loss: 4.472, critic_loss: 9.585, mean_reward: -1217.629, best_return: -1022.947\n",
      "episode:  111.0 alpha+beta:  3.0115695\n",
      "episode: 111, actor_loss: -1.125, critic_loss: 12.672, mean_reward: -1182.500, best_return: -1022.947\n",
      "episode:  112.0 alpha+beta:  2.8897357\n",
      "episode: 112, actor_loss: 0.552, critic_loss: 11.996, mean_reward: -1293.999, best_return: -1022.947\n",
      "episode:  113.0 alpha+beta:  3.4217136\n",
      "episode: 113, actor_loss: -0.044, critic_loss: 5.635, mean_reward: -1402.357, best_return: -1022.947\n",
      "episode:  114.0 alpha+beta:  2.787981\n",
      "episode: 114, actor_loss: 0.144, critic_loss: 5.507, mean_reward: -1207.583, best_return: -1022.947\n",
      "episode:  115.0 alpha+beta:  4.049469\n",
      "episode: 115, actor_loss: -1.257, critic_loss: 4.282, mean_reward: -1204.078, best_return: -1022.947\n",
      "episode:  116.0 alpha+beta:  3.489424\n",
      "episode: 116, actor_loss: 1.039, critic_loss: 9.157, mean_reward: -1327.018, best_return: -1022.947\n",
      "episode:  117.0 alpha+beta:  4.1487513\n",
      "episode: 117, actor_loss: 1.517, critic_loss: 11.175, mean_reward: -1208.888, best_return: -1022.947\n",
      "episode:  118.0 alpha+beta:  3.621715\n",
      "episode: 118, actor_loss: 1.026, critic_loss: 5.070, mean_reward: -1269.779, best_return: -1022.947\n",
      "episode:  119.0 alpha+beta:  3.884603\n",
      "episode: 119, actor_loss: 0.310, critic_loss: 3.962, mean_reward: -1169.991, best_return: -1022.947\n",
      "episode:  120.0 alpha+beta:  4.260992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 120, actor_loss: 4.842, critic_loss: 7.572, mean_reward: -1146.421, best_return: -1022.947\n",
      "episode:  121.0 alpha+beta:  3.2369306\n",
      "episode: 121, actor_loss: -1.650, critic_loss: 10.595, mean_reward: -1226.023, best_return: -1022.947\n",
      "episode:  122.0 alpha+beta:  2.9552324\n",
      "episode: 122, actor_loss: 0.601, critic_loss: 3.340, mean_reward: -1227.821, best_return: -1022.947\n",
      "episode:  123.0 alpha+beta:  5.0823374\n",
      "episode: 123, actor_loss: 1.760, critic_loss: 11.831, mean_reward: -1262.370, best_return: -1022.947\n",
      "episode:  124.0 alpha+beta:  3.7852888\n",
      "episode: 124, actor_loss: -0.048, critic_loss: 5.181, mean_reward: -1281.250, best_return: -1022.947\n",
      "episode:  125.0 alpha+beta:  4.2933445\n",
      "episode: 125, actor_loss: -0.378, critic_loss: 7.089, mean_reward: -1154.708, best_return: -1022.947\n",
      "episode:  126.0 alpha+beta:  2.7558932\n",
      "episode: 126, actor_loss: -2.170, critic_loss: 7.905, mean_reward: -1229.643, best_return: -1022.947\n",
      "episode:  127.0 alpha+beta:  3.4581807\n",
      "episode: 127, actor_loss: 1.349, critic_loss: 20.100, mean_reward: -1171.548, best_return: -1022.947\n",
      "episode:  128.0 alpha+beta:  3.0889502\n",
      "episode: 128, actor_loss: 0.054, critic_loss: 13.395, mean_reward: -1255.095, best_return: -1022.947\n",
      "episode:  129.0 alpha+beta:  4.0054545\n",
      "episode: 129, actor_loss: 0.606, critic_loss: 9.847, mean_reward: -1349.834, best_return: -1022.947\n",
      "episode:  130.0 alpha+beta:  3.5349028\n",
      "episode: 130, actor_loss: 4.974, critic_loss: 17.305, mean_reward: -1200.043, best_return: -1022.947\n",
      "episode:  131.0 alpha+beta:  3.4152923\n",
      "episode: 131, actor_loss: -1.242, critic_loss: 6.706, mean_reward: -1162.374, best_return: -1022.947\n",
      "episode:  132.0 alpha+beta:  2.9486585\n",
      "episode: 132, actor_loss: -1.668, critic_loss: 9.629, mean_reward: -1212.711, best_return: -1022.947\n",
      "episode:  133.0 alpha+beta:  3.339284\n",
      "episode: 133, actor_loss: -1.139, critic_loss: 7.351, mean_reward: -1312.479, best_return: -1022.947\n",
      "episode:  134.0 alpha+beta:  3.7218509\n",
      "episode: 134, actor_loss: 0.084, critic_loss: 4.153, mean_reward: -1240.046, best_return: -1022.947\n",
      "episode:  135.0 alpha+beta:  3.4841003\n",
      "episode: 135, actor_loss: 0.986, critic_loss: 7.852, mean_reward: -1186.794, best_return: -1022.947\n",
      "episode:  136.0 alpha+beta:  5.195344\n",
      "episode: 136, actor_loss: 0.243, critic_loss: 9.526, mean_reward: -1167.428, best_return: -1022.947\n",
      "episode:  137.0 alpha+beta:  3.544099\n",
      "episode: 137, actor_loss: 0.524, critic_loss: 7.862, mean_reward: -1218.576, best_return: -1022.947\n",
      "episode:  138.0 alpha+beta:  5.055194\n",
      "episode: 138, actor_loss: 3.228, critic_loss: 12.214, mean_reward: -1210.531, best_return: -1022.947\n",
      "episode:  139.0 alpha+beta:  2.6073554\n",
      "episode: 139, actor_loss: 1.100, critic_loss: 15.118, mean_reward: -1366.232, best_return: -1022.947\n",
      "episode:  140.0 alpha+beta:  2.5681343\n",
      "episode: 140, actor_loss: 1.421, critic_loss: 11.650, mean_reward: -1260.008, best_return: -1022.947\n",
      "episode:  141.0 alpha+beta:  3.122508\n",
      "episode: 141, actor_loss: -1.649, critic_loss: 11.536, mean_reward: -1314.061, best_return: -1022.947\n",
      "episode:  142.0 alpha+beta:  3.4952483\n",
      "episode: 142, actor_loss: 0.752, critic_loss: 7.778, mean_reward: -1143.153, best_return: -1022.947\n",
      "episode:  143.0 alpha+beta:  4.8831563\n",
      "episode: 143, actor_loss: 1.080, critic_loss: 4.477, mean_reward: -1245.339, best_return: -1022.947\n",
      "episode:  144.0 alpha+beta:  3.9654603\n",
      "episode: 144, actor_loss: -0.828, critic_loss: 8.699, mean_reward: -1400.124, best_return: -1022.947\n",
      "episode:  145.0 alpha+beta:  2.559784\n",
      "episode: 145, actor_loss: 0.374, critic_loss: 3.221, mean_reward: -1308.692, best_return: -1022.947\n",
      "episode:  146.0 alpha+beta:  4.9212265\n",
      "episode: 146, actor_loss: 2.278, critic_loss: 8.716, mean_reward: -1070.532, best_return: -1022.947\n",
      "episode:  147.0 alpha+beta:  4.4098854\n",
      "episode: 147, actor_loss: 0.679, critic_loss: 9.296, mean_reward: -1383.840, best_return: -1022.947\n",
      "episode:  148.0 alpha+beta:  2.7223697\n",
      "episode: 148, actor_loss: -0.877, critic_loss: 8.854, mean_reward: -1068.950, best_return: -1022.947\n",
      "episode:  149.0 alpha+beta:  3.660937\n",
      "episode: 149, actor_loss: 0.846, critic_loss: 6.359, mean_reward: -1056.552, best_return: -1022.947\n",
      "episode:  150.0 alpha+beta:  4.877616\n",
      "episode: 150, actor_loss: 6.741, critic_loss: 13.678, mean_reward: -1234.130, best_return: -1022.947\n",
      "last 50 episode mean reward:  -1220.6483614400477\n",
      "\n",
      "\n",
      "episode:  151.0 alpha+beta:  3.2459192\n",
      "episode: 151, actor_loss: -0.607, critic_loss: 8.483, mean_reward: -1148.511, best_return: -1022.947\n",
      "episode:  152.0 alpha+beta:  3.0285888\n",
      "episode: 152, actor_loss: -0.844, critic_loss: 5.091, mean_reward: -1131.785, best_return: -1022.947\n",
      "episode:  153.0 alpha+beta:  4.3297853\n",
      "episode: 153, actor_loss: 1.311, critic_loss: 4.391, mean_reward: -1147.296, best_return: -1022.947\n",
      "episode:  154.0 alpha+beta:  3.2725892\n",
      "episode: 154, actor_loss: -1.184, critic_loss: 15.533, mean_reward: -1215.917, best_return: -1022.947\n",
      "episode:  155.0 alpha+beta:  3.6633294\n",
      "episode: 155, actor_loss: -0.515, critic_loss: 7.988, mean_reward: -1102.828, best_return: -1022.947\n",
      "episode:  156.0 alpha+beta:  4.2273545\n",
      "episode: 156, actor_loss: 0.086, critic_loss: 5.023, mean_reward: -1151.882, best_return: -1022.947\n",
      "episode:  157.0 alpha+beta:  3.5347073\n",
      "episode: 157, actor_loss: 0.733, critic_loss: 10.142, mean_reward: -1133.094, best_return: -1022.947\n",
      "episode:  158.0 alpha+beta:  3.1752563\n",
      "episode: 158, actor_loss: 0.457, critic_loss: 6.813, mean_reward: -1291.678, best_return: -1022.947\n",
      "episode:  159.0 alpha+beta:  3.3541613\n",
      "episode: 159, actor_loss: 1.195, critic_loss: 2.810, mean_reward: -1248.386, best_return: -1022.947\n",
      "episode:  160.0 alpha+beta:  2.966382\n",
      "episode: 160, actor_loss: 5.478, critic_loss: 15.777, mean_reward: -1175.635, best_return: -1022.947\n",
      "episode:  161.0 alpha+beta:  3.5691116\n",
      "episode: 161, actor_loss: -2.726, critic_loss: 11.741, mean_reward: -1282.082, best_return: -1022.947\n",
      "episode:  162.0 alpha+beta:  3.3304157\n",
      "episode: 162, actor_loss: -1.229, critic_loss: 3.645, mean_reward: -1110.835, best_return: -1022.947\n",
      "episode:  163.0 alpha+beta:  4.119791\n",
      "episode: 163, actor_loss: -0.075, critic_loss: 10.099, mean_reward: -1198.664, best_return: -1022.947\n",
      "episode:  164.0 alpha+beta:  3.3292117\n",
      "episode: 164, actor_loss: -0.344, critic_loss: 6.977, mean_reward: -1222.361, best_return: -1022.947\n",
      "episode:  165.0 alpha+beta:  3.7452583\n",
      "episode: 165, actor_loss: 1.265, critic_loss: 11.607, mean_reward: -1105.895, best_return: -1022.947\n",
      "episode:  166.0 alpha+beta:  4.229643\n",
      "episode: 166, actor_loss: 1.461, critic_loss: 7.874, mean_reward: -1353.435, best_return: -1022.947\n",
      "episode:  167.0 alpha+beta:  2.5790224\n",
      "episode: 167, actor_loss: 0.495, critic_loss: 2.888, mean_reward: -1248.205, best_return: -1022.947\n",
      "episode:  168.0 alpha+beta:  2.582544\n",
      "episode: 168, actor_loss: -0.080, critic_loss: 4.384, mean_reward: -1324.896, best_return: -1022.947\n",
      "episode:  169.0 alpha+beta:  2.803712\n",
      "episode: 169, actor_loss: -0.241, critic_loss: 8.486, mean_reward: -1154.516, best_return: -1022.947\n",
      "episode:  170.0 alpha+beta:  2.774389\n",
      "episode: 170, actor_loss: 6.330, critic_loss: 12.570, mean_reward: -1168.468, best_return: -1022.947\n",
      "episode:  171.0 alpha+beta:  3.7323928\n",
      "episode: 171, actor_loss: -1.167, critic_loss: 5.275, mean_reward: -1206.812, best_return: -1022.947\n",
      "episode:  172.0 alpha+beta:  3.1236684\n",
      "episode: 172, actor_loss: -0.649, critic_loss: 5.778, mean_reward: -1199.249, best_return: -1022.947\n",
      "episode:  173.0 alpha+beta:  3.1830406\n",
      "episode: 173, actor_loss: 1.244, critic_loss: 4.005, mean_reward: -1123.878, best_return: -1022.947\n",
      "episode:  174.0 alpha+beta:  3.0049477\n",
      "episode: 174, actor_loss: -0.582, critic_loss: 11.035, mean_reward: -1172.609, best_return: -1022.947\n",
      "episode:  175.0 alpha+beta:  2.9270506\n",
      "episode: 175, actor_loss: 0.471, critic_loss: 6.353, mean_reward: -1260.585, best_return: -1022.947\n",
      "episode:  176.0 alpha+beta:  3.146884\n",
      "episode: 176, actor_loss: 1.734, critic_loss: 5.556, mean_reward: -1282.118, best_return: -1022.947\n",
      "episode:  177.0 alpha+beta:  2.451901\n",
      "episode: 177, actor_loss: -0.541, critic_loss: 10.599, mean_reward: -1127.408, best_return: -1022.947\n",
      "episode:  178.0 alpha+beta:  3.2442589\n",
      "episode: 178, actor_loss: -0.450, critic_loss: 3.529, mean_reward: -1039.460, best_return: -1022.947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  179.0 alpha+beta:  3.0568767\n",
      "episode: 179, actor_loss: 0.373, critic_loss: 5.389, mean_reward: -1090.829, best_return: -1022.947\n",
      "episode:  180.0 alpha+beta:  2.926392\n",
      "episode: 180, actor_loss: 8.024, critic_loss: 20.621, mean_reward: -1074.964, best_return: -1022.947\n",
      "episode:  181.0 alpha+beta:  3.3703275\n",
      "episode: 181, actor_loss: -1.482, critic_loss: 6.773, mean_reward: -1189.833, best_return: -1022.947\n",
      "episode:  182.0 alpha+beta:  4.5059137\n",
      "episode: 182, actor_loss: -0.947, critic_loss: 8.039, mean_reward: -1166.890, best_return: -1022.947\n",
      "episode:  183.0 alpha+beta:  3.6247878\n",
      "episode: 183, actor_loss: -1.504, critic_loss: 3.965, mean_reward: -1237.778, best_return: -1022.947\n",
      "episode:  184.0 alpha+beta:  4.0480504\n",
      "episode: 184, actor_loss: -0.288, critic_loss: 8.866, mean_reward: -1173.617, best_return: -1022.947\n",
      "episode:  185.0 alpha+beta:  4.1773396\n",
      "episode: 185, actor_loss: 1.178, critic_loss: 6.573, mean_reward: -1068.414, best_return: -1022.947\n",
      "episode:  186.0 alpha+beta:  3.7358813\n",
      "episode: 186, actor_loss: 0.132, critic_loss: 5.824, mean_reward: -1151.082, best_return: -1022.947\n",
      "episode:  187.0 alpha+beta:  4.4325356\n",
      "episode: 187, actor_loss: 0.712, critic_loss: 10.508, mean_reward: -1206.029, best_return: -1022.947\n",
      "episode:  188.0 alpha+beta:  3.3301988\n",
      "episode: 188, actor_loss: 1.102, critic_loss: 3.194, mean_reward: -1205.805, best_return: -1022.947\n",
      "episode:  189.0 alpha+beta:  4.6595807\n",
      "episode: 189, actor_loss: 1.771, critic_loss: 8.915, mean_reward: -1082.741, best_return: -1022.947\n",
      "episode:  190.0 alpha+beta:  3.7993987\n",
      "episode: 190, actor_loss: 4.898, critic_loss: 16.307, mean_reward: -1054.339, best_return: -1022.947\n",
      "episode:  191.0 alpha+beta:  3.1604805\n",
      "episode: 191, actor_loss: 0.078, critic_loss: 6.963, mean_reward: -1243.861, best_return: -1022.947\n",
      "episode:  192.0 alpha+beta:  4.2012815\n",
      "episode: 192, actor_loss: -2.328, critic_loss: 9.436, mean_reward: -1136.781, best_return: -1022.947\n",
      "episode:  193.0 alpha+beta:  3.2121315\n",
      "episode: 193, actor_loss: -0.505, critic_loss: 5.786, mean_reward: -1096.898, best_return: -1022.947\n",
      "episode:  194.0 alpha+beta:  4.4037237\n",
      "episode: 194, actor_loss: 1.124, critic_loss: 5.113, mean_reward: -1158.187, best_return: -1022.947\n",
      "episode:  195.0 alpha+beta:  3.1098533\n",
      "episode: 195, actor_loss: -1.302, critic_loss: 5.775, mean_reward: -1148.019, best_return: -1022.947\n",
      "episode:  196.0 alpha+beta:  4.0302343\n",
      "episode: 196, actor_loss: 2.590, critic_loss: 10.532, mean_reward: -1115.514, best_return: -1022.947\n",
      "episode:  197.0 alpha+beta:  3.529131\n",
      "episode: 197, actor_loss: -0.634, critic_loss: 5.964, mean_reward: -1192.396, best_return: -1022.947\n",
      "episode:  198.0 alpha+beta:  3.0788813\n",
      "episode: 198, actor_loss: 1.074, critic_loss: 5.629, mean_reward: -1165.927, best_return: -1022.947\n",
      "episode:  199.0 alpha+beta:  3.244587\n",
      "episode: 199, actor_loss: -0.635, critic_loss: 5.806, mean_reward: -1366.629, best_return: -1022.947\n",
      "episode:  200.0 alpha+beta:  4.6722\n",
      "episode: 200, actor_loss: 8.268, critic_loss: 17.585, mean_reward: -1168.057, best_return: -1022.947\n",
      "last 50 episode mean reward:  -1176.4616163304977\n",
      "\n",
      "\n",
      "episode:  201.0 alpha+beta:  5.369304\n",
      "episode: 201, actor_loss: -0.571, critic_loss: 4.055, mean_reward: -1295.247, best_return: -1022.947\n",
      "episode:  202.0 alpha+beta:  4.28207\n",
      "episode: 202, actor_loss: -2.259, critic_loss: 9.143, mean_reward: -1369.131, best_return: -1022.947\n",
      "episode:  203.0 alpha+beta:  4.473226\n",
      "episode: 203, actor_loss: 1.095, critic_loss: 7.293, mean_reward: -1121.543, best_return: -1022.947\n",
      "episode:  204.0 alpha+beta:  4.4457912\n",
      "episode: 204, actor_loss: -0.104, critic_loss: 8.111, mean_reward: -1156.474, best_return: -1022.947\n",
      "episode:  205.0 alpha+beta:  5.0996814\n",
      "episode: 205, actor_loss: 0.587, critic_loss: 5.196, mean_reward: -1099.063, best_return: -1022.947\n",
      "episode:  206.0 alpha+beta:  4.2705956\n",
      "episode: 206, actor_loss: 0.060, critic_loss: 11.684, mean_reward: -1248.426, best_return: -1022.947\n",
      "episode:  207.0 alpha+beta:  5.3849974\n",
      "episode: 207, actor_loss: 2.419, critic_loss: 10.453, mean_reward: -1220.163, best_return: -1022.947\n",
      "episode:  208.0 alpha+beta:  4.648739\n",
      "episode: 208, actor_loss: -1.726, critic_loss: 9.065, mean_reward: -1192.063, best_return: -1022.947\n",
      "episode:  209.0 alpha+beta:  4.691126\n",
      "episode: 209, actor_loss: -0.075, critic_loss: 7.263, mean_reward: -1091.958, best_return: -1022.947\n",
      "episode:  210.0 alpha+beta:  7.565321\n",
      "episode: 210, actor_loss: 4.531, critic_loss: 9.799, mean_reward: -1177.266, best_return: -1022.947\n",
      "episode:  211.0 alpha+beta:  4.8302107\n",
      "episode: 211, actor_loss: -2.744, critic_loss: 9.124, mean_reward: -1381.512, best_return: -1022.947\n",
      "episode:  212.0 alpha+beta:  6.1415\n",
      "episode: 212, actor_loss: -1.094, critic_loss: 6.830, mean_reward: -1102.063, best_return: -1022.947\n",
      "episode:  213.0 alpha+beta:  5.4843044\n",
      "episode: 213, actor_loss: 0.994, critic_loss: 5.997, mean_reward: -1107.598, best_return: -1022.947\n",
      "episode:  214.0 alpha+beta:  5.673031\n",
      "episode: 214, actor_loss: -0.847, critic_loss: 8.241, mean_reward: -1121.486, best_return: -1022.947\n",
      "episode:  215.0 alpha+beta:  5.895641\n",
      "episode: 215, actor_loss: -0.493, critic_loss: 12.190, mean_reward: -1156.262, best_return: -1022.947\n",
      "episode:  216.0 alpha+beta:  6.252327\n",
      "episode: 216, actor_loss: 2.228, critic_loss: 15.575, mean_reward: -1127.592, best_return: -1022.947\n",
      "episode:  217.0 alpha+beta:  6.5470886\n",
      "episode: 217, actor_loss: -0.808, critic_loss: 8.950, mean_reward: -1326.292, best_return: -1022.947\n",
      "episode:  218.0 alpha+beta:  5.305052\n",
      "episode: 218, actor_loss: 1.397, critic_loss: 12.092, mean_reward: -1176.396, best_return: -1022.947\n",
      "episode:  219.0 alpha+beta:  5.722369\n",
      "episode: 219, actor_loss: -0.143, critic_loss: 23.034, mean_reward: -1132.901, best_return: -1022.947\n",
      "episode:  220.0 alpha+beta:  7.2182446\n",
      "episode: 220, actor_loss: 12.242, critic_loss: 24.772, mean_reward: -1247.987, best_return: -1022.947\n",
      "episode:  221.0 alpha+beta:  5.568212\n",
      "episode: 221, actor_loss: -2.947, critic_loss: 8.042, mean_reward: -1205.661, best_return: -1022.947\n",
      "episode:  222.0 alpha+beta:  5.777064\n",
      "episode: 222, actor_loss: -1.249, critic_loss: 5.068, mean_reward: -1190.581, best_return: -1022.947\n",
      "episode:  223.0 alpha+beta:  6.365392\n",
      "episode: 223, actor_loss: -0.847, critic_loss: 12.702, mean_reward: -1159.608, best_return: -1022.947\n",
      "episode:  224.0 alpha+beta:  6.6975913\n",
      "episode: 224, actor_loss: 1.211, critic_loss: 12.948, mean_reward: -1103.993, best_return: -1022.947\n",
      "episode:  225.0 alpha+beta:  6.222562\n",
      "episode: 225, actor_loss: 1.156, critic_loss: 11.169, mean_reward: -1296.719, best_return: -1022.947\n",
      "episode:  226.0 alpha+beta:  4.5663047\n",
      "episode: 226, actor_loss: -2.768, critic_loss: 10.268, mean_reward: -1084.371, best_return: -1022.947\n",
      "episode:  227.0 alpha+beta:  6.984371\n",
      "episode: 227, actor_loss: 1.026, critic_loss: 20.756, mean_reward: -1313.405, best_return: -1022.947\n",
      "episode:  228.0 alpha+beta:  6.2979975\n",
      "episode: 228, actor_loss: 0.875, critic_loss: 13.510, mean_reward: -1038.785, best_return: -1022.947\n",
      "episode:  229.0 alpha+beta:  6.865815\n",
      "episode: 229, actor_loss: 0.212, critic_loss: 16.595, mean_reward: -1238.833, best_return: -1022.947\n",
      "episode:  230.0 alpha+beta:  6.1144333\n",
      "episode: 230, actor_loss: 7.403, critic_loss: 20.578, mean_reward: -1098.476, best_return: -1022.947\n",
      "episode:  231.0 alpha+beta:  6.8017044\n",
      "episode: 231, actor_loss: -2.808, critic_loss: 12.930, mean_reward: -1187.987, best_return: -1022.947\n",
      "episode:  232.0 alpha+beta:  7.419496\n",
      "episode: 232, actor_loss: -2.682, critic_loss: 10.106, mean_reward: -1164.503, best_return: -1022.947\n",
      "episode:  233.0 alpha+beta:  6.9205875\n",
      "episode: 233, actor_loss: -0.318, critic_loss: 13.598, mean_reward: -1151.278, best_return: -1022.947\n",
      "episode:  234.0 alpha+beta:  6.9451084\n",
      "episode: 234, actor_loss: 0.432, critic_loss: 12.136, mean_reward: -1123.252, best_return: -1022.947\n",
      "episode:  235.0 alpha+beta:  7.599426\n",
      "episode: 235, actor_loss: -0.823, critic_loss: 13.561, mean_reward: -1227.408, best_return: -1022.947\n",
      "episode:  236.0 alpha+beta:  7.9993954\n",
      "episode: 236, actor_loss: 1.539, critic_loss: 17.721, mean_reward: -1153.633, best_return: -1022.947\n",
      "episode:  237.0 alpha+beta:  7.1157036\n",
      "episode: 237, actor_loss: 0.972, critic_loss: 10.387, mean_reward: -1180.916, best_return: -1022.947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  238.0 alpha+beta:  7.211652\n",
      "episode: 238, actor_loss: 0.593, critic_loss: 4.265, mean_reward: -1102.602, best_return: -1022.947\n",
      "episode:  239.0 alpha+beta:  8.132441\n",
      "episode: 239, actor_loss: -0.659, critic_loss: 9.107, mean_reward: -1117.014, best_return: -1022.947\n",
      "episode:  240.0 alpha+beta:  8.363438\n",
      "episode: 240, actor_loss: -2.766, critic_loss: 8.472, mean_reward: -1190.414, best_return: -1022.947\n",
      "episode:  241.0 alpha+beta:  8.475252\n",
      "episode: 241, actor_loss: -0.748, critic_loss: 7.964, mean_reward: -1106.181, best_return: -1022.947\n",
      "episode:  242.0 alpha+beta:  9.64183\n",
      "episode: 242, actor_loss: 0.758, critic_loss: 9.869, mean_reward: -1071.301, best_return: -1022.947\n",
      "episode:  243.0 alpha+beta:  8.618517\n",
      "episode: 243, actor_loss: 1.050, critic_loss: 9.301, mean_reward: -1153.759, best_return: -1022.947\n",
      "episode:  244.0 alpha+beta:  7.4720335\n",
      "episode: 244, actor_loss: 0.584, critic_loss: 9.465, mean_reward: -1261.451, best_return: -1022.947\n",
      "episode:  245.0 alpha+beta:  6.3278785\n",
      "episode: 245, actor_loss: 0.697, critic_loss: 11.085, mean_reward: -1182.183, best_return: -1022.947\n",
      "episode:  246.0 alpha+beta:  6.663129\n",
      "episode: 246, actor_loss: 0.068, critic_loss: 7.823, mean_reward: -1170.000, best_return: -1022.947\n",
      "episode:  247.0 alpha+beta:  5.2453384\n",
      "episode: 247, actor_loss: -1.804, critic_loss: 10.581, mean_reward: -1236.826, best_return: -1022.947\n",
      "episode:  248.0 alpha+beta:  6.0420666\n",
      "episode: 248, actor_loss: -1.024, critic_loss: 3.877, mean_reward: -1279.622, best_return: -1022.947\n",
      "episode:  249.0 alpha+beta:  5.812199\n",
      "episode: 249, actor_loss: -2.212, critic_loss: 16.684, mean_reward: -1226.619, best_return: -1022.947\n",
      "episode:  250.0 alpha+beta:  6.88536\n",
      "episode: 250, actor_loss: -0.929, critic_loss: 47.204, mean_reward: -1265.063, best_return: -1022.947\n",
      "last 50 episode mean reward:  -1182.6774591700448\n",
      "\n",
      "\n",
      "episode:  251.0 alpha+beta:  7.357834\n",
      "episode: 251, actor_loss: -1.778, critic_loss: 14.617, mean_reward: -1241.040, best_return: -1022.947\n",
      "episode:  252.0 alpha+beta:  5.5884304\n",
      "episode: 252, actor_loss: -1.137, critic_loss: 15.311, mean_reward: -1122.863, best_return: -1022.947\n",
      "episode:  253.0 alpha+beta:  6.159363\n",
      "episode: 253, actor_loss: 1.433, critic_loss: 24.723, mean_reward: -1173.754, best_return: -1022.947\n",
      "episode:  254.0 alpha+beta:  8.613397\n",
      "episode: 254, actor_loss: 2.632, critic_loss: 22.110, mean_reward: -1173.642, best_return: -1022.947\n",
      "episode:  255.0 alpha+beta:  7.4762225\n",
      "episode: 255, actor_loss: -0.869, critic_loss: 15.071, mean_reward: -1057.250, best_return: -1022.947\n",
      "episode:  256.0 alpha+beta:  8.494723\n",
      "episode: 256, actor_loss: 2.492, critic_loss: 22.261, mean_reward: -1218.052, best_return: -1022.947\n",
      "episode:  257.0 alpha+beta:  6.9655123\n",
      "episode: 257, actor_loss: 1.223, critic_loss: 18.520, mean_reward: -1112.355, best_return: -1022.947\n",
      "episode:  258.0 alpha+beta:  8.371783\n",
      "episode: 258, actor_loss: 2.382, critic_loss: 23.925, mean_reward: -1193.618, best_return: -1022.947\n",
      "episode:  259.0 alpha+beta:  6.649068\n",
      "episode: 259, actor_loss: 1.754, critic_loss: 15.557, mean_reward: -1145.958, best_return: -1022.947\n",
      "episode:  260.0 alpha+beta:  6.716364\n",
      "episode: 260, actor_loss: 15.019, critic_loss: 52.780, mean_reward: -1168.394, best_return: -1022.947\n",
      "episode:  261.0 alpha+beta:  6.7837625\n",
      "episode: 261, actor_loss: -2.366, critic_loss: 11.083, mean_reward: -1187.402, best_return: -1022.947\n",
      "episode:  262.0 alpha+beta:  7.216071\n",
      "episode: 262, actor_loss: -1.107, critic_loss: 10.969, mean_reward: -1165.253, best_return: -1022.947\n",
      "episode:  263.0 alpha+beta:  8.245553\n",
      "episode: 263, actor_loss: 0.524, critic_loss: 18.985, mean_reward: -1021.018, best_return: -1021.018\n",
      "episode:  264.0 alpha+beta:  7.040036\n",
      "episode: 264, actor_loss: -1.196, critic_loss: 14.368, mean_reward: -1283.974, best_return: -1021.018\n",
      "episode:  265.0 alpha+beta:  6.6717114\n",
      "episode: 265, actor_loss: 1.748, critic_loss: 28.496, mean_reward: -1275.705, best_return: -1021.018\n",
      "episode:  266.0 alpha+beta:  6.73829\n",
      "episode: 266, actor_loss: 1.239, critic_loss: 16.596, mean_reward: -1137.022, best_return: -1021.018\n",
      "episode:  267.0 alpha+beta:  5.8635206\n",
      "episode: 267, actor_loss: 0.549, critic_loss: 21.375, mean_reward: -981.444, best_return: -981.444\n",
      "episode:  268.0 alpha+beta:  7.018344\n",
      "episode: 268, actor_loss: 0.818, critic_loss: 19.529, mean_reward: -1177.866, best_return: -981.444\n",
      "episode:  269.0 alpha+beta:  8.752214\n",
      "episode: 269, actor_loss: 2.557, critic_loss: 27.867, mean_reward: -1189.226, best_return: -981.444\n",
      "episode:  270.0 alpha+beta:  8.778814\n",
      "episode: 270, actor_loss: 16.899, critic_loss: 60.179, mean_reward: -1074.348, best_return: -981.444\n",
      "episode:  271.0 alpha+beta:  6.382809\n",
      "episode: 271, actor_loss: -5.775, critic_loss: 18.044, mean_reward: -1100.146, best_return: -981.444\n",
      "episode:  272.0 alpha+beta:  9.146389\n",
      "episode: 272, actor_loss: 1.637, critic_loss: 21.262, mean_reward: -1013.535, best_return: -981.444\n",
      "episode:  273.0 alpha+beta:  6.3554316\n",
      "episode: 273, actor_loss: -2.531, critic_loss: 9.495, mean_reward: -1021.888, best_return: -981.444\n",
      "episode:  274.0 alpha+beta:  5.599325\n",
      "episode: 274, actor_loss: 0.019, critic_loss: 12.306, mean_reward: -1162.661, best_return: -981.444\n",
      "episode:  275.0 alpha+beta:  5.6244335\n",
      "episode: 275, actor_loss: 1.525, critic_loss: 22.010, mean_reward: -1026.995, best_return: -981.444\n",
      "episode:  276.0 alpha+beta:  5.111851\n",
      "episode: 276, actor_loss: 1.734, critic_loss: 41.906, mean_reward: -1245.654, best_return: -981.444\n",
      "episode:  277.0 alpha+beta:  5.190337\n",
      "episode: 277, actor_loss: 1.797, critic_loss: 47.855, mean_reward: -1113.531, best_return: -981.444\n",
      "episode:  278.0 alpha+beta:  5.3911734\n",
      "episode: 278, actor_loss: 2.053, critic_loss: 80.241, mean_reward: -1077.289, best_return: -981.444\n",
      "episode:  279.0 alpha+beta:  5.4427814\n",
      "episode: 279, actor_loss: 0.894, critic_loss: 7.064, mean_reward: -1145.796, best_return: -981.444\n",
      "episode:  280.0 alpha+beta:  5.2127004\n",
      "episode: 280, actor_loss: 15.702, critic_loss: 75.915, mean_reward: -1220.914, best_return: -981.444\n",
      "episode:  281.0 alpha+beta:  5.6061745\n",
      "episode: 281, actor_loss: -4.557, critic_loss: 30.373, mean_reward: -1219.088, best_return: -981.444\n",
      "episode:  282.0 alpha+beta:  6.833715\n",
      "episode: 282, actor_loss: -1.970, critic_loss: 23.544, mean_reward: -1049.455, best_return: -981.444\n",
      "episode:  283.0 alpha+beta:  7.492955\n",
      "episode: 283, actor_loss: -1.929, critic_loss: 17.584, mean_reward: -1264.490, best_return: -981.444\n",
      "episode:  284.0 alpha+beta:  5.0282035\n",
      "episode: 284, actor_loss: -2.400, critic_loss: 23.992, mean_reward: -1282.720, best_return: -981.444\n",
      "episode:  285.0 alpha+beta:  8.06325\n",
      "episode: 285, actor_loss: -1.001, critic_loss: 22.025, mean_reward: -1145.048, best_return: -981.444\n",
      "episode:  286.0 alpha+beta:  6.0557947\n",
      "episode: 286, actor_loss: -4.802, critic_loss: 28.288, mean_reward: -1269.829, best_return: -981.444\n",
      "episode:  287.0 alpha+beta:  7.228651\n",
      "episode: 287, actor_loss: 2.016, critic_loss: 34.354, mean_reward: -1163.693, best_return: -981.444\n",
      "episode:  288.0 alpha+beta:  6.220126\n",
      "episode: 288, actor_loss: -0.462, critic_loss: 44.423, mean_reward: -1188.448, best_return: -981.444\n",
      "episode:  289.0 alpha+beta:  5.892626\n",
      "episode: 289, actor_loss: 0.598, critic_loss: 32.522, mean_reward: -1134.557, best_return: -981.444\n",
      "episode:  290.0 alpha+beta:  5.588078\n",
      "episode: 290, actor_loss: 12.938, critic_loss: 43.855, mean_reward: -1085.940, best_return: -981.444\n",
      "episode:  291.0 alpha+beta:  8.191325\n",
      "episode: 291, actor_loss: -6.063, critic_loss: 30.537, mean_reward: -1134.657, best_return: -981.444\n",
      "episode:  292.0 alpha+beta:  5.6148033\n",
      "episode: 292, actor_loss: -3.405, critic_loss: 33.514, mean_reward: -1163.129, best_return: -981.444\n",
      "episode:  293.0 alpha+beta:  5.257642\n",
      "episode: 293, actor_loss: -2.037, critic_loss: 25.426, mean_reward: -1239.678, best_return: -981.444\n",
      "episode:  294.0 alpha+beta:  5.8735647\n",
      "episode: 294, actor_loss: 0.273, critic_loss: 14.885, mean_reward: -1221.875, best_return: -981.444\n",
      "episode:  295.0 alpha+beta:  6.101224\n",
      "episode: 295, actor_loss: 0.286, critic_loss: 19.209, mean_reward: -1230.584, best_return: -981.444\n",
      "episode:  296.0 alpha+beta:  5.1168756\n",
      "episode: 296, actor_loss: 0.896, critic_loss: 28.632, mean_reward: -1093.902, best_return: -981.444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  297.0 alpha+beta:  6.3247056\n",
      "episode: 297, actor_loss: 0.134, critic_loss: 24.357, mean_reward: -1162.903, best_return: -981.444\n",
      "episode:  298.0 alpha+beta:  5.973825\n",
      "episode: 298, actor_loss: 0.610, critic_loss: 11.927, mean_reward: -1086.345, best_return: -981.444\n",
      "episode:  299.0 alpha+beta:  5.092509\n",
      "episode: 299, actor_loss: 1.370, critic_loss: 75.298, mean_reward: -1128.169, best_return: -981.444\n",
      "episode:  300.0 alpha+beta:  5.292901\n",
      "episode: 300, actor_loss: 13.479, critic_loss: 42.981, mean_reward: -1049.548, best_return: -981.444\n",
      "last 50 episode mean reward:  -1150.8530050054067\n",
      "\n",
      "\n",
      "episode:  301.0 alpha+beta:  8.34901\n",
      "episode: 301, actor_loss: -7.130, critic_loss: 45.260, mean_reward: -1165.442, best_return: -981.444\n",
      "episode:  302.0 alpha+beta:  7.3827105\n",
      "episode: 302, actor_loss: -3.336, critic_loss: 39.755, mean_reward: -1107.520, best_return: -981.444\n",
      "episode:  303.0 alpha+beta:  5.778127\n",
      "episode: 303, actor_loss: -1.187, critic_loss: 51.662, mean_reward: -1112.913, best_return: -981.444\n",
      "episode:  304.0 alpha+beta:  6.016226\n",
      "episode: 304, actor_loss: 1.126, critic_loss: 36.810, mean_reward: -1110.985, best_return: -981.444\n",
      "episode:  305.0 alpha+beta:  6.6311474\n",
      "episode: 305, actor_loss: -1.097, critic_loss: 22.421, mean_reward: -1146.505, best_return: -981.444\n",
      "episode:  306.0 alpha+beta:  9.004877\n",
      "episode: 306, actor_loss: 1.139, critic_loss: 28.183, mean_reward: -1214.005, best_return: -981.444\n",
      "episode:  307.0 alpha+beta:  6.6130304\n",
      "episode: 307, actor_loss: 2.409, critic_loss: 27.144, mean_reward: -1128.907, best_return: -981.444\n",
      "episode:  308.0 alpha+beta:  4.9813004\n",
      "episode: 308, actor_loss: 2.425, critic_loss: 41.117, mean_reward: -1097.077, best_return: -981.444\n",
      "episode:  309.0 alpha+beta:  4.5329075\n",
      "episode: 309, actor_loss: -2.429, critic_loss: 14.689, mean_reward: -1050.525, best_return: -981.444\n",
      "episode:  310.0 alpha+beta:  6.5239315\n",
      "episode: 310, actor_loss: -4.851, critic_loss: 27.903, mean_reward: -1181.321, best_return: -981.444\n",
      "episode:  311.0 alpha+beta:  9.705545\n",
      "episode: 311, actor_loss: -5.199, critic_loss: 28.379, mean_reward: -1059.740, best_return: -981.444\n",
      "episode:  312.0 alpha+beta:  9.609323\n",
      "episode: 312, actor_loss: -1.626, critic_loss: 20.294, mean_reward: -1110.619, best_return: -981.444\n",
      "episode:  313.0 alpha+beta:  7.3545055\n",
      "episode: 313, actor_loss: -4.141, critic_loss: 23.219, mean_reward: -1136.309, best_return: -981.444\n",
      "episode:  314.0 alpha+beta:  6.5817785\n",
      "episode: 314, actor_loss: -0.986, critic_loss: 12.021, mean_reward: -1159.363, best_return: -981.444\n",
      "episode:  315.0 alpha+beta:  9.466401\n",
      "episode: 315, actor_loss: 6.045, critic_loss: 67.924, mean_reward: -1127.462, best_return: -981.444\n",
      "episode:  316.0 alpha+beta:  6.9461327\n",
      "episode: 316, actor_loss: -2.064, critic_loss: 35.977, mean_reward: -1164.706, best_return: -981.444\n",
      "episode:  317.0 alpha+beta:  6.803476\n",
      "episode: 317, actor_loss: 0.428, critic_loss: 35.830, mean_reward: -1197.475, best_return: -981.444\n",
      "episode:  318.0 alpha+beta:  5.668851\n",
      "episode: 318, actor_loss: 0.860, critic_loss: 18.866, mean_reward: -1126.225, best_return: -981.444\n",
      "episode:  319.0 alpha+beta:  5.850328\n",
      "episode: 319, actor_loss: 0.573, critic_loss: 18.577, mean_reward: -1210.532, best_return: -981.444\n",
      "episode:  320.0 alpha+beta:  5.5517044\n",
      "episode: 320, actor_loss: 11.564, critic_loss: 77.875, mean_reward: -1130.763, best_return: -981.444\n",
      "episode:  321.0 alpha+beta:  6.2233515\n",
      "episode: 321, actor_loss: -6.150, critic_loss: 31.029, mean_reward: -1190.369, best_return: -981.444\n",
      "episode:  322.0 alpha+beta:  9.967743\n",
      "episode: 322, actor_loss: 0.678, critic_loss: 27.708, mean_reward: -1209.944, best_return: -981.444\n",
      "episode:  323.0 alpha+beta:  9.131469\n",
      "episode: 323, actor_loss: -2.560, critic_loss: 20.337, mean_reward: -1092.711, best_return: -981.444\n",
      "episode:  324.0 alpha+beta:  7.592453\n",
      "episode: 324, actor_loss: 1.352, critic_loss: 28.365, mean_reward: -1242.141, best_return: -981.444\n",
      "episode:  325.0 alpha+beta:  6.789581\n",
      "episode: 325, actor_loss: -0.345, critic_loss: 26.342, mean_reward: -1171.386, best_return: -981.444\n",
      "episode:  326.0 alpha+beta:  6.4442263\n",
      "episode: 326, actor_loss: 0.689, critic_loss: 28.218, mean_reward: -1210.228, best_return: -981.444\n",
      "episode:  327.0 alpha+beta:  7.3660865\n",
      "episode: 327, actor_loss: -0.429, critic_loss: 25.241, mean_reward: -1126.824, best_return: -981.444\n",
      "episode:  328.0 alpha+beta:  10.273319\n",
      "episode: 328, actor_loss: 3.377, critic_loss: 25.443, mean_reward: -1275.852, best_return: -981.444\n",
      "episode:  329.0 alpha+beta:  7.6833425\n",
      "episode: 329, actor_loss: 4.241, critic_loss: 58.165, mean_reward: -1207.130, best_return: -981.444\n",
      "episode:  330.0 alpha+beta:  7.5883007\n",
      "episode: 330, actor_loss: 2.472, critic_loss: 51.533, mean_reward: -1198.885, best_return: -981.444\n",
      "episode:  331.0 alpha+beta:  9.222914\n",
      "episode: 331, actor_loss: -6.727, critic_loss: 17.095, mean_reward: -1080.907, best_return: -981.444\n",
      "episode:  332.0 alpha+beta:  7.0473003\n",
      "episode: 332, actor_loss: -4.899, critic_loss: 30.741, mean_reward: -1201.853, best_return: -981.444\n",
      "episode:  333.0 alpha+beta:  6.3991156\n",
      "episode: 333, actor_loss: 0.007, critic_loss: 28.393, mean_reward: -1217.370, best_return: -981.444\n",
      "episode:  334.0 alpha+beta:  5.455184\n",
      "episode: 334, actor_loss: 0.903, critic_loss: 27.649, mean_reward: -1122.075, best_return: -981.444\n",
      "episode:  335.0 alpha+beta:  7.8740745\n",
      "episode: 335, actor_loss: 0.201, critic_loss: 9.595, mean_reward: -1235.946, best_return: -981.444\n",
      "episode:  336.0 alpha+beta:  6.732671\n",
      "episode: 336, actor_loss: 0.561, critic_loss: 45.211, mean_reward: -1102.338, best_return: -981.444\n",
      "episode:  337.0 alpha+beta:  6.4969897\n",
      "episode: 337, actor_loss: -2.117, critic_loss: 59.934, mean_reward: -1149.226, best_return: -981.444\n",
      "episode:  338.0 alpha+beta:  6.0691996\n",
      "episode: 338, actor_loss: -0.104, critic_loss: 13.515, mean_reward: -1180.407, best_return: -981.444\n",
      "episode:  339.0 alpha+beta:  10.202449\n",
      "episode: 339, actor_loss: 2.012, critic_loss: 23.432, mean_reward: -1085.359, best_return: -981.444\n",
      "episode:  340.0 alpha+beta:  6.586304\n",
      "episode: 340, actor_loss: 13.617, critic_loss: 91.054, mean_reward: -1218.434, best_return: -981.444\n",
      "episode:  341.0 alpha+beta:  7.638914\n",
      "episode: 341, actor_loss: -3.551, critic_loss: 20.607, mean_reward: -1225.774, best_return: -981.444\n",
      "episode:  342.0 alpha+beta:  10.789291\n",
      "episode: 342, actor_loss: 0.032, critic_loss: 57.672, mean_reward: -1050.682, best_return: -981.444\n",
      "episode:  343.0 alpha+beta:  7.0132384\n",
      "episode: 343, actor_loss: -2.990, critic_loss: 15.699, mean_reward: -1109.497, best_return: -981.444\n",
      "episode:  344.0 alpha+beta:  5.4102707\n",
      "episode: 344, actor_loss: -1.341, critic_loss: 40.021, mean_reward: -1180.755, best_return: -981.444\n",
      "episode:  345.0 alpha+beta:  6.814641\n",
      "episode: 345, actor_loss: 1.095, critic_loss: 14.504, mean_reward: -1144.318, best_return: -981.444\n",
      "episode:  346.0 alpha+beta:  5.5412893\n",
      "episode: 346, actor_loss: -0.336, critic_loss: 9.811, mean_reward: -1095.348, best_return: -981.444\n",
      "episode:  347.0 alpha+beta:  7.2191553\n",
      "episode: 347, actor_loss: 2.024, critic_loss: 73.089, mean_reward: -1218.124, best_return: -981.444\n",
      "episode:  348.0 alpha+beta:  5.9476576\n",
      "episode: 348, actor_loss: 1.202, critic_loss: 45.696, mean_reward: -1200.565, best_return: -981.444\n",
      "episode:  349.0 alpha+beta:  6.635145\n",
      "episode: 349, actor_loss: 0.822, critic_loss: 27.031, mean_reward: -1179.751, best_return: -981.444\n",
      "episode:  350.0 alpha+beta:  5.8067827\n",
      "episode: 350, actor_loss: 11.460, critic_loss: 84.854, mean_reward: -1150.526, best_return: -981.444\n",
      "last 50 episode mean reward:  -1156.262367933665\n",
      "\n",
      "\n",
      "episode:  351.0 alpha+beta:  8.331712\n",
      "episode: 351, actor_loss: -8.880, critic_loss: 44.182, mean_reward: -1354.731, best_return: -981.444\n",
      "episode:  352.0 alpha+beta:  10.304554\n",
      "episode: 352, actor_loss: 1.591, critic_loss: 47.100, mean_reward: -1226.839, best_return: -981.444\n",
      "episode:  353.0 alpha+beta:  8.359699\n",
      "episode: 353, actor_loss: -2.392, critic_loss: 42.559, mean_reward: -1074.243, best_return: -981.444\n",
      "episode:  354.0 alpha+beta:  8.731614\n",
      "episode: 354, actor_loss: 0.417, critic_loss: 23.733, mean_reward: -1189.345, best_return: -981.444\n",
      "episode:  355.0 alpha+beta:  7.3205338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 355, actor_loss: 2.315, critic_loss: 26.586, mean_reward: -1156.588, best_return: -981.444\n",
      "episode:  356.0 alpha+beta:  6.7975254\n",
      "episode: 356, actor_loss: 1.913, critic_loss: 37.799, mean_reward: -1143.593, best_return: -981.444\n",
      "episode:  357.0 alpha+beta:  4.2506714\n",
      "episode: 357, actor_loss: 4.997, critic_loss: 32.246, mean_reward: -1124.683, best_return: -981.444\n",
      "episode:  358.0 alpha+beta:  5.3039355\n",
      "episode: 358, actor_loss: -4.305, critic_loss: 15.869, mean_reward: -1137.042, best_return: -981.444\n",
      "episode:  359.0 alpha+beta:  6.9663277\n",
      "episode: 359, actor_loss: 0.466, critic_loss: 12.307, mean_reward: -1116.991, best_return: -981.444\n",
      "episode:  360.0 alpha+beta:  12.019426\n",
      "episode: 360, actor_loss: 12.229, critic_loss: 62.906, mean_reward: -1135.488, best_return: -981.444\n",
      "episode:  361.0 alpha+beta:  8.450974\n",
      "episode: 361, actor_loss: -5.623, critic_loss: 18.610, mean_reward: -1177.219, best_return: -981.444\n",
      "episode:  362.0 alpha+beta:  11.000484\n",
      "episode: 362, actor_loss: -3.160, critic_loss: 31.733, mean_reward: -1328.519, best_return: -981.444\n",
      "episode:  363.0 alpha+beta:  10.088631\n",
      "episode: 363, actor_loss: -2.190, critic_loss: 24.071, mean_reward: -1225.288, best_return: -981.444\n",
      "episode:  364.0 alpha+beta:  6.4202795\n",
      "episode: 364, actor_loss: -1.003, critic_loss: 27.020, mean_reward: -1178.236, best_return: -981.444\n",
      "episode:  365.0 alpha+beta:  6.0447655\n",
      "episode: 365, actor_loss: 0.874, critic_loss: 39.387, mean_reward: -1208.965, best_return: -981.444\n",
      "episode:  366.0 alpha+beta:  7.872846\n",
      "episode: 366, actor_loss: -0.479, critic_loss: 10.478, mean_reward: -1162.174, best_return: -981.444\n",
      "episode:  367.0 alpha+beta:  7.649478\n",
      "episode: 367, actor_loss: 0.191, critic_loss: 25.456, mean_reward: -1297.089, best_return: -981.444\n",
      "episode:  368.0 alpha+beta:  8.20702\n",
      "episode: 368, actor_loss: 1.366, critic_loss: 17.476, mean_reward: -1155.213, best_return: -981.444\n",
      "episode:  369.0 alpha+beta:  7.960299\n",
      "episode: 369, actor_loss: 1.857, critic_loss: 54.090, mean_reward: -1235.127, best_return: -981.444\n",
      "episode:  370.0 alpha+beta:  8.133336\n",
      "episode: 370, actor_loss: 16.712, critic_loss: 52.629, mean_reward: -1111.412, best_return: -981.444\n",
      "episode:  371.0 alpha+beta:  8.99802\n",
      "episode: 371, actor_loss: -5.500, critic_loss: 33.641, mean_reward: -1183.146, best_return: -981.444\n",
      "episode:  372.0 alpha+beta:  8.124487\n",
      "episode: 372, actor_loss: -0.225, critic_loss: 58.815, mean_reward: -1255.050, best_return: -981.444\n",
      "episode:  373.0 alpha+beta:  7.5158396\n",
      "episode: 373, actor_loss: -7.548, critic_loss: 43.513, mean_reward: -1120.461, best_return: -981.444\n",
      "episode:  374.0 alpha+beta:  6.371457\n",
      "episode: 374, actor_loss: -0.011, critic_loss: 40.669, mean_reward: -1211.945, best_return: -981.444\n",
      "episode:  375.0 alpha+beta:  7.0485106\n",
      "episode: 375, actor_loss: -0.341, critic_loss: 27.806, mean_reward: -1278.368, best_return: -981.444\n",
      "episode:  376.0 alpha+beta:  6.6428013\n",
      "episode: 376, actor_loss: -0.840, critic_loss: 8.829, mean_reward: -1188.214, best_return: -981.444\n",
      "episode:  377.0 alpha+beta:  6.499941\n",
      "episode: 377, actor_loss: 0.788, critic_loss: 16.035, mean_reward: -1191.068, best_return: -981.444\n",
      "episode:  378.0 alpha+beta:  6.5666666\n",
      "episode: 378, actor_loss: 1.527, critic_loss: 37.364, mean_reward: -1114.069, best_return: -981.444\n",
      "episode:  379.0 alpha+beta:  6.73529\n",
      "episode: 379, actor_loss: 1.067, critic_loss: 40.133, mean_reward: -1071.430, best_return: -981.444\n",
      "episode:  380.0 alpha+beta:  6.109967\n",
      "episode: 380, actor_loss: 12.122, critic_loss: 55.552, mean_reward: -1131.328, best_return: -981.444\n",
      "episode:  381.0 alpha+beta:  8.566897\n",
      "episode: 381, actor_loss: -4.484, critic_loss: 41.435, mean_reward: -1147.553, best_return: -981.444\n",
      "episode:  382.0 alpha+beta:  7.9731307\n",
      "episode: 382, actor_loss: -4.528, critic_loss: 49.789, mean_reward: -1117.425, best_return: -981.444\n",
      "episode:  383.0 alpha+beta:  5.619363\n",
      "episode: 383, actor_loss: -3.845, critic_loss: 27.525, mean_reward: -1166.816, best_return: -981.444\n",
      "episode:  384.0 alpha+beta:  14.971088\n",
      "episode: 384, actor_loss: 2.171, critic_loss: 29.574, mean_reward: -1121.182, best_return: -981.444\n",
      "episode:  385.0 alpha+beta:  7.4445057\n",
      "episode: 385, actor_loss: -1.722, critic_loss: 20.232, mean_reward: -1262.662, best_return: -981.444\n",
      "episode:  386.0 alpha+beta:  9.675311\n",
      "episode: 386, actor_loss: -1.888, critic_loss: 19.416, mean_reward: -1179.861, best_return: -981.444\n",
      "episode:  387.0 alpha+beta:  5.691957\n",
      "episode: 387, actor_loss: -1.074, critic_loss: 19.480, mean_reward: -1069.522, best_return: -981.444\n",
      "episode:  388.0 alpha+beta:  6.589121\n",
      "episode: 388, actor_loss: -0.368, critic_loss: 4.498, mean_reward: -1185.392, best_return: -981.444\n",
      "episode:  389.0 alpha+beta:  6.0150623\n",
      "episode: 389, actor_loss: 0.226, critic_loss: 3.295, mean_reward: -1295.777, best_return: -981.444\n",
      "episode:  390.0 alpha+beta:  6.0779266\n",
      "episode: 390, actor_loss: 13.381, critic_loss: 54.509, mean_reward: -1073.531, best_return: -981.444\n",
      "episode:  391.0 alpha+beta:  9.086878\n",
      "episode: 391, actor_loss: -5.195, critic_loss: 42.140, mean_reward: -1131.232, best_return: -981.444\n",
      "episode:  392.0 alpha+beta:  8.132545\n",
      "episode: 392, actor_loss: -4.569, critic_loss: 31.037, mean_reward: -1094.981, best_return: -981.444\n",
      "episode:  393.0 alpha+beta:  8.571511\n",
      "episode: 393, actor_loss: -6.119, critic_loss: 30.697, mean_reward: -1090.214, best_return: -981.444\n",
      "episode:  394.0 alpha+beta:  7.7166333\n",
      "episode: 394, actor_loss: -1.744, critic_loss: 23.523, mean_reward: -1040.245, best_return: -981.444\n",
      "episode:  395.0 alpha+beta:  6.9169188\n",
      "episode: 395, actor_loss: -1.530, critic_loss: 13.922, mean_reward: -1159.709, best_return: -981.444\n",
      "episode:  396.0 alpha+beta:  11.863724\n",
      "episode: 396, actor_loss: 5.649, critic_loss: 22.688, mean_reward: -1141.136, best_return: -981.444\n",
      "episode:  397.0 alpha+beta:  6.3287983\n",
      "episode: 397, actor_loss: -0.778, critic_loss: 20.104, mean_reward: -1133.848, best_return: -981.444\n",
      "episode:  398.0 alpha+beta:  7.230787\n",
      "episode: 398, actor_loss: 0.775, critic_loss: 11.335, mean_reward: -1085.682, best_return: -981.444\n",
      "episode:  399.0 alpha+beta:  7.368964\n",
      "episode: 399, actor_loss: 2.326, critic_loss: 27.484, mean_reward: -1160.571, best_return: -981.444\n",
      "episode:  400.0 alpha+beta:  7.5097847\n",
      "episode: 400, actor_loss: 7.888, critic_loss: 49.378, mean_reward: -1096.689, best_return: -981.444\n",
      "last 50 episode mean reward:  -1164.757817869012\n",
      "\n",
      "\n",
      "episode:  401.0 alpha+beta:  8.281788\n",
      "episode: 401, actor_loss: -9.842, critic_loss: 34.135, mean_reward: -1084.206, best_return: -981.444\n",
      "episode:  402.0 alpha+beta:  12.566792\n",
      "episode: 402, actor_loss: -2.117, critic_loss: 26.378, mean_reward: -1253.501, best_return: -981.444\n",
      "episode:  403.0 alpha+beta:  9.200914\n",
      "episode: 403, actor_loss: -3.949, critic_loss: 13.990, mean_reward: -1241.133, best_return: -981.444\n",
      "episode:  404.0 alpha+beta:  8.952364\n",
      "episode: 404, actor_loss: 2.027, critic_loss: 18.374, mean_reward: -1161.510, best_return: -981.444\n",
      "episode:  405.0 alpha+beta:  9.687386\n",
      "episode: 405, actor_loss: 0.687, critic_loss: 10.930, mean_reward: -1110.518, best_return: -981.444\n",
      "episode:  406.0 alpha+beta:  7.881067\n",
      "episode: 406, actor_loss: 0.239, critic_loss: 14.043, mean_reward: -1213.545, best_return: -981.444\n",
      "episode:  407.0 alpha+beta:  7.913548\n",
      "episode: 407, actor_loss: 1.163, critic_loss: 8.858, mean_reward: -1167.464, best_return: -981.444\n",
      "episode:  408.0 alpha+beta:  6.7890887\n",
      "episode: 408, actor_loss: 0.895, critic_loss: 10.294, mean_reward: -1168.486, best_return: -981.444\n",
      "episode:  409.0 alpha+beta:  6.6427493\n",
      "episode: 409, actor_loss: 1.512, critic_loss: 8.303, mean_reward: -1272.309, best_return: -981.444\n",
      "episode:  410.0 alpha+beta:  6.546433\n",
      "episode: 410, actor_loss: 8.028, critic_loss: 94.240, mean_reward: -1148.889, best_return: -981.444\n",
      "episode:  411.0 alpha+beta:  10.809102\n",
      "episode: 411, actor_loss: -11.429, critic_loss: 47.986, mean_reward: -1145.528, best_return: -981.444\n",
      "episode:  412.0 alpha+beta:  8.229698\n",
      "episode: 412, actor_loss: -5.523, critic_loss: 53.513, mean_reward: -1164.370, best_return: -981.444\n",
      "episode:  413.0 alpha+beta:  9.254654\n",
      "episode: 413, actor_loss: -0.194, critic_loss: 34.740, mean_reward: -1147.304, best_return: -981.444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  414.0 alpha+beta:  8.330599\n",
      "episode: 414, actor_loss: -0.938, critic_loss: 26.052, mean_reward: -1146.237, best_return: -981.444\n",
      "episode:  415.0 alpha+beta:  7.726184\n",
      "episode: 415, actor_loss: -2.090, critic_loss: 27.142, mean_reward: -1102.794, best_return: -981.444\n",
      "episode:  416.0 alpha+beta:  6.725858\n",
      "episode: 416, actor_loss: -0.020, critic_loss: 11.921, mean_reward: -1127.620, best_return: -981.444\n",
      "episode:  417.0 alpha+beta:  10.236495\n",
      "episode: 417, actor_loss: 4.833, critic_loss: 35.017, mean_reward: -1248.890, best_return: -981.444\n",
      "episode:  418.0 alpha+beta:  8.220944\n",
      "episode: 418, actor_loss: -1.088, critic_loss: 23.661, mean_reward: -1148.270, best_return: -981.444\n",
      "episode:  419.0 alpha+beta:  7.4014883\n",
      "episode: 419, actor_loss: 1.508, critic_loss: 23.951, mean_reward: -1154.774, best_return: -981.444\n",
      "episode:  420.0 alpha+beta:  8.155012\n",
      "episode: 420, actor_loss: 7.797, critic_loss: 74.470, mean_reward: -1132.443, best_return: -981.444\n",
      "episode:  421.0 alpha+beta:  12.155787\n",
      "episode: 421, actor_loss: -2.458, critic_loss: 16.033, mean_reward: -1173.416, best_return: -981.444\n",
      "episode:  422.0 alpha+beta:  10.789192\n",
      "episode: 422, actor_loss: -2.087, critic_loss: 24.732, mean_reward: -1144.175, best_return: -981.444\n",
      "episode:  423.0 alpha+beta:  9.751852\n",
      "episode: 423, actor_loss: -0.944, critic_loss: 18.209, mean_reward: -1260.147, best_return: -981.444\n",
      "episode:  424.0 alpha+beta:  7.358962\n",
      "episode: 424, actor_loss: -2.599, critic_loss: 16.517, mean_reward: -1152.206, best_return: -981.444\n",
      "episode:  425.0 alpha+beta:  7.9732857\n",
      "episode: 425, actor_loss: 1.441, critic_loss: 27.781, mean_reward: -1111.749, best_return: -981.444\n",
      "episode:  426.0 alpha+beta:  6.8703246\n",
      "episode: 426, actor_loss: 1.420, critic_loss: 30.984, mean_reward: -1204.466, best_return: -981.444\n",
      "episode:  427.0 alpha+beta:  7.0285254\n",
      "episode: 427, actor_loss: 1.429, critic_loss: 17.682, mean_reward: -1127.844, best_return: -981.444\n",
      "episode:  428.0 alpha+beta:  7.363934\n",
      "episode: 428, actor_loss: 1.245, critic_loss: 29.399, mean_reward: -1156.271, best_return: -981.444\n",
      "episode:  429.0 alpha+beta:  7.4275317\n",
      "episode: 429, actor_loss: 0.833, critic_loss: 23.160, mean_reward: -1186.009, best_return: -981.444\n",
      "episode:  430.0 alpha+beta:  7.593403\n",
      "episode: 430, actor_loss: 10.650, critic_loss: 48.212, mean_reward: -1101.826, best_return: -981.444\n",
      "episode:  431.0 alpha+beta:  8.627747\n",
      "episode: 431, actor_loss: -4.472, critic_loss: 27.079, mean_reward: -1296.309, best_return: -981.444\n",
      "episode:  432.0 alpha+beta:  7.541319\n",
      "episode: 432, actor_loss: 0.744, critic_loss: 32.904, mean_reward: -1011.632, best_return: -981.444\n",
      "episode:  433.0 alpha+beta:  11.6021385\n",
      "episode: 433, actor_loss: 0.747, critic_loss: 32.799, mean_reward: -1121.053, best_return: -981.444\n",
      "episode:  434.0 alpha+beta:  7.6444006\n",
      "episode: 434, actor_loss: -4.547, critic_loss: 36.894, mean_reward: -1187.068, best_return: -981.444\n",
      "episode:  435.0 alpha+beta:  9.238686\n",
      "episode: 435, actor_loss: -0.109, critic_loss: 19.564, mean_reward: -1082.253, best_return: -981.444\n",
      "episode:  436.0 alpha+beta:  7.767552\n",
      "episode: 436, actor_loss: 0.078, critic_loss: 16.702, mean_reward: -1253.508, best_return: -981.444\n",
      "episode:  437.0 alpha+beta:  12.386436\n",
      "episode: 437, actor_loss: -0.264, critic_loss: 19.249, mean_reward: -1136.954, best_return: -981.444\n",
      "episode:  438.0 alpha+beta:  8.978149\n",
      "episode: 438, actor_loss: 0.589, critic_loss: 6.254, mean_reward: -1076.805, best_return: -981.444\n",
      "episode:  439.0 alpha+beta:  6.6947637\n",
      "episode: 439, actor_loss: 1.407, critic_loss: 31.757, mean_reward: -1211.526, best_return: -981.444\n",
      "episode:  440.0 alpha+beta:  6.9320903\n",
      "episode: 440, actor_loss: 14.670, critic_loss: 51.175, mean_reward: -1337.642, best_return: -981.444\n",
      "episode:  441.0 alpha+beta:  11.209832\n",
      "episode: 441, actor_loss: -7.343, critic_loss: 35.225, mean_reward: -1177.072, best_return: -981.444\n",
      "episode:  442.0 alpha+beta:  9.081217\n",
      "episode: 442, actor_loss: -3.037, critic_loss: 35.144, mean_reward: -1201.748, best_return: -981.444\n",
      "episode:  443.0 alpha+beta:  12.151836\n",
      "episode: 443, actor_loss: -2.647, critic_loss: 21.811, mean_reward: -1166.050, best_return: -981.444\n",
      "episode:  444.0 alpha+beta:  9.623417\n",
      "episode: 444, actor_loss: -1.702, critic_loss: 38.347, mean_reward: -1116.166, best_return: -981.444\n",
      "episode:  445.0 alpha+beta:  7.566449\n",
      "episode: 445, actor_loss: -3.537, critic_loss: 11.118, mean_reward: -1162.157, best_return: -981.444\n",
      "episode:  446.0 alpha+beta:  8.501868\n",
      "episode: 446, actor_loss: 0.911, critic_loss: 61.118, mean_reward: -1122.739, best_return: -981.444\n",
      "episode:  447.0 alpha+beta:  7.667181\n",
      "episode: 447, actor_loss: 0.051, critic_loss: 3.540, mean_reward: -1136.750, best_return: -981.444\n",
      "episode:  448.0 alpha+beta:  7.779751\n",
      "episode: 448, actor_loss: 1.157, critic_loss: 14.605, mean_reward: -1192.538, best_return: -981.444\n",
      "episode:  449.0 alpha+beta:  7.631221\n",
      "episode: 449, actor_loss: 1.121, critic_loss: 20.267, mean_reward: -1112.157, best_return: -981.444\n",
      "episode:  450.0 alpha+beta:  7.9560776\n",
      "episode: 450, actor_loss: 13.744, critic_loss: 34.332, mean_reward: -1109.507, best_return: -981.444\n",
      "last 50 episode mean reward:  -1163.390619924441\n",
      "\n",
      "\n",
      "episode:  451.0 alpha+beta:  11.154368\n",
      "episode: 451, actor_loss: -9.861, critic_loss: 44.904, mean_reward: -1204.795, best_return: -981.444\n",
      "episode:  452.0 alpha+beta:  14.333251\n",
      "episode: 452, actor_loss: -3.891, critic_loss: 15.449, mean_reward: -1301.080, best_return: -981.444\n",
      "episode:  453.0 alpha+beta:  9.104806\n",
      "episode: 453, actor_loss: -4.211, critic_loss: 32.594, mean_reward: -1109.226, best_return: -981.444\n",
      "episode:  454.0 alpha+beta:  10.841252\n",
      "episode: 454, actor_loss: 1.770, critic_loss: 58.603, mean_reward: -1239.793, best_return: -981.444\n",
      "episode:  455.0 alpha+beta:  8.69374\n",
      "episode: 455, actor_loss: -1.707, critic_loss: 15.498, mean_reward: -1104.251, best_return: -981.444\n",
      "episode:  456.0 alpha+beta:  8.539173\n",
      "episode: 456, actor_loss: 1.662, critic_loss: 18.074, mean_reward: -1162.316, best_return: -981.444\n",
      "episode:  457.0 alpha+beta:  8.493792\n",
      "episode: 457, actor_loss: 1.670, critic_loss: 25.598, mean_reward: -1058.821, best_return: -981.444\n",
      "episode:  458.0 alpha+beta:  8.921549\n",
      "episode: 458, actor_loss: 1.750, critic_loss: 19.260, mean_reward: -1122.895, best_return: -981.444\n",
      "episode:  459.0 alpha+beta:  8.64884\n",
      "episode: 459, actor_loss: 1.320, critic_loss: 18.925, mean_reward: -1065.406, best_return: -981.444\n",
      "episode:  460.0 alpha+beta:  9.242949\n",
      "episode: 460, actor_loss: 11.989, critic_loss: 41.084, mean_reward: -1122.647, best_return: -981.444\n",
      "episode:  461.0 alpha+beta:  10.52277\n",
      "episode: 461, actor_loss: -8.232, critic_loss: 50.545, mean_reward: -1139.928, best_return: -981.444\n",
      "episode:  462.0 alpha+beta:  11.4081135\n",
      "episode: 462, actor_loss: -2.029, critic_loss: 20.111, mean_reward: -1240.165, best_return: -981.444\n",
      "episode:  463.0 alpha+beta:  9.958131\n",
      "episode: 463, actor_loss: 0.567, critic_loss: 25.321, mean_reward: -1153.092, best_return: -981.444\n",
      "episode:  464.0 alpha+beta:  10.567556\n",
      "episode: 464, actor_loss: -2.867, critic_loss: 14.577, mean_reward: -1177.098, best_return: -981.444\n",
      "episode:  465.0 alpha+beta:  16.194542\n",
      "episode: 465, actor_loss: -2.244, critic_loss: 19.600, mean_reward: -1025.947, best_return: -981.444\n",
      "episode:  466.0 alpha+beta:  8.933758\n",
      "episode: 466, actor_loss: -1.038, critic_loss: 11.074, mean_reward: -1114.073, best_return: -981.444\n",
      "episode:  467.0 alpha+beta:  9.725334\n",
      "episode: 467, actor_loss: 1.375, critic_loss: 26.095, mean_reward: -1096.110, best_return: -981.444\n",
      "episode:  468.0 alpha+beta:  8.914192\n",
      "episode: 468, actor_loss: 1.524, critic_loss: 19.762, mean_reward: -1226.442, best_return: -981.444\n",
      "episode:  469.0 alpha+beta:  9.290282\n",
      "episode: 469, actor_loss: 1.442, critic_loss: 21.098, mean_reward: -1172.774, best_return: -981.444\n",
      "episode:  470.0 alpha+beta:  9.302879\n",
      "episode: 470, actor_loss: 11.265, critic_loss: 25.984, mean_reward: -1289.928, best_return: -981.444\n",
      "episode:  471.0 alpha+beta:  15.977366\n",
      "episode: 471, actor_loss: -6.043, critic_loss: 34.435, mean_reward: -1187.198, best_return: -981.444\n",
      "episode:  472.0 alpha+beta:  19.04744\n",
      "episode: 472, actor_loss: -4.764, critic_loss: 71.410, mean_reward: -1158.178, best_return: -981.444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  473.0 alpha+beta:  9.180119\n",
      "episode: 473, actor_loss: -3.201, critic_loss: 26.841, mean_reward: -1256.707, best_return: -981.444\n",
      "episode:  474.0 alpha+beta:  10.500744\n",
      "episode: 474, actor_loss: 2.962, critic_loss: 36.060, mean_reward: -1126.789, best_return: -981.444\n",
      "episode:  475.0 alpha+beta:  9.526026\n",
      "episode: 475, actor_loss: -2.344, critic_loss: 17.247, mean_reward: -1102.908, best_return: -981.444\n",
      "episode:  476.0 alpha+beta:  10.46583\n",
      "episode: 476, actor_loss: -0.829, critic_loss: 19.387, mean_reward: -1259.163, best_return: -981.444\n",
      "episode:  477.0 alpha+beta:  10.576586\n",
      "episode: 477, actor_loss: 1.024, critic_loss: 16.366, mean_reward: -1123.997, best_return: -981.444\n",
      "episode:  478.0 alpha+beta:  10.4366665\n",
      "episode: 478, actor_loss: 0.577, critic_loss: 18.257, mean_reward: -1217.622, best_return: -981.444\n",
      "episode:  479.0 alpha+beta:  9.935036\n",
      "episode: 479, actor_loss: 0.263, critic_loss: 7.395, mean_reward: -1106.435, best_return: -981.444\n",
      "episode:  480.0 alpha+beta:  10.184955\n",
      "episode: 480, actor_loss: 12.308, critic_loss: 36.863, mean_reward: -1195.514, best_return: -981.444\n",
      "episode:  481.0 alpha+beta:  13.483793\n",
      "episode: 481, actor_loss: -12.430, critic_loss: 42.568, mean_reward: -1248.941, best_return: -981.444\n",
      "episode:  482.0 alpha+beta:  13.02277\n",
      "episode: 482, actor_loss: -3.011, critic_loss: 31.811, mean_reward: -1252.708, best_return: -981.444\n",
      "episode:  483.0 alpha+beta:  15.580075\n",
      "episode: 483, actor_loss: -4.192, critic_loss: 22.168, mean_reward: -1183.401, best_return: -981.444\n",
      "episode:  484.0 alpha+beta:  10.048014\n",
      "episode: 484, actor_loss: -4.009, critic_loss: 12.446, mean_reward: -1145.976, best_return: -981.444\n",
      "episode:  485.0 alpha+beta:  9.420179\n",
      "episode: 485, actor_loss: -0.720, critic_loss: 10.003, mean_reward: -1114.911, best_return: -981.444\n",
      "episode:  486.0 alpha+beta:  9.97123\n",
      "episode: 486, actor_loss: 1.777, critic_loss: 24.555, mean_reward: -1128.117, best_return: -981.444\n",
      "episode:  487.0 alpha+beta:  9.495435\n",
      "episode: 487, actor_loss: 1.616, critic_loss: 21.004, mean_reward: -1196.435, best_return: -981.444\n",
      "episode:  488.0 alpha+beta:  9.872626\n",
      "episode: 488, actor_loss: 1.396, critic_loss: 10.093, mean_reward: -1213.997, best_return: -981.444\n",
      "episode:  489.0 alpha+beta:  9.132317\n",
      "episode: 489, actor_loss: 1.074, critic_loss: 18.307, mean_reward: -1084.616, best_return: -981.444\n",
      "episode:  490.0 alpha+beta:  9.425377\n",
      "episode: 490, actor_loss: 9.912, critic_loss: 25.786, mean_reward: -1225.815, best_return: -981.444\n",
      "episode:  491.0 alpha+beta:  13.15947\n",
      "episode: 491, actor_loss: -8.268, critic_loss: 50.973, mean_reward: -1235.670, best_return: -981.444\n",
      "episode:  492.0 alpha+beta:  16.286331\n",
      "episode: 492, actor_loss: -3.566, critic_loss: 24.659, mean_reward: -1037.200, best_return: -981.444\n",
      "episode:  493.0 alpha+beta:  10.367128\n",
      "episode: 493, actor_loss: -4.250, critic_loss: 14.819, mean_reward: -1060.689, best_return: -981.444\n",
      "episode:  494.0 alpha+beta:  10.253998\n",
      "episode: 494, actor_loss: -1.933, critic_loss: 7.574, mean_reward: -1214.553, best_return: -981.444\n",
      "episode:  495.0 alpha+beta:  9.744255\n",
      "episode: 495, actor_loss: 1.911, critic_loss: 32.557, mean_reward: -1115.722, best_return: -981.444\n",
      "episode:  496.0 alpha+beta:  10.020461\n",
      "episode: 496, actor_loss: -1.338, critic_loss: 9.149, mean_reward: -1091.721, best_return: -981.444\n",
      "episode:  497.0 alpha+beta:  10.513071\n",
      "episode: 497, actor_loss: 2.065, critic_loss: 69.252, mean_reward: -1168.223, best_return: -981.444\n",
      "episode:  498.0 alpha+beta:  9.348961\n",
      "episode: 498, actor_loss: -1.211, critic_loss: 10.108, mean_reward: -1224.859, best_return: -981.444\n",
      "episode:  499.0 alpha+beta:  9.663872\n",
      "episode: 499, actor_loss: 1.823, critic_loss: 75.554, mean_reward: -1132.303, best_return: -981.444\n",
      "episode:  500.0 alpha+beta:  10.216713\n",
      "episode: 500, actor_loss: 10.058, critic_loss: 25.827, mean_reward: -1055.017, best_return: -981.444\n",
      "last 50 episode mean reward:  -1159.8434233660162\n",
      "\n",
      "\n",
      "episode:  501.0 alpha+beta:  11.88666\n",
      "episode: 501, actor_loss: -13.098, critic_loss: 76.516, mean_reward: -1096.770, best_return: -981.444\n",
      "episode:  502.0 alpha+beta:  15.078345\n",
      "episode: 502, actor_loss: -2.996, critic_loss: 51.755, mean_reward: -1163.464, best_return: -981.444\n",
      "episode:  503.0 alpha+beta:  13.404097\n",
      "episode: 503, actor_loss: -3.263, critic_loss: 33.630, mean_reward: -1126.915, best_return: -981.444\n",
      "episode:  504.0 alpha+beta:  15.277145\n",
      "episode: 504, actor_loss: -4.447, critic_loss: 25.671, mean_reward: -1122.616, best_return: -981.444\n",
      "episode:  505.0 alpha+beta:  8.894604\n",
      "episode: 505, actor_loss: -1.701, critic_loss: 13.002, mean_reward: -1038.433, best_return: -981.444\n",
      "episode:  506.0 alpha+beta:  10.137564\n",
      "episode: 506, actor_loss: 2.001, critic_loss: 78.344, mean_reward: -1264.140, best_return: -981.444\n",
      "episode:  507.0 alpha+beta:  8.496627\n",
      "episode: 507, actor_loss: -0.861, critic_loss: 11.464, mean_reward: -1191.438, best_return: -981.444\n",
      "episode:  508.0 alpha+beta:  9.341884\n",
      "episode: 508, actor_loss: 0.129, critic_loss: 15.884, mean_reward: -1082.165, best_return: -981.444\n",
      "episode:  509.0 alpha+beta:  9.263067\n",
      "episode: 509, actor_loss: 1.629, critic_loss: 27.191, mean_reward: -1274.186, best_return: -981.444\n",
      "episode:  510.0 alpha+beta:  8.866583\n",
      "episode: 510, actor_loss: 9.722, critic_loss: 27.678, mean_reward: -1170.059, best_return: -981.444\n",
      "episode:  511.0 alpha+beta:  12.891712\n",
      "episode: 511, actor_loss: -8.680, critic_loss: 51.261, mean_reward: -1160.272, best_return: -981.444\n",
      "episode:  512.0 alpha+beta:  10.587718\n",
      "episode: 512, actor_loss: -1.347, critic_loss: 18.180, mean_reward: -1179.162, best_return: -981.444\n",
      "episode:  513.0 alpha+beta:  11.358028\n",
      "episode: 513, actor_loss: 1.103, critic_loss: 79.985, mean_reward: -1113.633, best_return: -981.444\n",
      "episode:  514.0 alpha+beta:  8.4858885\n",
      "episode: 514, actor_loss: -3.999, critic_loss: 16.584, mean_reward: -1229.623, best_return: -981.444\n",
      "episode:  515.0 alpha+beta:  8.249213\n",
      "episode: 515, actor_loss: 0.383, critic_loss: 20.344, mean_reward: -1139.672, best_return: -981.444\n",
      "episode:  516.0 alpha+beta:  8.155586\n",
      "episode: 516, actor_loss: 0.116, critic_loss: 3.482, mean_reward: -1244.124, best_return: -981.444\n",
      "episode:  517.0 alpha+beta:  8.162843\n",
      "episode: 517, actor_loss: 0.869, critic_loss: 20.557, mean_reward: -1266.168, best_return: -981.444\n",
      "episode:  518.0 alpha+beta:  8.703818\n",
      "episode: 518, actor_loss: 0.750, critic_loss: 14.035, mean_reward: -1185.545, best_return: -981.444\n",
      "episode:  519.0 alpha+beta:  9.244395\n",
      "episode: 519, actor_loss: 0.832, critic_loss: 12.230, mean_reward: -1087.278, best_return: -981.444\n",
      "episode:  520.0 alpha+beta:  9.689218\n",
      "episode: 520, actor_loss: 8.955, critic_loss: 19.242, mean_reward: -1191.657, best_return: -981.444\n",
      "episode:  521.0 alpha+beta:  14.619877\n",
      "episode: 521, actor_loss: -7.269, critic_loss: 80.623, mean_reward: -1129.113, best_return: -981.444\n",
      "episode:  522.0 alpha+beta:  12.022232\n",
      "episode: 522, actor_loss: -2.405, critic_loss: 41.385, mean_reward: -1148.267, best_return: -981.444\n",
      "episode:  523.0 alpha+beta:  14.651361\n",
      "episode: 523, actor_loss: -4.506, critic_loss: 49.134, mean_reward: -1246.976, best_return: -981.444\n",
      "episode:  524.0 alpha+beta:  14.0280285\n",
      "episode: 524, actor_loss: -3.200, critic_loss: 31.719, mean_reward: -1170.604, best_return: -981.444\n",
      "episode:  525.0 alpha+beta:  10.054117\n",
      "episode: 525, actor_loss: -3.444, critic_loss: 17.769, mean_reward: -1199.147, best_return: -981.444\n",
      "episode:  526.0 alpha+beta:  15.019014\n",
      "episode: 526, actor_loss: -0.291, critic_loss: 13.096, mean_reward: -1170.721, best_return: -981.444\n",
      "episode:  527.0 alpha+beta:  10.033791\n",
      "episode: 527, actor_loss: -0.388, critic_loss: 10.754, mean_reward: -1123.845, best_return: -981.444\n",
      "episode:  528.0 alpha+beta:  10.816912\n",
      "episode: 528, actor_loss: -0.134, critic_loss: 14.810, mean_reward: -1139.893, best_return: -981.444\n",
      "episode:  529.0 alpha+beta:  11.694168\n",
      "episode: 529, actor_loss: -0.924, critic_loss: 20.277, mean_reward: -1251.706, best_return: -981.444\n",
      "episode:  530.0 alpha+beta:  10.517212\n",
      "episode: 530, actor_loss: 3.983, critic_loss: 49.838, mean_reward: -1136.229, best_return: -981.444\n",
      "episode:  531.0 alpha+beta:  14.2987385\n",
      "episode: 531, actor_loss: -8.980, critic_loss: 53.720, mean_reward: -1200.402, best_return: -981.444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  532.0 alpha+beta:  13.414271\n",
      "episode: 532, actor_loss: -6.507, critic_loss: 34.415, mean_reward: -1254.415, best_return: -981.444\n",
      "episode:  533.0 alpha+beta:  15.649731\n",
      "episode: 533, actor_loss: -4.681, critic_loss: 22.425, mean_reward: -1107.071, best_return: -981.444\n",
      "episode:  534.0 alpha+beta:  15.944957\n",
      "episode: 534, actor_loss: 6.478, critic_loss: 18.435, mean_reward: -1175.486, best_return: -981.444\n",
      "episode:  535.0 alpha+beta:  11.458169\n",
      "episode: 535, actor_loss: -1.461, critic_loss: 6.727, mean_reward: -1118.427, best_return: -981.444\n",
      "episode:  536.0 alpha+beta:  11.84318\n",
      "episode: 536, actor_loss: 1.485, critic_loss: 13.859, mean_reward: -1153.533, best_return: -981.444\n",
      "episode:  537.0 alpha+beta:  11.697023\n",
      "episode: 537, actor_loss: 2.869, critic_loss: 34.081, mean_reward: -1249.942, best_return: -981.444\n",
      "episode:  538.0 alpha+beta:  11.548382\n",
      "episode: 538, actor_loss: 2.537, critic_loss: 30.559, mean_reward: -1129.229, best_return: -981.444\n",
      "episode:  539.0 alpha+beta:  10.836513\n",
      "episode: 539, actor_loss: 2.567, critic_loss: 16.475, mean_reward: -1084.580, best_return: -981.444\n",
      "episode:  540.0 alpha+beta:  10.753923\n",
      "episode: 540, actor_loss: 18.828, critic_loss: 57.315, mean_reward: -1114.318, best_return: -981.444\n",
      "episode:  541.0 alpha+beta:  19.331657\n",
      "episode: 541, actor_loss: -9.748, critic_loss: 38.396, mean_reward: -1184.425, best_return: -981.444\n",
      "episode:  542.0 alpha+beta:  14.362213\n",
      "episode: 542, actor_loss: -1.856, critic_loss: 22.148, mean_reward: -1228.467, best_return: -981.444\n",
      "episode:  543.0 alpha+beta:  12.189205\n",
      "episode: 543, actor_loss: -3.663, critic_loss: 43.339, mean_reward: -1128.760, best_return: -981.444\n",
      "episode:  544.0 alpha+beta:  17.84066\n",
      "episode: 544, actor_loss: 3.731, critic_loss: 7.019, mean_reward: -1035.220, best_return: -981.444\n",
      "episode:  545.0 alpha+beta:  11.568786\n",
      "episode: 545, actor_loss: 0.171, critic_loss: 46.480, mean_reward: -1254.189, best_return: -981.444\n",
      "episode:  546.0 alpha+beta:  10.317118\n",
      "episode: 546, actor_loss: -1.621, critic_loss: 10.644, mean_reward: -1107.264, best_return: -981.444\n",
      "episode:  547.0 alpha+beta:  10.363743\n",
      "episode: 547, actor_loss: 0.923, critic_loss: 21.986, mean_reward: -1140.993, best_return: -981.444\n",
      "episode:  548.0 alpha+beta:  10.218699\n",
      "episode: 548, actor_loss: 0.908, critic_loss: 18.997, mean_reward: -1140.532, best_return: -981.444\n",
      "episode:  549.0 alpha+beta:  10.33333\n",
      "episode: 549, actor_loss: 0.936, critic_loss: 27.271, mean_reward: -1083.280, best_return: -981.444\n",
      "episode:  550.0 alpha+beta:  10.529996\n",
      "episode: 550, actor_loss: 10.136, critic_loss: 23.076, mean_reward: -1213.476, best_return: -981.444\n",
      "last 50 episode mean reward:  -1162.9565930194874\n",
      "\n",
      "\n",
      "episode:  551.0 alpha+beta:  20.028309\n",
      "episode: 551, actor_loss: -6.068, critic_loss: 26.852, mean_reward: -1076.349, best_return: -981.444\n",
      "episode:  552.0 alpha+beta:  16.327038\n",
      "episode: 552, actor_loss: -5.689, critic_loss: 26.741, mean_reward: -1162.803, best_return: -981.444\n",
      "episode:  553.0 alpha+beta:  17.36293\n",
      "episode: 553, actor_loss: 1.906, critic_loss: 7.466, mean_reward: -1137.280, best_return: -981.444\n",
      "episode:  554.0 alpha+beta:  10.445456\n",
      "episode: 554, actor_loss: -0.981, critic_loss: 14.567, mean_reward: -1058.392, best_return: -981.444\n",
      "episode:  555.0 alpha+beta:  9.669569\n",
      "episode: 555, actor_loss: 0.775, critic_loss: 35.547, mean_reward: -1023.152, best_return: -981.444\n",
      "episode:  556.0 alpha+beta:  9.587119\n",
      "episode: 556, actor_loss: 1.271, critic_loss: 30.180, mean_reward: -1040.107, best_return: -981.444\n",
      "episode:  557.0 alpha+beta:  10.062091\n",
      "episode: 557, actor_loss: 0.587, critic_loss: 15.583, mean_reward: -1079.869, best_return: -981.444\n",
      "episode:  558.0 alpha+beta:  10.038889\n",
      "episode: 558, actor_loss: 0.634, critic_loss: 11.445, mean_reward: -1324.180, best_return: -981.444\n",
      "episode:  559.0 alpha+beta:  10.971949\n",
      "episode: 559, actor_loss: 0.557, critic_loss: 6.424, mean_reward: -1108.465, best_return: -981.444\n",
      "episode:  560.0 alpha+beta:  11.383545\n",
      "episode: 560, actor_loss: 5.774, critic_loss: 17.112, mean_reward: -1186.060, best_return: -981.444\n",
      "episode:  561.0 alpha+beta:  19.155527\n",
      "episode: 561, actor_loss: -8.803, critic_loss: 30.954, mean_reward: -1014.218, best_return: -981.444\n",
      "episode:  562.0 alpha+beta:  16.4674\n",
      "episode: 562, actor_loss: 0.012, critic_loss: 28.927, mean_reward: -1151.996, best_return: -981.444\n",
      "episode:  563.0 alpha+beta:  17.116243\n",
      "episode: 563, actor_loss: -4.111, critic_loss: 27.887, mean_reward: -1137.429, best_return: -981.444\n",
      "episode:  564.0 alpha+beta:  14.903793\n",
      "episode: 564, actor_loss: -2.686, critic_loss: 13.152, mean_reward: -1126.932, best_return: -981.444\n",
      "episode:  565.0 alpha+beta:  12.702435\n",
      "episode: 565, actor_loss: 3.087, critic_loss: 28.102, mean_reward: -1120.907, best_return: -981.444\n",
      "episode:  566.0 alpha+beta:  12.6334095\n",
      "episode: 566, actor_loss: 0.074, critic_loss: 7.326, mean_reward: -1074.193, best_return: -981.444\n",
      "episode:  567.0 alpha+beta:  12.007704\n",
      "episode: 567, actor_loss: 0.916, critic_loss: 43.717, mean_reward: -1110.475, best_return: -981.444\n",
      "episode:  568.0 alpha+beta:  11.261206\n",
      "episode: 568, actor_loss: 0.879, critic_loss: 22.242, mean_reward: -1077.463, best_return: -981.444\n",
      "episode:  569.0 alpha+beta:  10.876682\n",
      "episode: 569, actor_loss: 0.495, critic_loss: 13.283, mean_reward: -1267.652, best_return: -981.444\n",
      "episode:  570.0 alpha+beta:  10.747295\n",
      "episode: 570, actor_loss: 7.508, critic_loss: 15.954, mean_reward: -1135.678, best_return: -981.444\n",
      "episode:  571.0 alpha+beta:  18.845467\n",
      "episode: 571, actor_loss: -8.310, critic_loss: 44.909, mean_reward: -1165.024, best_return: -981.444\n",
      "episode:  572.0 alpha+beta:  18.212975\n",
      "episode: 572, actor_loss: -4.782, critic_loss: 38.370, mean_reward: -1163.114, best_return: -981.444\n",
      "episode:  573.0 alpha+beta:  16.240906\n",
      "episode: 573, actor_loss: -6.157, critic_loss: 35.010, mean_reward: -1186.154, best_return: -981.444\n",
      "episode:  574.0 alpha+beta:  13.63224\n",
      "episode: 574, actor_loss: -1.108, critic_loss: 23.307, mean_reward: -1082.729, best_return: -981.444\n",
      "episode:  575.0 alpha+beta:  11.095078\n",
      "episode: 575, actor_loss: -3.595, critic_loss: 32.494, mean_reward: -1121.764, best_return: -981.444\n",
      "episode:  576.0 alpha+beta:  11.042465\n",
      "episode: 576, actor_loss: 1.094, critic_loss: 39.766, mean_reward: -1163.475, best_return: -981.444\n",
      "episode:  577.0 alpha+beta:  10.441259\n",
      "episode: 577, actor_loss: 1.261, critic_loss: 35.895, mean_reward: -1122.561, best_return: -981.444\n",
      "episode:  578.0 alpha+beta:  11.152363\n",
      "episode: 578, actor_loss: 1.083, critic_loss: 19.007, mean_reward: -1126.029, best_return: -981.444\n",
      "episode:  579.0 alpha+beta:  11.9377365\n",
      "episode: 579, actor_loss: 1.616, critic_loss: 11.957, mean_reward: -1240.519, best_return: -981.444\n",
      "episode:  580.0 alpha+beta:  12.265011\n",
      "episode: 580, actor_loss: 10.297, critic_loss: 23.135, mean_reward: -1016.411, best_return: -981.444\n",
      "episode:  581.0 alpha+beta:  20.382744\n",
      "episode: 581, actor_loss: -11.528, critic_loss: 43.896, mean_reward: -1158.491, best_return: -981.444\n",
      "episode:  582.0 alpha+beta:  18.03975\n",
      "episode: 582, actor_loss: -3.636, critic_loss: 15.941, mean_reward: -1063.041, best_return: -981.444\n",
      "episode:  583.0 alpha+beta:  17.065582\n",
      "episode: 583, actor_loss: 2.508, critic_loss: 38.473, mean_reward: -1082.068, best_return: -981.444\n",
      "episode:  584.0 alpha+beta:  15.069725\n",
      "episode: 584, actor_loss: -1.567, critic_loss: 14.475, mean_reward: -1145.438, best_return: -981.444\n",
      "episode:  585.0 alpha+beta:  12.419134\n",
      "episode: 585, actor_loss: 0.386, critic_loss: 5.618, mean_reward: -982.037, best_return: -981.444\n",
      "episode:  586.0 alpha+beta:  11.504929\n",
      "episode: 586, actor_loss: 1.055, critic_loss: 24.347, mean_reward: -957.428, best_return: -957.428\n",
      "episode:  587.0 alpha+beta:  15.18116\n",
      "episode: 587, actor_loss: 5.715, critic_loss: 16.632, mean_reward: -1305.025, best_return: -957.428\n",
      "episode:  588.0 alpha+beta:  11.486242\n",
      "episode: 588, actor_loss: 1.532, critic_loss: 16.291, mean_reward: -1185.080, best_return: -957.428\n",
      "episode:  589.0 alpha+beta:  11.136589\n",
      "episode: 589, actor_loss: 1.492, critic_loss: 19.395, mean_reward: -1314.089, best_return: -957.428\n",
      "episode:  590.0 alpha+beta:  11.687004\n",
      "episode: 590, actor_loss: 14.374, critic_loss: 42.285, mean_reward: -1186.006, best_return: -957.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  591.0 alpha+beta:  22.270668\n",
      "episode: 591, actor_loss: -11.572, critic_loss: 45.189, mean_reward: -1121.275, best_return: -957.428\n",
      "episode:  592.0 alpha+beta:  20.46231\n",
      "episode: 592, actor_loss: 0.877, critic_loss: 59.314, mean_reward: -1146.729, best_return: -957.428\n",
      "episode:  593.0 alpha+beta:  15.964819\n",
      "episode: 593, actor_loss: -4.360, critic_loss: 68.300, mean_reward: -1162.807, best_return: -957.428\n",
      "episode:  594.0 alpha+beta:  16.044075\n",
      "episode: 594, actor_loss: -2.412, critic_loss: 18.048, mean_reward: -1211.791, best_return: -957.428\n",
      "episode:  595.0 alpha+beta:  15.712444\n",
      "episode: 595, actor_loss: -0.730, critic_loss: 6.480, mean_reward: -1160.137, best_return: -957.428\n",
      "episode:  596.0 alpha+beta:  15.456858\n",
      "episode: 596, actor_loss: 5.573, critic_loss: 16.523, mean_reward: -1015.625, best_return: -957.428\n",
      "episode:  597.0 alpha+beta:  12.698062\n",
      "episode: 597, actor_loss: 0.679, critic_loss: 29.852, mean_reward: -1043.126, best_return: -957.428\n",
      "episode:  598.0 alpha+beta:  12.3137245\n",
      "episode: 598, actor_loss: 1.426, critic_loss: 14.651, mean_reward: -1152.383, best_return: -957.428\n",
      "episode:  599.0 alpha+beta:  12.630508\n",
      "episode: 599, actor_loss: 1.227, critic_loss: 12.193, mean_reward: -1154.718, best_return: -957.428\n",
      "episode:  600.0 alpha+beta:  12.4809\n",
      "episode: 600, actor_loss: 11.639, critic_loss: 31.513, mean_reward: -1110.231, best_return: -957.428\n",
      "last 50 episode mean reward:  -1129.1780846959557\n",
      "\n",
      "\n",
      "episode:  601.0 alpha+beta:  25.072424\n",
      "episode: 601, actor_loss: -8.467, critic_loss: 33.427, mean_reward: -1208.559, best_return: -957.428\n",
      "episode:  602.0 alpha+beta:  24.127445\n",
      "episode: 602, actor_loss: -2.491, critic_loss: 33.657, mean_reward: -1194.724, best_return: -957.428\n",
      "episode:  603.0 alpha+beta:  21.67996\n",
      "episode: 603, actor_loss: -6.806, critic_loss: 21.289, mean_reward: -1153.719, best_return: -957.428\n",
      "episode:  604.0 alpha+beta:  15.753666\n",
      "episode: 604, actor_loss: -2.278, critic_loss: 22.995, mean_reward: -1141.263, best_return: -957.428\n",
      "episode:  605.0 alpha+beta:  15.006485\n",
      "episode: 605, actor_loss: -1.711, critic_loss: 8.170, mean_reward: -1266.101, best_return: -957.428\n",
      "episode:  606.0 alpha+beta:  12.605824\n",
      "episode: 606, actor_loss: -0.155, critic_loss: 9.446, mean_reward: -1155.405, best_return: -957.428\n",
      "episode:  607.0 alpha+beta:  12.083136\n",
      "episode: 607, actor_loss: 1.243, critic_loss: 22.186, mean_reward: -1259.561, best_return: -957.428\n",
      "episode:  608.0 alpha+beta:  11.928815\n",
      "episode: 608, actor_loss: 1.132, critic_loss: 9.806, mean_reward: -1183.784, best_return: -957.428\n",
      "episode:  609.0 alpha+beta:  12.078611\n",
      "episode: 609, actor_loss: 0.945, critic_loss: 8.758, mean_reward: -1174.150, best_return: -957.428\n",
      "episode:  610.0 alpha+beta:  12.506423\n",
      "episode: 610, actor_loss: 9.531, critic_loss: 19.631, mean_reward: -1185.643, best_return: -957.428\n",
      "episode:  611.0 alpha+beta:  21.515223\n",
      "episode: 611, actor_loss: -8.379, critic_loss: 32.533, mean_reward: -1242.246, best_return: -957.428\n",
      "episode:  612.0 alpha+beta:  19.591177\n",
      "episode: 612, actor_loss: -5.849, critic_loss: 27.741, mean_reward: -1264.487, best_return: -957.428\n",
      "episode:  613.0 alpha+beta:  20.902708\n",
      "episode: 613, actor_loss: 2.497, critic_loss: 48.312, mean_reward: -1111.385, best_return: -957.428\n",
      "episode:  614.0 alpha+beta:  18.759546\n",
      "episode: 614, actor_loss: 1.484, critic_loss: 19.783, mean_reward: -1122.480, best_return: -957.428\n",
      "episode:  615.0 alpha+beta:  14.114475\n",
      "episode: 615, actor_loss: -1.653, critic_loss: 9.537, mean_reward: -1139.596, best_return: -957.428\n",
      "episode:  616.0 alpha+beta:  12.470549\n",
      "episode: 616, actor_loss: 0.615, critic_loss: 20.153, mean_reward: -1109.760, best_return: -957.428\n",
      "episode:  617.0 alpha+beta:  12.524069\n",
      "episode: 617, actor_loss: 1.123, critic_loss: 14.577, mean_reward: -1134.899, best_return: -957.428\n",
      "episode:  618.0 alpha+beta:  12.674166\n",
      "episode: 618, actor_loss: 1.175, critic_loss: 10.453, mean_reward: -1111.679, best_return: -957.428\n",
      "episode:  619.0 alpha+beta:  13.230811\n",
      "episode: 619, actor_loss: 1.271, critic_loss: 10.425, mean_reward: -1325.035, best_return: -957.428\n",
      "episode:  620.0 alpha+beta:  13.275746\n",
      "episode: 620, actor_loss: 11.353, critic_loss: 24.883, mean_reward: -1203.879, best_return: -957.428\n",
      "episode:  621.0 alpha+beta:  25.396912\n",
      "episode: 621, actor_loss: -8.151, critic_loss: 32.991, mean_reward: -1100.095, best_return: -957.428\n",
      "episode:  622.0 alpha+beta:  20.62864\n",
      "episode: 622, actor_loss: -7.194, critic_loss: 44.250, mean_reward: -1182.608, best_return: -957.428\n",
      "episode:  623.0 alpha+beta:  27.781788\n",
      "episode: 623, actor_loss: 1.787, critic_loss: 23.574, mean_reward: -1203.930, best_return: -957.428\n",
      "episode:  624.0 alpha+beta:  20.35815\n",
      "episode: 624, actor_loss: -3.541, critic_loss: 13.640, mean_reward: -1168.004, best_return: -957.428\n",
      "episode:  625.0 alpha+beta:  16.599693\n",
      "episode: 625, actor_loss: -2.918, critic_loss: 17.988, mean_reward: -1200.880, best_return: -957.428\n",
      "episode:  626.0 alpha+beta:  13.942814\n",
      "episode: 626, actor_loss: -1.575, critic_loss: 11.172, mean_reward: -1247.709, best_return: -957.428\n",
      "episode:  627.0 alpha+beta:  13.718872\n",
      "episode: 627, actor_loss: 0.095, critic_loss: 6.883, mean_reward: -1221.331, best_return: -957.428\n",
      "episode:  628.0 alpha+beta:  13.424161\n",
      "episode: 628, actor_loss: 0.902, critic_loss: 15.727, mean_reward: -1301.636, best_return: -957.428\n",
      "episode:  629.0 alpha+beta:  13.562115\n",
      "episode: 629, actor_loss: 0.841, critic_loss: 15.024, mean_reward: -1090.374, best_return: -957.428\n",
      "episode:  630.0 alpha+beta:  13.6875725\n",
      "episode: 630, actor_loss: 10.561, critic_loss: 27.114, mean_reward: -1176.672, best_return: -957.428\n",
      "episode:  631.0 alpha+beta:  24.008514\n",
      "episode: 631, actor_loss: -8.379, critic_loss: 42.751, mean_reward: -1230.425, best_return: -957.428\n",
      "episode:  632.0 alpha+beta:  21.235502\n",
      "episode: 632, actor_loss: -2.172, critic_loss: 13.798, mean_reward: -1211.885, best_return: -957.428\n",
      "episode:  633.0 alpha+beta:  25.761276\n",
      "episode: 633, actor_loss: -0.625, critic_loss: 9.436, mean_reward: -1161.210, best_return: -957.428\n",
      "episode:  634.0 alpha+beta:  19.86546\n",
      "episode: 634, actor_loss: -4.215, critic_loss: 16.940, mean_reward: -1014.589, best_return: -957.428\n",
      "episode:  635.0 alpha+beta:  15.611191\n",
      "episode: 635, actor_loss: -6.249, critic_loss: 12.117, mean_reward: -1227.037, best_return: -957.428\n",
      "episode:  636.0 alpha+beta:  14.950111\n",
      "episode: 636, actor_loss: -0.190, critic_loss: 19.411, mean_reward: -1215.298, best_return: -957.428\n",
      "episode:  637.0 alpha+beta:  14.232252\n",
      "episode: 637, actor_loss: 0.990, critic_loss: 31.856, mean_reward: -1257.301, best_return: -957.428\n",
      "episode:  638.0 alpha+beta:  13.374874\n",
      "episode: 638, actor_loss: 1.321, critic_loss: 15.136, mean_reward: -1181.540, best_return: -957.428\n",
      "episode:  639.0 alpha+beta:  14.386755\n",
      "episode: 639, actor_loss: 1.158, critic_loss: 11.368, mean_reward: -1140.281, best_return: -957.428\n",
      "episode:  640.0 alpha+beta:  14.972752\n",
      "episode: 640, actor_loss: 8.628, critic_loss: 19.895, mean_reward: -1154.849, best_return: -957.428\n",
      "episode:  641.0 alpha+beta:  25.349686\n",
      "episode: 641, actor_loss: -3.719, critic_loss: 20.953, mean_reward: -1177.279, best_return: -957.428\n",
      "episode:  642.0 alpha+beta:  21.997313\n",
      "episode: 642, actor_loss: -3.723, critic_loss: 23.531, mean_reward: -1201.206, best_return: -957.428\n",
      "episode:  643.0 alpha+beta:  28.29996\n",
      "episode: 643, actor_loss: 1.800, critic_loss: 11.960, mean_reward: -1141.290, best_return: -957.428\n",
      "episode:  644.0 alpha+beta:  18.527157\n",
      "episode: 644, actor_loss: -0.510, critic_loss: 5.349, mean_reward: -1108.317, best_return: -957.428\n",
      "episode:  645.0 alpha+beta:  16.432972\n",
      "episode: 645, actor_loss: -2.860, critic_loss: 10.290, mean_reward: -1273.152, best_return: -957.428\n",
      "episode:  646.0 alpha+beta:  16.618801\n",
      "episode: 646, actor_loss: 1.255, critic_loss: 17.679, mean_reward: -1155.892, best_return: -957.428\n",
      "episode:  647.0 alpha+beta:  13.78801\n",
      "episode: 647, actor_loss: 0.427, critic_loss: 16.193, mean_reward: -1188.851, best_return: -957.428\n",
      "episode:  648.0 alpha+beta:  13.696011\n",
      "episode: 648, actor_loss: 0.636, critic_loss: 14.385, mean_reward: -1117.953, best_return: -957.428\n",
      "episode:  649.0 alpha+beta:  14.720856\n",
      "episode: 649, actor_loss: 0.579, critic_loss: 11.085, mean_reward: -1247.160, best_return: -957.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  650.0 alpha+beta:  15.814585\n",
      "episode: 650, actor_loss: 4.480, critic_loss: 13.831, mean_reward: -1176.114, best_return: -957.428\n",
      "last 50 episode mean reward:  -1183.2644470071014\n",
      "\n",
      "\n",
      "episode:  651.0 alpha+beta:  22.559101\n",
      "episode: 651, actor_loss: -5.583, critic_loss: 16.492, mean_reward: -1199.067, best_return: -957.428\n",
      "episode:  652.0 alpha+beta:  22.358765\n",
      "episode: 652, actor_loss: -0.500, critic_loss: 15.860, mean_reward: -1128.276, best_return: -957.428\n",
      "episode:  653.0 alpha+beta:  27.400414\n",
      "episode: 653, actor_loss: -0.404, critic_loss: 21.623, mean_reward: -1199.930, best_return: -957.428\n",
      "episode:  654.0 alpha+beta:  19.928577\n",
      "episode: 654, actor_loss: 3.485, critic_loss: 11.688, mean_reward: -1113.746, best_return: -957.428\n",
      "episode:  655.0 alpha+beta:  17.95264\n",
      "episode: 655, actor_loss: -3.303, critic_loss: 6.994, mean_reward: -1086.141, best_return: -957.428\n",
      "episode:  656.0 alpha+beta:  17.575153\n",
      "episode: 656, actor_loss: -0.068, critic_loss: 15.317, mean_reward: -1312.677, best_return: -957.428\n",
      "episode:  657.0 alpha+beta:  15.856392\n",
      "episode: 657, actor_loss: -1.211, critic_loss: 8.633, mean_reward: -1132.671, best_return: -957.428\n",
      "episode:  658.0 alpha+beta:  15.402096\n",
      "episode: 658, actor_loss: -0.893, critic_loss: 17.986, mean_reward: -1101.554, best_return: -957.428\n",
      "episode:  659.0 alpha+beta:  15.8908825\n",
      "episode: 659, actor_loss: 0.926, critic_loss: 17.371, mean_reward: -1029.996, best_return: -957.428\n",
      "episode:  660.0 alpha+beta:  16.469814\n",
      "episode: 660, actor_loss: 6.711, critic_loss: 17.496, mean_reward: -992.333, best_return: -957.428\n",
      "episode:  661.0 alpha+beta:  26.946201\n",
      "episode: 661, actor_loss: -7.183, critic_loss: 33.779, mean_reward: -1249.840, best_return: -957.428\n",
      "episode:  662.0 alpha+beta:  20.887331\n",
      "episode: 662, actor_loss: -2.059, critic_loss: 33.648, mean_reward: -1128.693, best_return: -957.428\n",
      "episode:  663.0 alpha+beta:  27.031681\n",
      "episode: 663, actor_loss: 4.922, critic_loss: 47.990, mean_reward: -1258.189, best_return: -957.428\n",
      "episode:  664.0 alpha+beta:  19.855083\n",
      "episode: 664, actor_loss: -4.152, critic_loss: 17.382, mean_reward: -1135.037, best_return: -957.428\n",
      "episode:  665.0 alpha+beta:  18.38583\n",
      "episode: 665, actor_loss: 1.522, critic_loss: 13.709, mean_reward: -1114.039, best_return: -957.428\n",
      "episode:  666.0 alpha+beta:  17.258106\n",
      "episode: 666, actor_loss: -0.694, critic_loss: 6.378, mean_reward: -1186.259, best_return: -957.428\n",
      "episode:  667.0 alpha+beta:  17.333675\n",
      "episode: 667, actor_loss: 0.493, critic_loss: 27.404, mean_reward: -1248.731, best_return: -957.428\n",
      "episode:  668.0 alpha+beta:  17.415287\n",
      "episode: 668, actor_loss: 0.499, critic_loss: 24.275, mean_reward: -1013.790, best_return: -957.428\n",
      "episode:  669.0 alpha+beta:  17.513514\n",
      "episode: 669, actor_loss: 1.281, critic_loss: 17.769, mean_reward: -1110.662, best_return: -957.428\n",
      "episode:  670.0 alpha+beta:  17.328787\n",
      "episode: 670, actor_loss: 7.059, critic_loss: 19.742, mean_reward: -1120.338, best_return: -957.428\n",
      "episode:  671.0 alpha+beta:  27.456362\n",
      "episode: 671, actor_loss: -3.971, critic_loss: 36.148, mean_reward: -1071.853, best_return: -957.428\n",
      "episode:  672.0 alpha+beta:  24.212868\n",
      "episode: 672, actor_loss: -1.550, critic_loss: 18.565, mean_reward: -978.866, best_return: -957.428\n",
      "episode:  673.0 alpha+beta:  19.521166\n",
      "episode: 673, actor_loss: -2.536, critic_loss: 12.401, mean_reward: -1158.753, best_return: -957.428\n",
      "episode:  674.0 alpha+beta:  20.116985\n",
      "episode: 674, actor_loss: 4.982, critic_loss: 19.932, mean_reward: -1264.723, best_return: -957.428\n",
      "episode:  675.0 alpha+beta:  17.873642\n",
      "episode: 675, actor_loss: -0.837, critic_loss: 5.137, mean_reward: -1178.054, best_return: -957.428\n",
      "episode:  676.0 alpha+beta:  18.459063\n",
      "episode: 676, actor_loss: 0.306, critic_loss: 30.639, mean_reward: -1185.144, best_return: -957.428\n",
      "episode:  677.0 alpha+beta:  17.429588\n",
      "episode: 677, actor_loss: 0.761, critic_loss: 20.208, mean_reward: -1111.135, best_return: -957.428\n",
      "episode:  678.0 alpha+beta:  18.244411\n",
      "episode: 678, actor_loss: 0.281, critic_loss: 15.053, mean_reward: -1102.289, best_return: -957.428\n",
      "episode:  679.0 alpha+beta:  19.344501\n",
      "episode: 679, actor_loss: 0.929, critic_loss: 14.712, mean_reward: -1262.210, best_return: -957.428\n",
      "episode:  680.0 alpha+beta:  18.52912\n",
      "episode: 680, actor_loss: 5.762, critic_loss: 14.151, mean_reward: -1158.419, best_return: -957.428\n",
      "episode:  681.0 alpha+beta:  27.08957\n",
      "episode: 681, actor_loss: -6.835, critic_loss: 35.756, mean_reward: -1053.311, best_return: -957.428\n",
      "episode:  682.0 alpha+beta:  25.07929\n",
      "episode: 682, actor_loss: -7.433, critic_loss: 31.972, mean_reward: -908.498, best_return: -908.498\n",
      "episode:  683.0 alpha+beta:  25.945988\n",
      "episode: 683, actor_loss: 1.637, critic_loss: 82.447, mean_reward: -1104.835, best_return: -908.498\n",
      "episode:  684.0 alpha+beta:  23.583565\n",
      "episode: 684, actor_loss: -5.543, critic_loss: 21.339, mean_reward: -1190.806, best_return: -908.498\n",
      "episode:  685.0 alpha+beta:  22.767162\n",
      "episode: 685, actor_loss: -1.436, critic_loss: 12.563, mean_reward: -1149.618, best_return: -908.498\n",
      "episode:  686.0 alpha+beta:  22.018055\n",
      "episode: 686, actor_loss: 0.322, critic_loss: 6.690, mean_reward: -1277.343, best_return: -908.498\n",
      "episode:  687.0 alpha+beta:  21.040611\n",
      "episode: 687, actor_loss: 2.096, critic_loss: 36.354, mean_reward: -1146.180, best_return: -908.498\n",
      "episode:  688.0 alpha+beta:  20.488567\n",
      "episode: 688, actor_loss: 1.773, critic_loss: 32.235, mean_reward: -1269.667, best_return: -908.498\n",
      "episode:  689.0 alpha+beta:  19.437098\n",
      "episode: 689, actor_loss: 2.225, critic_loss: 23.185, mean_reward: -1193.293, best_return: -908.498\n",
      "episode:  690.0 alpha+beta:  19.967846\n",
      "episode: 690, actor_loss: 17.558, critic_loss: 54.140, mean_reward: -1140.861, best_return: -908.498\n",
      "episode:  691.0 alpha+beta:  36.18667\n",
      "episode: 691, actor_loss: -10.330, critic_loss: 39.596, mean_reward: -1080.322, best_return: -908.498\n",
      "episode:  692.0 alpha+beta:  27.566578\n",
      "episode: 692, actor_loss: -3.245, critic_loss: 33.160, mean_reward: -1332.236, best_return: -908.498\n",
      "episode:  693.0 alpha+beta:  34.289944\n",
      "episode: 693, actor_loss: -4.957, critic_loss: 29.133, mean_reward: -1040.334, best_return: -908.498\n",
      "episode:  694.0 alpha+beta:  32.061504\n",
      "episode: 694, actor_loss: 0.387, critic_loss: 52.140, mean_reward: -1207.514, best_return: -908.498\n",
      "episode:  695.0 alpha+beta:  22.833544\n",
      "episode: 695, actor_loss: 0.353, critic_loss: 63.436, mean_reward: -1194.590, best_return: -908.498\n",
      "episode:  696.0 alpha+beta:  19.745358\n",
      "episode: 696, actor_loss: -2.160, critic_loss: 11.006, mean_reward: -1049.805, best_return: -908.498\n",
      "episode:  697.0 alpha+beta:  18.313011\n",
      "episode: 697, actor_loss: 0.246, critic_loss: 18.957, mean_reward: -1039.628, best_return: -908.498\n",
      "episode:  698.0 alpha+beta:  18.419067\n",
      "episode: 698, actor_loss: 1.369, critic_loss: 26.305, mean_reward: -1111.275, best_return: -908.498\n",
      "episode:  699.0 alpha+beta:  18.155476\n",
      "episode: 699, actor_loss: 1.741, critic_loss: 17.451, mean_reward: -1257.008, best_return: -908.498\n",
      "episode:  700.0 alpha+beta:  18.49734\n",
      "episode: 700, actor_loss: 12.972, critic_loss: 32.013, mean_reward: -1053.253, best_return: -908.498\n",
      "last 50 episode mean reward:  -1142.675855351685\n",
      "\n",
      "\n",
      "episode:  701.0 alpha+beta:  30.086105\n",
      "episode: 701, actor_loss: -4.556, critic_loss: 27.929, mean_reward: -1069.690, best_return: -908.498\n",
      "episode:  702.0 alpha+beta:  31.26014\n",
      "episode: 702, actor_loss: -1.836, critic_loss: 21.527, mean_reward: -1120.920, best_return: -908.498\n",
      "episode:  703.0 alpha+beta:  29.145678\n",
      "episode: 703, actor_loss: 1.586, critic_loss: 37.312, mean_reward: -1101.500, best_return: -908.498\n",
      "episode:  704.0 alpha+beta:  27.366425\n",
      "episode: 704, actor_loss: 4.360, critic_loss: 23.891, mean_reward: -1218.852, best_return: -908.498\n",
      "episode:  705.0 alpha+beta:  27.410698\n",
      "episode: 705, actor_loss: -0.258, critic_loss: 11.994, mean_reward: -1245.488, best_return: -908.498\n",
      "episode:  706.0 alpha+beta:  23.737514\n",
      "episode: 706, actor_loss: 5.490, critic_loss: 74.435, mean_reward: -1104.489, best_return: -908.498\n",
      "episode:  707.0 alpha+beta:  18.939735\n",
      "episode: 707, actor_loss: -1.610, critic_loss: 5.007, mean_reward: -1146.961, best_return: -908.498\n",
      "episode:  708.0 alpha+beta:  21.19133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 708, actor_loss: 1.914, critic_loss: 13.192, mean_reward: -1145.912, best_return: -908.498\n",
      "episode:  709.0 alpha+beta:  17.96331\n",
      "episode: 709, actor_loss: 1.714, critic_loss: 43.857, mean_reward: -1098.479, best_return: -908.498\n",
      "episode:  710.0 alpha+beta:  17.692371\n",
      "episode: 710, actor_loss: 10.852, critic_loss: 41.971, mean_reward: -1210.311, best_return: -908.498\n",
      "episode:  711.0 alpha+beta:  34.77404\n",
      "episode: 711, actor_loss: -4.062, critic_loss: 46.158, mean_reward: -1167.091, best_return: -908.498\n",
      "episode:  712.0 alpha+beta:  27.026506\n",
      "episode: 712, actor_loss: -8.216, critic_loss: 65.815, mean_reward: -1278.975, best_return: -908.498\n",
      "episode:  713.0 alpha+beta:  27.38565\n",
      "episode: 713, actor_loss: -4.373, critic_loss: 25.620, mean_reward: -1256.376, best_return: -908.498\n",
      "episode:  714.0 alpha+beta:  31.008533\n",
      "episode: 714, actor_loss: 1.297, critic_loss: 17.189, mean_reward: -1067.786, best_return: -908.498\n",
      "episode:  715.0 alpha+beta:  20.472092\n",
      "episode: 715, actor_loss: 0.037, critic_loss: 34.119, mean_reward: -1107.909, best_return: -908.498\n",
      "episode:  716.0 alpha+beta:  18.461786\n",
      "episode: 716, actor_loss: -1.341, critic_loss: 6.154, mean_reward: -1266.692, best_return: -908.498\n",
      "episode:  717.0 alpha+beta:  18.27426\n",
      "episode: 717, actor_loss: 1.604, critic_loss: 16.449, mean_reward: -1146.963, best_return: -908.498\n",
      "episode:  718.0 alpha+beta:  18.049137\n",
      "episode: 718, actor_loss: 1.398, critic_loss: 46.735, mean_reward: -1107.030, best_return: -908.498\n",
      "episode:  719.0 alpha+beta:  17.777313\n",
      "episode: 719, actor_loss: 1.631, critic_loss: 31.369, mean_reward: -1114.980, best_return: -908.498\n",
      "episode:  720.0 alpha+beta:  18.741566\n",
      "episode: 720, actor_loss: 13.555, critic_loss: 37.635, mean_reward: -1249.644, best_return: -908.498\n",
      "episode:  721.0 alpha+beta:  33.95374\n",
      "episode: 721, actor_loss: -7.662, critic_loss: 35.806, mean_reward: -1101.283, best_return: -908.498\n",
      "episode:  722.0 alpha+beta:  36.10139\n",
      "episode: 722, actor_loss: -5.682, critic_loss: 39.664, mean_reward: -1217.287, best_return: -908.498\n",
      "episode:  723.0 alpha+beta:  29.673464\n",
      "episode: 723, actor_loss: -0.068, critic_loss: 123.712, mean_reward: -1087.395, best_return: -908.498\n",
      "episode:  724.0 alpha+beta:  32.71247\n",
      "episode: 724, actor_loss: -2.915, critic_loss: 47.971, mean_reward: -1205.659, best_return: -908.498\n",
      "episode:  725.0 alpha+beta:  27.26743\n",
      "episode: 725, actor_loss: -0.762, critic_loss: 35.827, mean_reward: -1032.258, best_return: -908.498\n",
      "episode:  726.0 alpha+beta:  25.5786\n",
      "episode: 726, actor_loss: 0.252, critic_loss: 12.950, mean_reward: -987.104, best_return: -908.498\n",
      "episode:  727.0 alpha+beta:  19.450226\n",
      "episode: 727, actor_loss: -1.482, critic_loss: 8.866, mean_reward: -1129.874, best_return: -908.498\n",
      "episode:  728.0 alpha+beta:  18.448435\n",
      "episode: 728, actor_loss: 0.270, critic_loss: 19.412, mean_reward: -1133.945, best_return: -908.498\n",
      "episode:  729.0 alpha+beta:  18.676653\n",
      "episode: 729, actor_loss: 1.106, critic_loss: 36.034, mean_reward: -1010.377, best_return: -908.498\n",
      "episode:  730.0 alpha+beta:  18.766056\n",
      "episode: 730, actor_loss: 11.125, critic_loss: 21.686, mean_reward: -1055.173, best_return: -908.498\n",
      "episode:  731.0 alpha+beta:  36.15412\n",
      "episode: 731, actor_loss: 3.041, critic_loss: 60.765, mean_reward: -1210.528, best_return: -908.498\n",
      "episode:  732.0 alpha+beta:  27.421476\n",
      "episode: 732, actor_loss: -13.795, critic_loss: 74.158, mean_reward: -1102.061, best_return: -908.498\n",
      "episode:  733.0 alpha+beta:  29.727848\n",
      "episode: 733, actor_loss: 6.201, critic_loss: 57.120, mean_reward: -1219.883, best_return: -908.498\n",
      "episode:  734.0 alpha+beta:  23.192028\n",
      "episode: 734, actor_loss: -0.849, critic_loss: 24.343, mean_reward: -1073.329, best_return: -908.498\n",
      "episode:  735.0 alpha+beta:  27.538374\n",
      "episode: 735, actor_loss: 7.504, critic_loss: 13.992, mean_reward: -1085.914, best_return: -908.498\n",
      "episode:  736.0 alpha+beta:  20.499575\n",
      "episode: 736, actor_loss: -1.258, critic_loss: 5.643, mean_reward: -1216.126, best_return: -908.498\n",
      "episode:  737.0 alpha+beta:  19.14421\n",
      "episode: 737, actor_loss: 0.118, critic_loss: 19.277, mean_reward: -1296.124, best_return: -908.498\n",
      "episode:  738.0 alpha+beta:  18.7411\n",
      "episode: 738, actor_loss: 0.656, critic_loss: 11.852, mean_reward: -1197.157, best_return: -908.498\n",
      "episode:  739.0 alpha+beta:  18.89383\n",
      "episode: 739, actor_loss: 0.154, critic_loss: 26.391, mean_reward: -1192.219, best_return: -908.498\n",
      "episode:  740.0 alpha+beta:  19.84473\n",
      "episode: 740, actor_loss: 4.082, critic_loss: 28.000, mean_reward: -1067.131, best_return: -908.498\n",
      "episode:  741.0 alpha+beta:  33.39465\n",
      "episode: 741, actor_loss: -2.407, critic_loss: 29.705, mean_reward: -996.255, best_return: -908.498\n",
      "episode:  742.0 alpha+beta:  35.149815\n",
      "episode: 742, actor_loss: -0.644, critic_loss: 28.887, mean_reward: -1138.421, best_return: -908.498\n",
      "episode:  743.0 alpha+beta:  33.89696\n",
      "episode: 743, actor_loss: -0.433, critic_loss: 43.508, mean_reward: -1234.897, best_return: -908.498\n",
      "episode:  744.0 alpha+beta:  27.61429\n",
      "episode: 744, actor_loss: -0.026, critic_loss: 17.852, mean_reward: -1222.224, best_return: -908.498\n",
      "episode:  745.0 alpha+beta:  28.47157\n",
      "episode: 745, actor_loss: -4.094, critic_loss: 18.805, mean_reward: -1086.090, best_return: -908.498\n",
      "episode:  746.0 alpha+beta:  22.161879\n",
      "episode: 746, actor_loss: -2.302, critic_loss: 25.809, mean_reward: -1203.153, best_return: -908.498\n",
      "episode:  747.0 alpha+beta:  20.118114\n",
      "episode: 747, actor_loss: 0.726, critic_loss: 36.741, mean_reward: -976.241, best_return: -908.498\n",
      "episode:  748.0 alpha+beta:  18.839336\n",
      "episode: 748, actor_loss: 0.757, critic_loss: 14.866, mean_reward: -1176.446, best_return: -908.498\n",
      "episode:  749.0 alpha+beta:  18.29566\n",
      "episode: 749, actor_loss: 0.932, critic_loss: 10.644, mean_reward: -1050.081, best_return: -908.498\n",
      "episode:  750.0 alpha+beta:  18.420399\n",
      "episode: 750, actor_loss: 8.715, critic_loss: 21.544, mean_reward: -1015.760, best_return: -908.498\n",
      "last 50 episode mean reward:  -1138.9288632746104\n",
      "\n",
      "\n",
      "episode:  751.0 alpha+beta:  31.878134\n",
      "episode: 751, actor_loss: -5.500, critic_loss: 24.697, mean_reward: -1096.966, best_return: -908.498\n",
      "episode:  752.0 alpha+beta:  31.695232\n",
      "episode: 752, actor_loss: -3.423, critic_loss: 35.930, mean_reward: -1159.753, best_return: -908.498\n",
      "episode:  753.0 alpha+beta:  26.163359\n",
      "episode: 753, actor_loss: 7.324, critic_loss: 23.991, mean_reward: -1201.304, best_return: -908.498\n",
      "episode:  754.0 alpha+beta:  19.420094\n",
      "episode: 754, actor_loss: -4.674, critic_loss: 16.855, mean_reward: -1284.409, best_return: -908.498\n",
      "episode:  755.0 alpha+beta:  21.9881\n",
      "episode: 755, actor_loss: 1.450, critic_loss: 34.391, mean_reward: -1189.214, best_return: -908.498\n",
      "episode:  756.0 alpha+beta:  19.530369\n",
      "episode: 756, actor_loss: 0.133, critic_loss: 40.149, mean_reward: -1112.314, best_return: -908.498\n",
      "episode:  757.0 alpha+beta:  18.849665\n",
      "episode: 757, actor_loss: 0.285, critic_loss: 38.655, mean_reward: -1128.217, best_return: -908.498\n",
      "episode:  758.0 alpha+beta:  18.532448\n",
      "episode: 758, actor_loss: 0.450, critic_loss: 16.424, mean_reward: -1065.160, best_return: -908.498\n",
      "episode:  759.0 alpha+beta:  18.827225\n",
      "episode: 759, actor_loss: 0.570, critic_loss: 14.754, mean_reward: -1168.855, best_return: -908.498\n",
      "episode:  760.0 alpha+beta:  19.055428\n",
      "episode: 760, actor_loss: 5.570, critic_loss: 20.821, mean_reward: -1024.280, best_return: -908.498\n",
      "episode:  761.0 alpha+beta:  41.16552\n",
      "episode: 761, actor_loss: -9.548, critic_loss: 35.114, mean_reward: -1190.431, best_return: -908.498\n",
      "episode:  762.0 alpha+beta:  34.035732\n",
      "episode: 762, actor_loss: -5.923, critic_loss: 49.389, mean_reward: -1195.108, best_return: -908.498\n",
      "episode:  763.0 alpha+beta:  31.48878\n",
      "episode: 763, actor_loss: -6.563, critic_loss: 41.805, mean_reward: -1043.338, best_return: -908.498\n",
      "episode:  764.0 alpha+beta:  28.531301\n",
      "episode: 764, actor_loss: -0.678, critic_loss: 19.162, mean_reward: -1242.987, best_return: -908.498\n",
      "episode:  765.0 alpha+beta:  24.000988\n",
      "episode: 765, actor_loss: -1.730, critic_loss: 12.117, mean_reward: -1172.516, best_return: -908.498\n",
      "episode:  766.0 alpha+beta:  22.609612\n",
      "episode: 766, actor_loss: 0.799, critic_loss: 23.112, mean_reward: -1188.330, best_return: -908.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  767.0 alpha+beta:  24.90226\n",
      "episode: 767, actor_loss: 2.210, critic_loss: 33.904, mean_reward: -1094.055, best_return: -908.498\n",
      "episode:  768.0 alpha+beta:  19.789719\n",
      "episode: 768, actor_loss: -1.469, critic_loss: 12.928, mean_reward: -1126.874, best_return: -908.498\n",
      "episode:  769.0 alpha+beta:  23.021542\n",
      "episode: 769, actor_loss: -0.181, critic_loss: 26.517, mean_reward: -1086.328, best_return: -908.498\n",
      "episode:  770.0 alpha+beta:  22.982319\n",
      "episode: 770, actor_loss: 6.945, critic_loss: 88.914, mean_reward: -1166.875, best_return: -908.498\n",
      "episode:  771.0 alpha+beta:  38.17476\n",
      "episode: 771, actor_loss: -9.703, critic_loss: 49.569, mean_reward: -1155.830, best_return: -908.498\n",
      "episode:  772.0 alpha+beta:  34.516563\n",
      "episode: 772, actor_loss: -2.557, critic_loss: 45.759, mean_reward: -1229.513, best_return: -908.498\n",
      "episode:  773.0 alpha+beta:  28.269468\n",
      "episode: 773, actor_loss: -3.263, critic_loss: 51.604, mean_reward: -1093.719, best_return: -908.498\n",
      "episode:  774.0 alpha+beta:  23.105537\n",
      "episode: 774, actor_loss: -2.973, critic_loss: 18.868, mean_reward: -1215.220, best_return: -908.498\n",
      "episode:  775.0 alpha+beta:  21.638855\n",
      "episode: 775, actor_loss: 0.987, critic_loss: 29.048, mean_reward: -1196.365, best_return: -908.498\n",
      "episode:  776.0 alpha+beta:  20.858696\n",
      "episode: 776, actor_loss: 1.287, critic_loss: 47.330, mean_reward: -1088.473, best_return: -908.498\n",
      "episode:  777.0 alpha+beta:  20.370775\n",
      "episode: 777, actor_loss: 1.192, critic_loss: 43.406, mean_reward: -1170.033, best_return: -908.498\n",
      "episode:  778.0 alpha+beta:  20.942669\n",
      "episode: 778, actor_loss: 0.898, critic_loss: 27.447, mean_reward: -1134.517, best_return: -908.498\n",
      "episode:  779.0 alpha+beta:  20.837645\n",
      "episode: 779, actor_loss: 1.399, critic_loss: 23.775, mean_reward: -1097.248, best_return: -908.498\n",
      "episode:  780.0 alpha+beta:  21.629278\n",
      "episode: 780, actor_loss: 9.967, critic_loss: 27.316, mean_reward: -1102.457, best_return: -908.498\n",
      "episode:  781.0 alpha+beta:  43.43786\n",
      "episode: 781, actor_loss: -8.623, critic_loss: 30.577, mean_reward: -1198.133, best_return: -908.498\n",
      "episode:  782.0 alpha+beta:  37.025593\n",
      "episode: 782, actor_loss: -4.100, critic_loss: 33.000, mean_reward: -1130.024, best_return: -908.498\n",
      "episode:  783.0 alpha+beta:  35.24209\n",
      "episode: 783, actor_loss: 2.188, critic_loss: 24.718, mean_reward: -1216.228, best_return: -908.498\n",
      "episode:  784.0 alpha+beta:  30.657124\n",
      "episode: 784, actor_loss: -1.304, critic_loss: 11.349, mean_reward: -1107.238, best_return: -908.498\n",
      "episode:  785.0 alpha+beta:  21.312767\n",
      "episode: 785, actor_loss: 6.596, critic_loss: 63.059, mean_reward: -1128.682, best_return: -908.498\n",
      "episode:  786.0 alpha+beta:  23.192148\n",
      "episode: 786, actor_loss: -1.068, critic_loss: 11.220, mean_reward: -1087.507, best_return: -908.498\n",
      "episode:  787.0 alpha+beta:  22.824326\n",
      "episode: 787, actor_loss: 0.775, critic_loss: 41.668, mean_reward: -1195.598, best_return: -908.498\n",
      "episode:  788.0 alpha+beta:  23.533978\n",
      "episode: 788, actor_loss: 0.682, critic_loss: 16.533, mean_reward: -1243.315, best_return: -908.498\n",
      "episode:  789.0 alpha+beta:  23.115463\n",
      "episode: 789, actor_loss: 0.432, critic_loss: 8.522, mean_reward: -1238.126, best_return: -908.498\n",
      "episode:  790.0 alpha+beta:  23.399994\n",
      "episode: 790, actor_loss: 3.953, critic_loss: 110.820, mean_reward: -1079.997, best_return: -908.498\n",
      "episode:  791.0 alpha+beta:  43.872864\n",
      "episode: 791, actor_loss: -3.870, critic_loss: 118.882, mean_reward: -1156.442, best_return: -908.498\n",
      "episode:  792.0 alpha+beta:  37.59997\n",
      "episode: 792, actor_loss: 5.633, critic_loss: 115.507, mean_reward: -1158.591, best_return: -908.498\n",
      "episode:  793.0 alpha+beta:  28.957737\n",
      "episode: 793, actor_loss: -0.915, critic_loss: 78.177, mean_reward: -1166.807, best_return: -908.498\n",
      "episode:  794.0 alpha+beta:  27.70108\n",
      "episode: 794, actor_loss: -2.168, critic_loss: 28.966, mean_reward: -1088.838, best_return: -908.498\n",
      "episode:  795.0 alpha+beta:  27.872318\n",
      "episode: 795, actor_loss: -0.533, critic_loss: 5.785, mean_reward: -1050.007, best_return: -908.498\n",
      "episode:  796.0 alpha+beta:  28.963768\n",
      "episode: 796, actor_loss: -0.815, critic_loss: 5.197, mean_reward: -1096.658, best_return: -908.498\n",
      "episode:  797.0 alpha+beta:  29.12717\n",
      "episode: 797, actor_loss: -0.918, critic_loss: 3.605, mean_reward: -1165.483, best_return: -908.498\n",
      "episode:  798.0 alpha+beta:  35.329643\n",
      "episode: 798, actor_loss: 11.232, critic_loss: 22.459, mean_reward: -1162.624, best_return: -908.498\n",
      "episode:  799.0 alpha+beta:  55.90584\n",
      "episode: 799, actor_loss: 0.576, critic_loss: 22.314, mean_reward: -1218.313, best_return: -908.498\n",
      "episode:  800.0 alpha+beta:  35.899513\n",
      "episode: 800, actor_loss: 45.546, critic_loss: 250.142, mean_reward: -1073.635, best_return: -908.498\n",
      "last 50 episode mean reward:  -1147.6586663129517\n",
      "\n",
      "\n",
      "episode:  801.0 alpha+beta:  43.099014\n",
      "episode: 801, actor_loss: -8.196, critic_loss: 70.032, mean_reward: -1169.480, best_return: -908.498\n",
      "episode:  802.0 alpha+beta:  38.04298\n",
      "episode: 802, actor_loss: -1.010, critic_loss: 22.137, mean_reward: -1209.499, best_return: -908.498\n",
      "episode:  803.0 alpha+beta:  31.40144\n",
      "episode: 803, actor_loss: -4.795, critic_loss: 74.683, mean_reward: -1165.134, best_return: -908.498\n",
      "episode:  804.0 alpha+beta:  24.002155\n",
      "episode: 804, actor_loss: -1.210, critic_loss: 23.944, mean_reward: -1290.843, best_return: -908.498\n",
      "episode:  805.0 alpha+beta:  25.098253\n",
      "episode: 805, actor_loss: 0.808, critic_loss: 77.602, mean_reward: -1072.822, best_return: -908.498\n",
      "episode:  806.0 alpha+beta:  24.05167\n",
      "episode: 806, actor_loss: 1.075, critic_loss: 8.080, mean_reward: -1266.519, best_return: -908.498\n",
      "episode:  807.0 alpha+beta:  25.34512\n",
      "episode: 807, actor_loss: 1.217, critic_loss: 22.824, mean_reward: -1202.297, best_return: -908.498\n",
      "episode:  808.0 alpha+beta:  24.852283\n",
      "episode: 808, actor_loss: 0.953, critic_loss: 11.425, mean_reward: -1159.853, best_return: -908.498\n",
      "episode:  809.0 alpha+beta:  23.862766\n",
      "episode: 809, actor_loss: 0.752, critic_loss: 8.515, mean_reward: -1082.263, best_return: -908.498\n",
      "episode:  810.0 alpha+beta:  23.667187\n",
      "episode: 810, actor_loss: 7.684, critic_loss: 114.259, mean_reward: -1068.750, best_return: -908.498\n",
      "episode:  811.0 alpha+beta:  44.203773\n",
      "episode: 811, actor_loss: -7.246, critic_loss: 42.606, mean_reward: -1205.532, best_return: -908.498\n",
      "episode:  812.0 alpha+beta:  37.458843\n",
      "episode: 812, actor_loss: 1.481, critic_loss: 37.238, mean_reward: -1210.483, best_return: -908.498\n",
      "episode:  813.0 alpha+beta:  31.381289\n",
      "episode: 813, actor_loss: -2.529, critic_loss: 37.320, mean_reward: -1102.436, best_return: -908.498\n",
      "episode:  814.0 alpha+beta:  27.732775\n",
      "episode: 814, actor_loss: -0.903, critic_loss: 27.766, mean_reward: -1170.460, best_return: -908.498\n",
      "episode:  815.0 alpha+beta:  29.227074\n",
      "episode: 815, actor_loss: -1.340, critic_loss: 10.016, mean_reward: -1095.967, best_return: -908.498\n",
      "episode:  816.0 alpha+beta:  29.465511\n",
      "episode: 816, actor_loss: 4.436, critic_loss: 29.799, mean_reward: -1110.677, best_return: -908.498\n",
      "episode:  817.0 alpha+beta:  26.713486\n",
      "episode: 817, actor_loss: -0.310, critic_loss: 6.774, mean_reward: -1136.463, best_return: -908.498\n",
      "episode:  818.0 alpha+beta:  23.976837\n",
      "episode: 818, actor_loss: 0.348, critic_loss: 4.368, mean_reward: -1085.787, best_return: -908.498\n",
      "episode:  819.0 alpha+beta:  25.134825\n",
      "episode: 819, actor_loss: 0.369, critic_loss: 3.809, mean_reward: -1154.609, best_return: -908.498\n",
      "episode:  820.0 alpha+beta:  25.5696\n",
      "episode: 820, actor_loss: 2.906, critic_loss: 162.107, mean_reward: -1138.542, best_return: -908.498\n",
      "episode:  821.0 alpha+beta:  42.7202\n",
      "episode: 821, actor_loss: -7.249, critic_loss: 58.354, mean_reward: -1143.713, best_return: -908.498\n",
      "episode:  822.0 alpha+beta:  31.201033\n",
      "episode: 822, actor_loss: -1.625, critic_loss: 26.513, mean_reward: -1224.305, best_return: -908.498\n",
      "episode:  823.0 alpha+beta:  28.55692\n",
      "episode: 823, actor_loss: -2.938, critic_loss: 16.088, mean_reward: -1189.667, best_return: -908.498\n",
      "episode:  824.0 alpha+beta:  24.40382\n",
      "episode: 824, actor_loss: -0.709, critic_loss: 14.691, mean_reward: -1217.124, best_return: -908.498\n",
      "episode:  825.0 alpha+beta:  24.39833\n",
      "episode: 825, actor_loss: 0.559, critic_loss: 13.482, mean_reward: -1194.233, best_return: -908.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  826.0 alpha+beta:  25.665495\n",
      "episode: 826, actor_loss: 1.060, critic_loss: 12.009, mean_reward: -1235.219, best_return: -908.498\n",
      "episode:  827.0 alpha+beta:  26.134716\n",
      "episode: 827, actor_loss: 0.712, critic_loss: 6.445, mean_reward: -1183.030, best_return: -908.498\n",
      "episode:  828.0 alpha+beta:  24.673264\n",
      "episode: 828, actor_loss: 0.425, critic_loss: 6.486, mean_reward: -1121.775, best_return: -908.498\n",
      "episode:  829.0 alpha+beta:  22.803776\n",
      "episode: 829, actor_loss: 0.709, critic_loss: 4.880, mean_reward: -1115.970, best_return: -908.498\n",
      "episode:  830.0 alpha+beta:  22.954136\n",
      "episode: 830, actor_loss: 15.138, critic_loss: 216.880, mean_reward: -1127.418, best_return: -908.498\n",
      "episode:  831.0 alpha+beta:  42.126495\n",
      "episode: 831, actor_loss: -7.939, critic_loss: 44.352, mean_reward: -1180.548, best_return: -908.498\n",
      "episode:  832.0 alpha+beta:  41.184814\n",
      "episode: 832, actor_loss: -1.000, critic_loss: 61.431, mean_reward: -1108.010, best_return: -908.498\n",
      "episode:  833.0 alpha+beta:  37.732975\n",
      "episode: 833, actor_loss: -2.089, critic_loss: 73.636, mean_reward: -1162.364, best_return: -908.498\n",
      "episode:  834.0 alpha+beta:  36.097168\n",
      "episode: 834, actor_loss: 3.420, critic_loss: 52.993, mean_reward: -1319.024, best_return: -908.498\n",
      "episode:  835.0 alpha+beta:  35.792473\n",
      "episode: 835, actor_loss: -3.642, critic_loss: 51.627, mean_reward: -1131.763, best_return: -908.498\n",
      "episode:  836.0 alpha+beta:  39.263424\n",
      "episode: 836, actor_loss: 4.092, critic_loss: 18.222, mean_reward: -1122.776, best_return: -908.498\n",
      "episode:  837.0 alpha+beta:  43.69873\n",
      "episode: 837, actor_loss: 0.099, critic_loss: 21.053, mean_reward: -1269.381, best_return: -908.498\n",
      "episode:  838.0 alpha+beta:  47.212402\n",
      "episode: 838, actor_loss: -2.714, critic_loss: 15.659, mean_reward: -1202.730, best_return: -908.498\n",
      "episode:  839.0 alpha+beta:  54.59095\n",
      "episode: 839, actor_loss: 2.634, critic_loss: 22.629, mean_reward: -1185.731, best_return: -908.498\n",
      "episode:  840.0 alpha+beta:  45.255043\n",
      "episode: 840, actor_loss: 25.187, critic_loss: 94.521, mean_reward: -1117.669, best_return: -908.498\n",
      "episode:  841.0 alpha+beta:  37.87361\n",
      "episode: 841, actor_loss: -5.355, critic_loss: 33.649, mean_reward: -1154.021, best_return: -908.498\n",
      "episode:  842.0 alpha+beta:  36.12604\n",
      "episode: 842, actor_loss: 2.266, critic_loss: 33.654, mean_reward: -1139.397, best_return: -908.498\n",
      "episode:  843.0 alpha+beta:  31.05361\n",
      "episode: 843, actor_loss: 3.717, critic_loss: 37.760, mean_reward: -1171.238, best_return: -908.498\n",
      "episode:  844.0 alpha+beta:  34.34292\n",
      "episode: 844, actor_loss: 0.883, critic_loss: 82.121, mean_reward: -1052.758, best_return: -908.498\n",
      "episode:  845.0 alpha+beta:  40.987877\n",
      "episode: 845, actor_loss: 8.982, critic_loss: 99.123, mean_reward: -1260.405, best_return: -908.498\n",
      "episode:  846.0 alpha+beta:  40.697903\n",
      "episode: 846, actor_loss: -1.670, critic_loss: 44.491, mean_reward: -1154.452, best_return: -908.498\n",
      "episode:  847.0 alpha+beta:  39.578007\n",
      "episode: 847, actor_loss: -2.514, critic_loss: 20.586, mean_reward: -1162.328, best_return: -908.498\n",
      "episode:  848.0 alpha+beta:  36.731853\n",
      "episode: 848, actor_loss: -3.735, critic_loss: 12.589, mean_reward: -1109.865, best_return: -908.498\n",
      "episode:  849.0 alpha+beta:  44.380623\n",
      "episode: 849, actor_loss: -4.533, critic_loss: 16.612, mean_reward: -1217.798, best_return: -908.498\n",
      "episode:  850.0 alpha+beta:  42.882816\n",
      "episode: 850, actor_loss: 14.561, critic_loss: 84.359, mean_reward: -1097.376, best_return: -908.498\n",
      "last 50 episode mean reward:  -1163.410078299059\n",
      "\n",
      "\n",
      "episode:  851.0 alpha+beta:  36.335667\n",
      "episode: 851, actor_loss: -4.350, critic_loss: 38.290, mean_reward: -1003.890, best_return: -908.498\n",
      "episode:  852.0 alpha+beta:  35.51417\n",
      "episode: 852, actor_loss: 3.601, critic_loss: 40.045, mean_reward: -1144.133, best_return: -908.498\n",
      "episode:  853.0 alpha+beta:  28.901417\n",
      "episode: 853, actor_loss: -5.859, critic_loss: 70.339, mean_reward: -1204.171, best_return: -908.498\n",
      "episode:  854.0 alpha+beta:  30.554054\n",
      "episode: 854, actor_loss: 5.358, critic_loss: 49.609, mean_reward: -1124.033, best_return: -908.498\n",
      "episode:  855.0 alpha+beta:  35.43386\n",
      "episode: 855, actor_loss: 2.834, critic_loss: 42.378, mean_reward: -1168.914, best_return: -908.498\n",
      "episode:  856.0 alpha+beta:  41.6698\n",
      "episode: 856, actor_loss: -0.702, critic_loss: 16.691, mean_reward: -1220.153, best_return: -908.498\n",
      "episode:  857.0 alpha+beta:  58.77948\n",
      "episode: 857, actor_loss: 6.105, critic_loss: 18.235, mean_reward: -1098.576, best_return: -908.498\n",
      "episode:  858.0 alpha+beta:  39.90146\n",
      "episode: 858, actor_loss: 0.364, critic_loss: 18.118, mean_reward: -964.606, best_return: -908.498\n",
      "episode:  859.0 alpha+beta:  42.633133\n",
      "episode: 859, actor_loss: 1.536, critic_loss: 5.661, mean_reward: -1040.230, best_return: -908.498\n",
      "episode:  860.0 alpha+beta:  43.081554\n",
      "episode: 860, actor_loss: -23.587, critic_loss: 49.719, mean_reward: -1053.523, best_return: -908.498\n",
      "episode:  861.0 alpha+beta:  37.842125\n",
      "episode: 861, actor_loss: -3.722, critic_loss: 27.075, mean_reward: -1120.648, best_return: -908.498\n",
      "episode:  862.0 alpha+beta:  37.186172\n",
      "episode: 862, actor_loss: -2.710, critic_loss: 37.230, mean_reward: -1074.562, best_return: -908.498\n",
      "episode:  863.0 alpha+beta:  39.37177\n",
      "episode: 863, actor_loss: 1.619, critic_loss: 30.051, mean_reward: -1309.743, best_return: -908.498\n",
      "episode:  864.0 alpha+beta:  31.318325\n",
      "episode: 864, actor_loss: 1.981, critic_loss: 28.994, mean_reward: -1165.121, best_return: -908.498\n",
      "episode:  865.0 alpha+beta:  32.252033\n",
      "episode: 865, actor_loss: 2.084, critic_loss: 8.115, mean_reward: -898.205, best_return: -898.205\n",
      "episode:  866.0 alpha+beta:  37.238605\n",
      "episode: 866, actor_loss: 4.536, critic_loss: 9.890, mean_reward: -1125.962, best_return: -898.205\n",
      "episode:  867.0 alpha+beta:  40.40612\n",
      "episode: 867, actor_loss: -2.099, critic_loss: 12.464, mean_reward: -1004.010, best_return: -898.205\n",
      "episode:  868.0 alpha+beta:  44.43563\n",
      "episode: 868, actor_loss: 7.943, critic_loss: 9.919, mean_reward: -1114.480, best_return: -898.205\n",
      "episode:  869.0 alpha+beta:  34.329266\n",
      "episode: 869, actor_loss: -0.665, critic_loss: 11.729, mean_reward: -1189.770, best_return: -898.205\n",
      "episode:  870.0 alpha+beta:  45.012836\n",
      "episode: 870, actor_loss: 20.888, critic_loss: 31.423, mean_reward: -1035.729, best_return: -898.205\n",
      "episode:  871.0 alpha+beta:  35.24724\n",
      "episode: 871, actor_loss: -2.105, critic_loss: 19.970, mean_reward: -1135.304, best_return: -898.205\n",
      "episode:  872.0 alpha+beta:  35.43316\n",
      "episode: 872, actor_loss: -0.305, critic_loss: 28.390, mean_reward: -1091.886, best_return: -898.205\n",
      "episode:  873.0 alpha+beta:  33.011185\n",
      "episode: 873, actor_loss: 2.170, critic_loss: 48.697, mean_reward: -1266.336, best_return: -898.205\n",
      "episode:  874.0 alpha+beta:  31.940336\n",
      "episode: 874, actor_loss: 1.366, critic_loss: 61.779, mean_reward: -1077.768, best_return: -898.205\n",
      "episode:  875.0 alpha+beta:  35.284386\n",
      "episode: 875, actor_loss: 1.496, critic_loss: 33.955, mean_reward: -999.154, best_return: -898.205\n",
      "episode:  876.0 alpha+beta:  35.434155\n",
      "episode: 876, actor_loss: -0.488, critic_loss: 20.650, mean_reward: -1084.121, best_return: -898.205\n",
      "episode:  877.0 alpha+beta:  41.111683\n",
      "episode: 877, actor_loss: 0.665, critic_loss: 10.470, mean_reward: -1149.863, best_return: -898.205\n",
      "episode:  878.0 alpha+beta:  41.547188\n",
      "episode: 878, actor_loss: 3.730, critic_loss: 8.334, mean_reward: -1095.674, best_return: -898.205\n",
      "episode:  879.0 alpha+beta:  37.76455\n",
      "episode: 879, actor_loss: 0.170, critic_loss: 5.601, mean_reward: -1329.349, best_return: -898.205\n",
      "episode:  880.0 alpha+beta:  30.314764\n",
      "episode: 880, actor_loss: 0.673, critic_loss: 153.725, mean_reward: -1021.371, best_return: -898.205\n",
      "episode:  881.0 alpha+beta:  41.054848\n",
      "episode: 881, actor_loss: -2.791, critic_loss: 28.751, mean_reward: -1261.376, best_return: -898.205\n",
      "episode:  882.0 alpha+beta:  41.399143\n",
      "episode: 882, actor_loss: 1.019, critic_loss: 25.175, mean_reward: -1218.693, best_return: -898.205\n",
      "episode:  883.0 alpha+beta:  32.336098\n",
      "episode: 883, actor_loss: -1.718, critic_loss: 59.644, mean_reward: -1141.076, best_return: -898.205\n",
      "episode:  884.0 alpha+beta:  41.399822\n",
      "episode: 884, actor_loss: 4.213, critic_loss: 23.593, mean_reward: -1033.906, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  885.0 alpha+beta:  33.2068\n",
      "episode: 885, actor_loss: 0.812, critic_loss: 20.998, mean_reward: -998.057, best_return: -898.205\n",
      "episode:  886.0 alpha+beta:  37.06687\n",
      "episode: 886, actor_loss: 1.885, critic_loss: 9.247, mean_reward: -989.181, best_return: -898.205\n",
      "episode:  887.0 alpha+beta:  33.002583\n",
      "episode: 887, actor_loss: 3.019, critic_loss: 5.617, mean_reward: -1095.713, best_return: -898.205\n",
      "episode:  888.0 alpha+beta:  32.48486\n",
      "episode: 888, actor_loss: 0.616, critic_loss: 2.710, mean_reward: -1131.343, best_return: -898.205\n",
      "episode:  889.0 alpha+beta:  35.98937\n",
      "episode: 889, actor_loss: -1.411, critic_loss: 18.269, mean_reward: -903.302, best_return: -898.205\n",
      "episode:  890.0 alpha+beta:  36.20054\n",
      "episode: 890, actor_loss: -14.204, critic_loss: 93.582, mean_reward: -1065.012, best_return: -898.205\n",
      "episode:  891.0 alpha+beta:  38.402508\n",
      "episode: 891, actor_loss: -1.172, critic_loss: 34.164, mean_reward: -1082.619, best_return: -898.205\n",
      "episode:  892.0 alpha+beta:  40.522064\n",
      "episode: 892, actor_loss: -2.698, critic_loss: 24.132, mean_reward: -1122.485, best_return: -898.205\n",
      "episode:  893.0 alpha+beta:  34.87107\n",
      "episode: 893, actor_loss: -0.926, critic_loss: 43.154, mean_reward: -1004.051, best_return: -898.205\n",
      "episode:  894.0 alpha+beta:  32.097378\n",
      "episode: 894, actor_loss: -2.818, critic_loss: 26.634, mean_reward: -924.244, best_return: -898.205\n",
      "episode:  895.0 alpha+beta:  34.63998\n",
      "episode: 895, actor_loss: 0.816, critic_loss: 32.034, mean_reward: -1087.987, best_return: -898.205\n",
      "episode:  896.0 alpha+beta:  34.645527\n",
      "episode: 896, actor_loss: 0.644, critic_loss: 12.924, mean_reward: -1267.978, best_return: -898.205\n",
      "episode:  897.0 alpha+beta:  34.748436\n",
      "episode: 897, actor_loss: -2.772, critic_loss: 8.240, mean_reward: -1101.600, best_return: -898.205\n",
      "episode:  898.0 alpha+beta:  42.09162\n",
      "episode: 898, actor_loss: 4.157, critic_loss: 17.839, mean_reward: -1105.910, best_return: -898.205\n",
      "episode:  899.0 alpha+beta:  35.650345\n",
      "episode: 899, actor_loss: 0.716, critic_loss: 4.573, mean_reward: -1178.602, best_return: -898.205\n",
      "episode:  900.0 alpha+beta:  33.83852\n",
      "episode: 900, actor_loss: 6.911, critic_loss: 93.165, mean_reward: -1138.156, best_return: -898.205\n",
      "last 50 episode mean reward:  -1103.251567508825\n",
      "\n",
      "\n",
      "episode:  901.0 alpha+beta:  37.49849\n",
      "episode: 901, actor_loss: -1.393, critic_loss: 13.595, mean_reward: -1007.630, best_return: -898.205\n",
      "episode:  902.0 alpha+beta:  37.172447\n",
      "episode: 902, actor_loss: 1.041, critic_loss: 23.116, mean_reward: -1078.142, best_return: -898.205\n",
      "episode:  903.0 alpha+beta:  27.511517\n",
      "episode: 903, actor_loss: -1.704, critic_loss: 63.308, mean_reward: -1055.646, best_return: -898.205\n",
      "episode:  904.0 alpha+beta:  32.319572\n",
      "episode: 904, actor_loss: 4.286, critic_loss: 34.971, mean_reward: -1135.220, best_return: -898.205\n",
      "episode:  905.0 alpha+beta:  37.945263\n",
      "episode: 905, actor_loss: 0.234, critic_loss: 22.640, mean_reward: -983.303, best_return: -898.205\n",
      "episode:  906.0 alpha+beta:  42.65513\n",
      "episode: 906, actor_loss: 5.763, critic_loss: 9.428, mean_reward: -1126.884, best_return: -898.205\n",
      "episode:  907.0 alpha+beta:  32.766296\n",
      "episode: 907, actor_loss: 0.409, critic_loss: 19.590, mean_reward: -1154.053, best_return: -898.205\n",
      "episode:  908.0 alpha+beta:  35.479572\n",
      "episode: 908, actor_loss: 0.039, critic_loss: 8.584, mean_reward: -1056.801, best_return: -898.205\n",
      "episode:  909.0 alpha+beta:  33.626442\n",
      "episode: 909, actor_loss: -2.767, critic_loss: 5.019, mean_reward: -1070.096, best_return: -898.205\n",
      "episode:  910.0 alpha+beta:  34.580624\n",
      "episode: 910, actor_loss: -22.364, critic_loss: 75.869, mean_reward: -1227.656, best_return: -898.205\n",
      "episode:  911.0 alpha+beta:  38.436234\n",
      "episode: 911, actor_loss: -0.667, critic_loss: 27.477, mean_reward: -1137.854, best_return: -898.205\n",
      "episode:  912.0 alpha+beta:  37.558693\n",
      "episode: 912, actor_loss: 1.165, critic_loss: 38.560, mean_reward: -1149.305, best_return: -898.205\n",
      "episode:  913.0 alpha+beta:  27.14645\n",
      "episode: 913, actor_loss: 3.043, critic_loss: 83.935, mean_reward: -1113.896, best_return: -898.205\n",
      "episode:  914.0 alpha+beta:  23.440662\n",
      "episode: 914, actor_loss: 1.783, critic_loss: 65.304, mean_reward: -1255.299, best_return: -898.205\n",
      "episode:  915.0 alpha+beta:  22.165977\n",
      "episode: 915, actor_loss: 2.953, critic_loss: 32.089, mean_reward: -1200.175, best_return: -898.205\n",
      "episode:  916.0 alpha+beta:  27.192577\n",
      "episode: 916, actor_loss: 3.322, critic_loss: 62.805, mean_reward: -1228.573, best_return: -898.205\n",
      "episode:  917.0 alpha+beta:  29.66454\n",
      "episode: 917, actor_loss: 3.914, critic_loss: 35.157, mean_reward: -1103.975, best_return: -898.205\n",
      "episode:  918.0 alpha+beta:  35.01983\n",
      "episode: 918, actor_loss: 0.372, critic_loss: 54.930, mean_reward: -1095.871, best_return: -898.205\n",
      "episode:  919.0 alpha+beta:  39.147102\n",
      "episode: 919, actor_loss: 0.383, critic_loss: 26.120, mean_reward: -1174.769, best_return: -898.205\n",
      "episode:  920.0 alpha+beta:  39.153656\n",
      "episode: 920, actor_loss: -19.953, critic_loss: 180.955, mean_reward: -1137.961, best_return: -898.205\n",
      "episode:  921.0 alpha+beta:  37.223366\n",
      "episode: 921, actor_loss: -1.425, critic_loss: 29.870, mean_reward: -1104.970, best_return: -898.205\n",
      "episode:  922.0 alpha+beta:  38.53527\n",
      "episode: 922, actor_loss: -4.354, critic_loss: 49.424, mean_reward: -1120.212, best_return: -898.205\n",
      "episode:  923.0 alpha+beta:  27.43296\n",
      "episode: 923, actor_loss: 0.696, critic_loss: 63.654, mean_reward: -1086.219, best_return: -898.205\n",
      "episode:  924.0 alpha+beta:  27.42153\n",
      "episode: 924, actor_loss: -3.686, critic_loss: 21.768, mean_reward: -1049.046, best_return: -898.205\n",
      "episode:  925.0 alpha+beta:  22.518263\n",
      "episode: 925, actor_loss: -1.730, critic_loss: 4.809, mean_reward: -1108.335, best_return: -898.205\n",
      "episode:  926.0 alpha+beta:  20.961784\n",
      "episode: 926, actor_loss: 1.309, critic_loss: 13.550, mean_reward: -1050.672, best_return: -898.205\n",
      "episode:  927.0 alpha+beta:  20.56522\n",
      "episode: 927, actor_loss: 2.176, critic_loss: 35.364, mean_reward: -1140.038, best_return: -898.205\n",
      "episode:  928.0 alpha+beta:  20.741222\n",
      "episode: 928, actor_loss: 2.749, critic_loss: 52.855, mean_reward: -1136.707, best_return: -898.205\n",
      "episode:  929.0 alpha+beta:  19.680357\n",
      "episode: 929, actor_loss: 2.227, critic_loss: 31.433, mean_reward: -1265.050, best_return: -898.205\n",
      "episode:  930.0 alpha+beta:  19.305515\n",
      "episode: 930, actor_loss: 20.747, critic_loss: 66.855, mean_reward: -1152.056, best_return: -898.205\n",
      "episode:  931.0 alpha+beta:  40.27582\n",
      "episode: 931, actor_loss: -3.711, critic_loss: 56.637, mean_reward: -1160.283, best_return: -898.205\n",
      "episode:  932.0 alpha+beta:  39.103615\n",
      "episode: 932, actor_loss: -1.099, critic_loss: 36.111, mean_reward: -1125.961, best_return: -898.205\n",
      "episode:  933.0 alpha+beta:  33.13557\n",
      "episode: 933, actor_loss: -0.973, critic_loss: 64.558, mean_reward: -1217.940, best_return: -898.205\n",
      "episode:  934.0 alpha+beta:  30.394516\n",
      "episode: 934, actor_loss: 5.380, critic_loss: 51.332, mean_reward: -1191.012, best_return: -898.205\n",
      "episode:  935.0 alpha+beta:  30.887735\n",
      "episode: 935, actor_loss: -2.895, critic_loss: 16.064, mean_reward: -1168.256, best_return: -898.205\n",
      "episode:  936.0 alpha+beta:  25.306387\n",
      "episode: 936, actor_loss: 0.335, critic_loss: 15.927, mean_reward: -1169.527, best_return: -898.205\n",
      "episode:  937.0 alpha+beta:  29.134697\n",
      "episode: 937, actor_loss: -0.114, critic_loss: 13.223, mean_reward: -1129.590, best_return: -898.205\n",
      "episode:  938.0 alpha+beta:  30.609165\n",
      "episode: 938, actor_loss: 0.288, critic_loss: 9.042, mean_reward: -1197.882, best_return: -898.205\n",
      "episode:  939.0 alpha+beta:  28.535038\n",
      "episode: 939, actor_loss: 4.328, critic_loss: 14.249, mean_reward: -1137.738, best_return: -898.205\n",
      "episode:  940.0 alpha+beta:  29.10764\n",
      "episode: 940, actor_loss: -1.077, critic_loss: 75.696, mean_reward: -1136.844, best_return: -898.205\n",
      "episode:  941.0 alpha+beta:  39.59471\n",
      "episode: 941, actor_loss: -7.190, critic_loss: 30.151, mean_reward: -1118.593, best_return: -898.205\n",
      "episode:  942.0 alpha+beta:  31.217728\n",
      "episode: 942, actor_loss: -2.868, critic_loss: 36.077, mean_reward: -1170.527, best_return: -898.205\n",
      "episode:  943.0 alpha+beta:  30.839802\n",
      "episode: 943, actor_loss: -2.070, critic_loss: 41.619, mean_reward: -1015.562, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  944.0 alpha+beta:  28.35344\n",
      "episode: 944, actor_loss: 0.387, critic_loss: 51.852, mean_reward: -1054.692, best_return: -898.205\n",
      "episode:  945.0 alpha+beta:  27.425442\n",
      "episode: 945, actor_loss: -1.109, critic_loss: 21.342, mean_reward: -1061.251, best_return: -898.205\n",
      "episode:  946.0 alpha+beta:  25.347885\n",
      "episode: 946, actor_loss: -0.180, critic_loss: 12.786, mean_reward: -953.950, best_return: -898.205\n",
      "episode:  947.0 alpha+beta:  22.789875\n",
      "episode: 947, actor_loss: 1.433, critic_loss: 7.221, mean_reward: -1094.913, best_return: -898.205\n",
      "episode:  948.0 alpha+beta:  20.995834\n",
      "episode: 948, actor_loss: 0.873, critic_loss: 7.501, mean_reward: -1174.310, best_return: -898.205\n",
      "episode:  949.0 alpha+beta:  25.33509\n",
      "episode: 949, actor_loss: 1.729, critic_loss: 7.420, mean_reward: -1133.776, best_return: -898.205\n",
      "episode:  950.0 alpha+beta:  24.640955\n",
      "episode: 950, actor_loss: 4.203, critic_loss: 87.360, mean_reward: -1106.811, best_return: -898.205\n",
      "last 50 episode mean reward:  -1124.516595143846\n",
      "\n",
      "\n",
      "episode:  951.0 alpha+beta:  35.811985\n",
      "episode: 951, actor_loss: -6.683, critic_loss: 45.475, mean_reward: -1119.037, best_return: -898.205\n",
      "episode:  952.0 alpha+beta:  34.23307\n",
      "episode: 952, actor_loss: -4.071, critic_loss: 34.521, mean_reward: -985.027, best_return: -898.205\n",
      "episode:  953.0 alpha+beta:  31.3944\n",
      "episode: 953, actor_loss: -1.214, critic_loss: 34.539, mean_reward: -1166.105, best_return: -898.205\n",
      "episode:  954.0 alpha+beta:  24.124561\n",
      "episode: 954, actor_loss: 0.670, critic_loss: 44.870, mean_reward: -1143.757, best_return: -898.205\n",
      "episode:  955.0 alpha+beta:  26.218285\n",
      "episode: 955, actor_loss: 6.572, critic_loss: 53.275, mean_reward: -1115.701, best_return: -898.205\n",
      "episode:  956.0 alpha+beta:  25.01034\n",
      "episode: 956, actor_loss: -0.281, critic_loss: 7.184, mean_reward: -1075.749, best_return: -898.205\n",
      "episode:  957.0 alpha+beta:  25.361223\n",
      "episode: 957, actor_loss: 2.875, critic_loss: 23.496, mean_reward: -1123.579, best_return: -898.205\n",
      "episode:  958.0 alpha+beta:  20.020342\n",
      "episode: 958, actor_loss: 3.849, critic_loss: 16.258, mean_reward: -1222.393, best_return: -898.205\n",
      "episode:  959.0 alpha+beta:  23.8097\n",
      "episode: 959, actor_loss: 1.857, critic_loss: 6.029, mean_reward: -1151.671, best_return: -898.205\n",
      "episode:  960.0 alpha+beta:  20.884277\n",
      "episode: 960, actor_loss: 7.484, critic_loss: 96.398, mean_reward: -1159.966, best_return: -898.205\n",
      "episode:  961.0 alpha+beta:  33.97121\n",
      "episode: 961, actor_loss: -3.161, critic_loss: 27.502, mean_reward: -1122.342, best_return: -898.205\n",
      "episode:  962.0 alpha+beta:  39.08947\n",
      "episode: 962, actor_loss: -2.152, critic_loss: 11.487, mean_reward: -1228.158, best_return: -898.205\n",
      "episode:  963.0 alpha+beta:  32.957207\n",
      "episode: 963, actor_loss: 0.761, critic_loss: 19.574, mean_reward: -1221.723, best_return: -898.205\n",
      "episode:  964.0 alpha+beta:  35.35903\n",
      "episode: 964, actor_loss: -0.900, critic_loss: 25.199, mean_reward: -1305.971, best_return: -898.205\n",
      "episode:  965.0 alpha+beta:  39.530952\n",
      "episode: 965, actor_loss: 1.508, critic_loss: 37.864, mean_reward: -972.619, best_return: -898.205\n",
      "episode:  966.0 alpha+beta:  30.53244\n",
      "episode: 966, actor_loss: 3.028, critic_loss: 10.854, mean_reward: -1104.459, best_return: -898.205\n",
      "episode:  967.0 alpha+beta:  31.562748\n",
      "episode: 967, actor_loss: 0.097, critic_loss: 4.449, mean_reward: -1292.649, best_return: -898.205\n",
      "episode:  968.0 alpha+beta:  32.509388\n",
      "episode: 968, actor_loss: -2.114, critic_loss: 4.105, mean_reward: -1168.660, best_return: -898.205\n",
      "episode:  969.0 alpha+beta:  35.282215\n",
      "episode: 969, actor_loss: -0.937, critic_loss: 2.646, mean_reward: -1081.125, best_return: -898.205\n",
      "episode:  970.0 alpha+beta:  33.52153\n",
      "episode: 970, actor_loss: 6.256, critic_loss: 97.459, mean_reward: -1251.675, best_return: -898.205\n",
      "episode:  971.0 alpha+beta:  34.59459\n",
      "episode: 971, actor_loss: -1.367, critic_loss: 19.260, mean_reward: -1099.577, best_return: -898.205\n",
      "episode:  972.0 alpha+beta:  41.263004\n",
      "episode: 972, actor_loss: -2.604, critic_loss: 35.819, mean_reward: -1265.221, best_return: -898.205\n",
      "episode:  973.0 alpha+beta:  34.252438\n",
      "episode: 973, actor_loss: -3.313, critic_loss: 84.765, mean_reward: -1204.886, best_return: -898.205\n",
      "episode:  974.0 alpha+beta:  36.436867\n",
      "episode: 974, actor_loss: -1.118, critic_loss: 20.227, mean_reward: -1100.763, best_return: -898.205\n",
      "episode:  975.0 alpha+beta:  37.10283\n",
      "episode: 975, actor_loss: 0.433, critic_loss: 17.272, mean_reward: -1150.041, best_return: -898.205\n",
      "episode:  976.0 alpha+beta:  36.077896\n",
      "episode: 976, actor_loss: -1.045, critic_loss: 12.957, mean_reward: -1252.572, best_return: -898.205\n",
      "episode:  977.0 alpha+beta:  36.496933\n",
      "episode: 977, actor_loss: 0.280, critic_loss: 12.518, mean_reward: -1084.387, best_return: -898.205\n",
      "episode:  978.0 alpha+beta:  32.376633\n",
      "episode: 978, actor_loss: 0.783, critic_loss: 10.774, mean_reward: -1113.934, best_return: -898.205\n",
      "episode:  979.0 alpha+beta:  30.196087\n",
      "episode: 979, actor_loss: -0.098, critic_loss: 6.250, mean_reward: -1090.143, best_return: -898.205\n",
      "episode:  980.0 alpha+beta:  28.733782\n",
      "episode: 980, actor_loss: -12.378, critic_loss: 73.849, mean_reward: -1178.514, best_return: -898.205\n",
      "episode:  981.0 alpha+beta:  37.1193\n",
      "episode: 981, actor_loss: -2.397, critic_loss: 12.554, mean_reward: -1115.889, best_return: -898.205\n",
      "episode:  982.0 alpha+beta:  37.868004\n",
      "episode: 982, actor_loss: -1.260, critic_loss: 13.875, mean_reward: -1202.277, best_return: -898.205\n",
      "episode:  983.0 alpha+beta:  30.626417\n",
      "episode: 983, actor_loss: -4.897, critic_loss: 65.199, mean_reward: -1274.582, best_return: -898.205\n",
      "episode:  984.0 alpha+beta:  26.761246\n",
      "episode: 984, actor_loss: -1.844, critic_loss: 43.004, mean_reward: -1190.419, best_return: -898.205\n",
      "episode:  985.0 alpha+beta:  28.291752\n",
      "episode: 985, actor_loss: 1.497, critic_loss: 33.507, mean_reward: -1133.077, best_return: -898.205\n",
      "episode:  986.0 alpha+beta:  26.535652\n",
      "episode: 986, actor_loss: 1.391, critic_loss: 6.136, mean_reward: -1045.226, best_return: -898.205\n",
      "episode:  987.0 alpha+beta:  25.245415\n",
      "episode: 987, actor_loss: 0.272, critic_loss: 5.768, mean_reward: -1138.668, best_return: -898.205\n",
      "episode:  988.0 alpha+beta:  19.702356\n",
      "episode: 988, actor_loss: 6.829, critic_loss: 20.561, mean_reward: -1240.965, best_return: -898.205\n",
      "episode:  989.0 alpha+beta:  21.755144\n",
      "episode: 989, actor_loss: 2.060, critic_loss: 5.401, mean_reward: -1018.642, best_return: -898.205\n",
      "episode:  990.0 alpha+beta:  20.13147\n",
      "episode: 990, actor_loss: 33.646, critic_loss: 85.626, mean_reward: -1113.988, best_return: -898.205\n",
      "episode:  991.0 alpha+beta:  35.32617\n",
      "episode: 991, actor_loss: -3.734, critic_loss: 27.913, mean_reward: -1150.142, best_return: -898.205\n",
      "episode:  992.0 alpha+beta:  35.801193\n",
      "episode: 992, actor_loss: 3.692, critic_loss: 26.183, mean_reward: -1265.709, best_return: -898.205\n",
      "episode:  993.0 alpha+beta:  28.04475\n",
      "episode: 993, actor_loss: 1.152, critic_loss: 62.351, mean_reward: -1198.704, best_return: -898.205\n",
      "episode:  994.0 alpha+beta:  20.068077\n",
      "episode: 994, actor_loss: -1.745, critic_loss: 28.920, mean_reward: -1151.547, best_return: -898.205\n",
      "episode:  995.0 alpha+beta:  24.925041\n",
      "episode: 995, actor_loss: 3.770, critic_loss: 16.950, mean_reward: -1057.108, best_return: -898.205\n",
      "episode:  996.0 alpha+beta:  20.514698\n",
      "episode: 996, actor_loss: 2.941, critic_loss: 20.292, mean_reward: -1108.867, best_return: -898.205\n",
      "episode:  997.0 alpha+beta:  19.89006\n",
      "episode: 997, actor_loss: 3.632, critic_loss: 7.505, mean_reward: -1015.649, best_return: -898.205\n",
      "episode:  998.0 alpha+beta:  19.953049\n",
      "episode: 998, actor_loss: 4.581, critic_loss: 29.403, mean_reward: -1312.830, best_return: -898.205\n",
      "episode:  999.0 alpha+beta:  20.09903\n",
      "episode: 999, actor_loss: 4.143, critic_loss: 20.909, mean_reward: -1038.408, best_return: -898.205\n",
      "episode:  1000.0 alpha+beta:  20.799526\n",
      "episode: 1000, actor_loss: 35.419, critic_loss: 198.430, mean_reward: -1151.037, best_return: -898.205\n",
      "last 50 episode mean reward:  -1149.522820984384\n",
      "\n",
      "\n",
      "episode:  1001.0 alpha+beta:  37.635033\n",
      "episode: 1001, actor_loss: -7.272, critic_loss: 26.823, mean_reward: -1169.953, best_return: -898.205\n",
      "episode:  1002.0 alpha+beta:  40.93572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1002, actor_loss: 1.001, critic_loss: 26.567, mean_reward: -1137.900, best_return: -898.205\n",
      "episode:  1003.0 alpha+beta:  34.39392\n",
      "episode: 1003, actor_loss: -8.390, critic_loss: 31.373, mean_reward: -1192.269, best_return: -898.205\n",
      "episode:  1004.0 alpha+beta:  30.471403\n",
      "episode: 1004, actor_loss: -1.670, critic_loss: 21.285, mean_reward: -1114.849, best_return: -898.205\n",
      "episode:  1005.0 alpha+beta:  24.253157\n",
      "episode: 1005, actor_loss: -6.044, critic_loss: 12.233, mean_reward: -972.151, best_return: -898.205\n",
      "episode:  1006.0 alpha+beta:  22.56905\n",
      "episode: 1006, actor_loss: 1.399, critic_loss: 15.039, mean_reward: -1094.137, best_return: -898.205\n",
      "episode:  1007.0 alpha+beta:  21.232552\n",
      "episode: 1007, actor_loss: 0.454, critic_loss: 10.292, mean_reward: -990.241, best_return: -898.205\n",
      "episode:  1008.0 alpha+beta:  22.080662\n",
      "episode: 1008, actor_loss: 2.385, critic_loss: 14.439, mean_reward: -1292.770, best_return: -898.205\n",
      "episode:  1009.0 alpha+beta:  19.95631\n",
      "episode: 1009, actor_loss: 1.377, critic_loss: 12.608, mean_reward: -1162.908, best_return: -898.205\n",
      "episode:  1010.0 alpha+beta:  20.985506\n",
      "episode: 1010, actor_loss: 12.893, critic_loss: 71.090, mean_reward: -1061.221, best_return: -898.205\n",
      "episode:  1011.0 alpha+beta:  40.756264\n",
      "episode: 1011, actor_loss: -7.498, critic_loss: 27.305, mean_reward: -1276.177, best_return: -898.205\n",
      "episode:  1012.0 alpha+beta:  34.656425\n",
      "episode: 1012, actor_loss: -6.139, critic_loss: 22.431, mean_reward: -1144.061, best_return: -898.205\n",
      "episode:  1013.0 alpha+beta:  26.727564\n",
      "episode: 1013, actor_loss: -5.977, critic_loss: 74.363, mean_reward: -1124.771, best_return: -898.205\n",
      "episode:  1014.0 alpha+beta:  25.461294\n",
      "episode: 1014, actor_loss: -2.882, critic_loss: 19.019, mean_reward: -1189.850, best_return: -898.205\n",
      "episode:  1015.0 alpha+beta:  21.234104\n",
      "episode: 1015, actor_loss: -1.053, critic_loss: 19.892, mean_reward: -1194.712, best_return: -898.205\n",
      "episode:  1016.0 alpha+beta:  23.970985\n",
      "episode: 1016, actor_loss: 2.012, critic_loss: 31.356, mean_reward: -1179.450, best_return: -898.205\n",
      "episode:  1017.0 alpha+beta:  21.77224\n",
      "episode: 1017, actor_loss: 1.291, critic_loss: 3.937, mean_reward: -1198.917, best_return: -898.205\n",
      "episode:  1018.0 alpha+beta:  22.549889\n",
      "episode: 1018, actor_loss: 0.655, critic_loss: 6.103, mean_reward: -1296.876, best_return: -898.205\n",
      "episode:  1019.0 alpha+beta:  20.961523\n",
      "episode: 1019, actor_loss: 1.756, critic_loss: 9.368, mean_reward: -1204.193, best_return: -898.205\n",
      "episode:  1020.0 alpha+beta:  19.625404\n",
      "episode: 1020, actor_loss: 12.123, critic_loss: 71.032, mean_reward: -1222.954, best_return: -898.205\n",
      "episode:  1021.0 alpha+beta:  40.652027\n",
      "episode: 1021, actor_loss: -10.896, critic_loss: 44.079, mean_reward: -1077.327, best_return: -898.205\n",
      "episode:  1022.0 alpha+beta:  32.31215\n",
      "episode: 1022, actor_loss: -1.432, critic_loss: 37.731, mean_reward: -1084.684, best_return: -898.205\n",
      "episode:  1023.0 alpha+beta:  31.41834\n",
      "episode: 1023, actor_loss: 4.705, critic_loss: 133.964, mean_reward: -1138.851, best_return: -898.205\n",
      "episode:  1024.0 alpha+beta:  25.05333\n",
      "episode: 1024, actor_loss: -4.544, critic_loss: 35.425, mean_reward: -1132.005, best_return: -898.205\n",
      "episode:  1025.0 alpha+beta:  24.186039\n",
      "episode: 1025, actor_loss: 1.026, critic_loss: 6.424, mean_reward: -1095.445, best_return: -898.205\n",
      "episode:  1026.0 alpha+beta:  19.211758\n",
      "episode: 1026, actor_loss: 0.294, critic_loss: 1.674, mean_reward: -1230.553, best_return: -898.205\n",
      "episode:  1027.0 alpha+beta:  18.297096\n",
      "episode: 1027, actor_loss: 1.798, critic_loss: 38.368, mean_reward: -1301.745, best_return: -898.205\n",
      "episode:  1028.0 alpha+beta:  17.333033\n",
      "episode: 1028, actor_loss: 1.942, critic_loss: 29.470, mean_reward: -1265.272, best_return: -898.205\n",
      "episode:  1029.0 alpha+beta:  17.638983\n",
      "episode: 1029, actor_loss: 1.978, critic_loss: 18.296, mean_reward: -1067.647, best_return: -898.205\n",
      "episode:  1030.0 alpha+beta:  17.796627\n",
      "episode: 1030, actor_loss: 15.125, critic_loss: 59.279, mean_reward: -1264.411, best_return: -898.205\n",
      "episode:  1031.0 alpha+beta:  37.753685\n",
      "episode: 1031, actor_loss: -6.995, critic_loss: 20.698, mean_reward: -1193.046, best_return: -898.205\n",
      "episode:  1032.0 alpha+beta:  26.843918\n",
      "episode: 1032, actor_loss: -4.062, critic_loss: 36.397, mean_reward: -1092.235, best_return: -898.205\n",
      "episode:  1033.0 alpha+beta:  28.729855\n",
      "episode: 1033, actor_loss: 0.643, critic_loss: 13.409, mean_reward: -1177.146, best_return: -898.205\n",
      "episode:  1034.0 alpha+beta:  20.066843\n",
      "episode: 1034, actor_loss: -3.892, critic_loss: 29.164, mean_reward: -1181.528, best_return: -898.205\n",
      "episode:  1035.0 alpha+beta:  19.872072\n",
      "episode: 1035, actor_loss: -0.681, critic_loss: 4.618, mean_reward: -1281.206, best_return: -898.205\n",
      "episode:  1036.0 alpha+beta:  17.47052\n",
      "episode: 1036, actor_loss: 1.439, critic_loss: 14.193, mean_reward: -1258.746, best_return: -898.205\n",
      "episode:  1037.0 alpha+beta:  16.614952\n",
      "episode: 1037, actor_loss: 1.369, critic_loss: 23.274, mean_reward: -1136.397, best_return: -898.205\n",
      "episode:  1038.0 alpha+beta:  17.110518\n",
      "episode: 1038, actor_loss: 1.557, critic_loss: 8.396, mean_reward: -1205.214, best_return: -898.205\n",
      "episode:  1039.0 alpha+beta:  16.695559\n",
      "episode: 1039, actor_loss: 1.126, critic_loss: 13.619, mean_reward: -1124.789, best_return: -898.205\n",
      "episode:  1040.0 alpha+beta:  17.664015\n",
      "episode: 1040, actor_loss: 10.316, critic_loss: 23.300, mean_reward: -1306.045, best_return: -898.205\n",
      "episode:  1041.0 alpha+beta:  39.792904\n",
      "episode: 1041, actor_loss: -11.637, critic_loss: 45.827, mean_reward: -1144.377, best_return: -898.205\n",
      "episode:  1042.0 alpha+beta:  25.605907\n",
      "episode: 1042, actor_loss: -1.748, critic_loss: 12.553, mean_reward: -1134.651, best_return: -898.205\n",
      "episode:  1043.0 alpha+beta:  23.180878\n",
      "episode: 1043, actor_loss: -2.910, critic_loss: 25.288, mean_reward: -1154.469, best_return: -898.205\n",
      "episode:  1044.0 alpha+beta:  20.225971\n",
      "episode: 1044, actor_loss: 0.087, critic_loss: 3.280, mean_reward: -1074.713, best_return: -898.205\n",
      "episode:  1045.0 alpha+beta:  19.350582\n",
      "episode: 1045, actor_loss: 1.421, critic_loss: 36.191, mean_reward: -1062.975, best_return: -898.205\n",
      "episode:  1046.0 alpha+beta:  19.048681\n",
      "episode: 1046, actor_loss: 1.096, critic_loss: 20.075, mean_reward: -1150.133, best_return: -898.205\n",
      "episode:  1047.0 alpha+beta:  19.577274\n",
      "episode: 1047, actor_loss: 1.031, critic_loss: 12.492, mean_reward: -1111.434, best_return: -898.205\n",
      "episode:  1048.0 alpha+beta:  19.905897\n",
      "episode: 1048, actor_loss: 1.077, critic_loss: 11.288, mean_reward: -1148.053, best_return: -898.205\n",
      "episode:  1049.0 alpha+beta:  20.351545\n",
      "episode: 1049, actor_loss: 0.595, critic_loss: 21.600, mean_reward: -1237.093, best_return: -898.205\n",
      "episode:  1050.0 alpha+beta:  20.402248\n",
      "episode: 1050, actor_loss: 7.121, critic_loss: 25.919, mean_reward: -1215.679, best_return: -898.205\n",
      "last 50 episode mean reward:  -1165.3645514164218\n",
      "\n",
      "\n",
      "episode:  1051.0 alpha+beta:  38.74639\n",
      "episode: 1051, actor_loss: -7.825, critic_loss: 33.254, mean_reward: -1221.526, best_return: -898.205\n",
      "episode:  1052.0 alpha+beta:  39.81705\n",
      "episode: 1052, actor_loss: -5.063, critic_loss: 40.814, mean_reward: -1098.291, best_return: -898.205\n",
      "episode:  1053.0 alpha+beta:  33.263203\n",
      "episode: 1053, actor_loss: -4.711, critic_loss: 37.667, mean_reward: -1113.583, best_return: -898.205\n",
      "episode:  1054.0 alpha+beta:  28.048794\n",
      "episode: 1054, actor_loss: -4.245, critic_loss: 26.945, mean_reward: -1094.332, best_return: -898.205\n",
      "episode:  1055.0 alpha+beta:  22.820992\n",
      "episode: 1055, actor_loss: -3.222, critic_loss: 7.491, mean_reward: -1285.999, best_return: -898.205\n",
      "episode:  1056.0 alpha+beta:  23.63605\n",
      "episode: 1056, actor_loss: 3.159, critic_loss: 23.962, mean_reward: -1186.687, best_return: -898.205\n",
      "episode:  1057.0 alpha+beta:  20.68615\n",
      "episode: 1057, actor_loss: 1.042, critic_loss: 12.527, mean_reward: -1186.895, best_return: -898.205\n",
      "episode:  1058.0 alpha+beta:  20.51639\n",
      "episode: 1058, actor_loss: 1.369, critic_loss: 20.794, mean_reward: -1185.529, best_return: -898.205\n",
      "episode:  1059.0 alpha+beta:  19.81052\n",
      "episode: 1059, actor_loss: 1.434, critic_loss: 21.353, mean_reward: -1116.067, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1060.0 alpha+beta:  19.885284\n",
      "episode: 1060, actor_loss: 7.435, critic_loss: 16.388, mean_reward: -1164.185, best_return: -898.205\n",
      "episode:  1061.0 alpha+beta:  42.676308\n",
      "episode: 1061, actor_loss: -5.000, critic_loss: 15.422, mean_reward: -1240.414, best_return: -898.205\n",
      "episode:  1062.0 alpha+beta:  31.49641\n",
      "episode: 1062, actor_loss: -4.865, critic_loss: 29.632, mean_reward: -1220.272, best_return: -898.205\n",
      "episode:  1063.0 alpha+beta:  35.009937\n",
      "episode: 1063, actor_loss: -0.869, critic_loss: 63.406, mean_reward: -1254.617, best_return: -898.205\n",
      "episode:  1064.0 alpha+beta:  25.304117\n",
      "episode: 1064, actor_loss: -0.798, critic_loss: 18.079, mean_reward: -1197.575, best_return: -898.205\n",
      "episode:  1065.0 alpha+beta:  25.593216\n",
      "episode: 1065, actor_loss: -1.671, critic_loss: 19.351, mean_reward: -1175.827, best_return: -898.205\n",
      "episode:  1066.0 alpha+beta:  23.946249\n",
      "episode: 1066, actor_loss: -0.658, critic_loss: 6.255, mean_reward: -1145.294, best_return: -898.205\n",
      "episode:  1067.0 alpha+beta:  31.736118\n",
      "episode: 1067, actor_loss: 5.131, critic_loss: 32.324, mean_reward: -1243.518, best_return: -898.205\n",
      "episode:  1068.0 alpha+beta:  22.271015\n",
      "episode: 1068, actor_loss: 0.736, critic_loss: 51.954, mean_reward: -1108.755, best_return: -898.205\n",
      "episode:  1069.0 alpha+beta:  22.592415\n",
      "episode: 1069, actor_loss: 0.989, critic_loss: 16.177, mean_reward: -1121.380, best_return: -898.205\n",
      "episode:  1070.0 alpha+beta:  22.380278\n",
      "episode: 1070, actor_loss: 9.207, critic_loss: 20.223, mean_reward: -1219.442, best_return: -898.205\n",
      "episode:  1071.0 alpha+beta:  42.987892\n",
      "episode: 1071, actor_loss: -5.775, critic_loss: 14.139, mean_reward: -1184.255, best_return: -898.205\n",
      "episode:  1072.0 alpha+beta:  38.99121\n",
      "episode: 1072, actor_loss: -4.138, critic_loss: 24.106, mean_reward: -1205.633, best_return: -898.205\n",
      "episode:  1073.0 alpha+beta:  28.163754\n",
      "episode: 1073, actor_loss: 1.810, critic_loss: 16.235, mean_reward: -1263.657, best_return: -898.205\n",
      "episode:  1074.0 alpha+beta:  29.960514\n",
      "episode: 1074, actor_loss: -2.127, critic_loss: 11.496, mean_reward: -1101.181, best_return: -898.205\n",
      "episode:  1075.0 alpha+beta:  38.575417\n",
      "episode: 1075, actor_loss: -0.953, critic_loss: 19.842, mean_reward: -1122.012, best_return: -898.205\n",
      "episode:  1076.0 alpha+beta:  32.47494\n",
      "episode: 1076, actor_loss: -1.818, critic_loss: 12.949, mean_reward: -1355.606, best_return: -898.205\n",
      "episode:  1077.0 alpha+beta:  33.491085\n",
      "episode: 1077, actor_loss: 0.718, critic_loss: 13.039, mean_reward: -1146.377, best_return: -898.205\n",
      "episode:  1078.0 alpha+beta:  35.37707\n",
      "episode: 1078, actor_loss: 6.726, critic_loss: 27.084, mean_reward: -1143.616, best_return: -898.205\n",
      "episode:  1079.0 alpha+beta:  46.7507\n",
      "episode: 1079, actor_loss: -9.870, critic_loss: 18.889, mean_reward: -1168.484, best_return: -898.205\n",
      "episode:  1080.0 alpha+beta:  27.416683\n",
      "episode: 1080, actor_loss: 7.701, critic_loss: 40.248, mean_reward: -1080.912, best_return: -898.205\n",
      "episode:  1081.0 alpha+beta:  43.15566\n",
      "episode: 1081, actor_loss: -7.664, critic_loss: 18.090, mean_reward: -1192.078, best_return: -898.205\n",
      "episode:  1082.0 alpha+beta:  34.826767\n",
      "episode: 1082, actor_loss: -3.184, critic_loss: 20.547, mean_reward: -1171.090, best_return: -898.205\n",
      "episode:  1083.0 alpha+beta:  49.498775\n",
      "episode: 1083, actor_loss: -1.943, critic_loss: 78.256, mean_reward: -1293.379, best_return: -898.205\n",
      "episode:  1084.0 alpha+beta:  32.43112\n",
      "episode: 1084, actor_loss: -2.559, critic_loss: 25.914, mean_reward: -1267.605, best_return: -898.205\n",
      "episode:  1085.0 alpha+beta:  23.464762\n",
      "episode: 1085, actor_loss: 1.753, critic_loss: 24.319, mean_reward: -1170.048, best_return: -898.205\n",
      "episode:  1086.0 alpha+beta:  27.142876\n",
      "episode: 1086, actor_loss: -0.905, critic_loss: 6.938, mean_reward: -1176.819, best_return: -898.205\n",
      "episode:  1087.0 alpha+beta:  26.904228\n",
      "episode: 1087, actor_loss: 1.517, critic_loss: 16.364, mean_reward: -1134.613, best_return: -898.205\n",
      "episode:  1088.0 alpha+beta:  26.606949\n",
      "episode: 1088, actor_loss: 0.537, critic_loss: 3.924, mean_reward: -1082.521, best_return: -898.205\n",
      "episode:  1089.0 alpha+beta:  30.894848\n",
      "episode: 1089, actor_loss: 1.297, critic_loss: 9.046, mean_reward: -1200.697, best_return: -898.205\n",
      "episode:  1090.0 alpha+beta:  32.716587\n",
      "episode: 1090, actor_loss: 5.743, critic_loss: 96.391, mean_reward: -1217.527, best_return: -898.205\n",
      "episode:  1091.0 alpha+beta:  44.784473\n",
      "episode: 1091, actor_loss: -5.433, critic_loss: 17.698, mean_reward: -1098.419, best_return: -898.205\n",
      "episode:  1092.0 alpha+beta:  42.492733\n",
      "episode: 1092, actor_loss: 1.333, critic_loss: 33.334, mean_reward: -1163.242, best_return: -898.205\n",
      "episode:  1093.0 alpha+beta:  23.772835\n",
      "episode: 1093, actor_loss: 1.260, critic_loss: 45.325, mean_reward: -1314.714, best_return: -898.205\n",
      "episode:  1094.0 alpha+beta:  33.499496\n",
      "episode: 1094, actor_loss: -3.141, critic_loss: 26.122, mean_reward: -1319.330, best_return: -898.205\n",
      "episode:  1095.0 alpha+beta:  31.609215\n",
      "episode: 1095, actor_loss: 0.725, critic_loss: 14.577, mean_reward: -1206.622, best_return: -898.205\n",
      "episode:  1096.0 alpha+beta:  28.733034\n",
      "episode: 1096, actor_loss: 0.583, critic_loss: 16.765, mean_reward: -1131.279, best_return: -898.205\n",
      "episode:  1097.0 alpha+beta:  28.868378\n",
      "episode: 1097, actor_loss: 0.683, critic_loss: 12.193, mean_reward: -1123.462, best_return: -898.205\n",
      "episode:  1098.0 alpha+beta:  29.366142\n",
      "episode: 1098, actor_loss: 0.587, critic_loss: 7.608, mean_reward: -1180.097, best_return: -898.205\n",
      "episode:  1099.0 alpha+beta:  28.325586\n",
      "episode: 1099, actor_loss: 0.041, critic_loss: 5.872, mean_reward: -1176.132, best_return: -898.205\n",
      "episode:  1100.0 alpha+beta:  28.972471\n",
      "episode: 1100, actor_loss: 1.632, critic_loss: 112.132, mean_reward: -1173.719, best_return: -898.205\n",
      "last 50 episode mean reward:  -1182.906235971564\n",
      "\n",
      "\n",
      "episode:  1101.0 alpha+beta:  48.22548\n",
      "episode: 1101, actor_loss: -5.252, critic_loss: 27.451, mean_reward: -1235.983, best_return: -898.205\n",
      "episode:  1102.0 alpha+beta:  35.17777\n",
      "episode: 1102, actor_loss: -4.740, critic_loss: 34.653, mean_reward: -1289.049, best_return: -898.205\n",
      "episode:  1103.0 alpha+beta:  34.0942\n",
      "episode: 1103, actor_loss: -3.392, critic_loss: 31.658, mean_reward: -1118.515, best_return: -898.205\n",
      "episode:  1104.0 alpha+beta:  27.718533\n",
      "episode: 1104, actor_loss: -1.673, critic_loss: 16.205, mean_reward: -1142.419, best_return: -898.205\n",
      "episode:  1105.0 alpha+beta:  26.036688\n",
      "episode: 1105, actor_loss: 0.522, critic_loss: 9.602, mean_reward: -1143.770, best_return: -898.205\n",
      "episode:  1106.0 alpha+beta:  26.655783\n",
      "episode: 1106, actor_loss: 1.437, critic_loss: 16.957, mean_reward: -1256.050, best_return: -898.205\n",
      "episode:  1107.0 alpha+beta:  26.700504\n",
      "episode: 1107, actor_loss: 1.162, critic_loss: 4.377, mean_reward: -1164.192, best_return: -898.205\n",
      "episode:  1108.0 alpha+beta:  18.837688\n",
      "episode: 1108, actor_loss: 1.911, critic_loss: 6.709, mean_reward: -1117.208, best_return: -898.205\n",
      "episode:  1109.0 alpha+beta:  26.67898\n",
      "episode: 1109, actor_loss: 0.682, critic_loss: 6.308, mean_reward: -1176.820, best_return: -898.205\n",
      "episode:  1110.0 alpha+beta:  27.450249\n",
      "episode: 1110, actor_loss: 6.529, critic_loss: 139.201, mean_reward: -1163.782, best_return: -898.205\n",
      "episode:  1111.0 alpha+beta:  49.30574\n",
      "episode: 1111, actor_loss: -7.756, critic_loss: 35.348, mean_reward: -1135.268, best_return: -898.205\n",
      "episode:  1112.0 alpha+beta:  52.5014\n",
      "episode: 1112, actor_loss: -0.670, critic_loss: 46.144, mean_reward: -1195.625, best_return: -898.205\n",
      "episode:  1113.0 alpha+beta:  26.893055\n",
      "episode: 1113, actor_loss: 1.609, critic_loss: 133.274, mean_reward: -1190.508, best_return: -898.205\n",
      "episode:  1114.0 alpha+beta:  26.639412\n",
      "episode: 1114, actor_loss: -2.726, critic_loss: 34.583, mean_reward: -1227.585, best_return: -898.205\n",
      "episode:  1115.0 alpha+beta:  43.039467\n",
      "episode: 1115, actor_loss: 0.468, critic_loss: 35.078, mean_reward: -1234.761, best_return: -898.205\n",
      "episode:  1116.0 alpha+beta:  30.079496\n",
      "episode: 1116, actor_loss: 0.978, critic_loss: 14.069, mean_reward: -1121.738, best_return: -898.205\n",
      "episode:  1117.0 alpha+beta:  54.385582\n",
      "episode: 1117, actor_loss: 5.549, critic_loss: 47.641, mean_reward: -1067.336, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1118.0 alpha+beta:  39.878517\n",
      "episode: 1118, actor_loss: -2.187, critic_loss: 24.798, mean_reward: -1196.817, best_return: -898.205\n",
      "episode:  1119.0 alpha+beta:  40.300835\n",
      "episode: 1119, actor_loss: -1.037, critic_loss: 6.672, mean_reward: -1160.465, best_return: -898.205\n",
      "episode:  1120.0 alpha+beta:  29.703552\n",
      "episode: 1120, actor_loss: 4.880, critic_loss: 226.024, mean_reward: -1180.760, best_return: -898.205\n",
      "episode:  1121.0 alpha+beta:  41.78198\n",
      "episode: 1121, actor_loss: -5.213, critic_loss: 26.675, mean_reward: -1255.595, best_return: -898.205\n",
      "episode:  1122.0 alpha+beta:  33.92876\n",
      "episode: 1122, actor_loss: -2.234, critic_loss: 21.673, mean_reward: -1205.836, best_return: -898.205\n",
      "episode:  1123.0 alpha+beta:  41.876823\n",
      "episode: 1123, actor_loss: 3.814, critic_loss: 39.986, mean_reward: -1102.387, best_return: -898.205\n",
      "episode:  1124.0 alpha+beta:  25.820076\n",
      "episode: 1124, actor_loss: -0.666, critic_loss: 10.778, mean_reward: -1228.590, best_return: -898.205\n",
      "episode:  1125.0 alpha+beta:  24.6501\n",
      "episode: 1125, actor_loss: 0.981, critic_loss: 19.074, mean_reward: -1156.986, best_return: -898.205\n",
      "episode:  1126.0 alpha+beta:  24.506\n",
      "episode: 1126, actor_loss: 2.418, critic_loss: 25.152, mean_reward: -1096.533, best_return: -898.205\n",
      "episode:  1127.0 alpha+beta:  24.935127\n",
      "episode: 1127, actor_loss: 1.235, critic_loss: 11.600, mean_reward: -1195.605, best_return: -898.205\n",
      "episode:  1128.0 alpha+beta:  25.919289\n",
      "episode: 1128, actor_loss: 0.558, critic_loss: 20.290, mean_reward: -1178.925, best_return: -898.205\n",
      "episode:  1129.0 alpha+beta:  27.111237\n",
      "episode: 1129, actor_loss: 1.552, critic_loss: 37.518, mean_reward: -1216.048, best_return: -898.205\n",
      "episode:  1130.0 alpha+beta:  25.945158\n",
      "episode: 1130, actor_loss: 13.597, critic_loss: 103.087, mean_reward: -1174.513, best_return: -898.205\n",
      "episode:  1131.0 alpha+beta:  49.408222\n",
      "episode: 1131, actor_loss: -1.915, critic_loss: 53.513, mean_reward: -1155.709, best_return: -898.205\n",
      "episode:  1132.0 alpha+beta:  41.982883\n",
      "episode: 1132, actor_loss: -4.072, critic_loss: 90.312, mean_reward: -1116.096, best_return: -898.205\n",
      "episode:  1133.0 alpha+beta:  37.310696\n",
      "episode: 1133, actor_loss: -0.937, critic_loss: 36.266, mean_reward: -1162.308, best_return: -898.205\n",
      "episode:  1134.0 alpha+beta:  21.915565\n",
      "episode: 1134, actor_loss: 1.647, critic_loss: 35.086, mean_reward: -1150.985, best_return: -898.205\n",
      "episode:  1135.0 alpha+beta:  28.385624\n",
      "episode: 1135, actor_loss: 0.434, critic_loss: 25.150, mean_reward: -1098.158, best_return: -898.205\n",
      "episode:  1136.0 alpha+beta:  29.651754\n",
      "episode: 1136, actor_loss: 0.068, critic_loss: 6.307, mean_reward: -1196.239, best_return: -898.205\n",
      "episode:  1137.0 alpha+beta:  28.361118\n",
      "episode: 1137, actor_loss: -1.157, critic_loss: 9.056, mean_reward: -1203.026, best_return: -898.205\n",
      "episode:  1138.0 alpha+beta:  30.513727\n",
      "episode: 1138, actor_loss: 1.966, critic_loss: 31.636, mean_reward: -1269.420, best_return: -898.205\n",
      "episode:  1139.0 alpha+beta:  27.526932\n",
      "episode: 1139, actor_loss: -1.086, critic_loss: 5.042, mean_reward: -1203.888, best_return: -898.205\n",
      "episode:  1140.0 alpha+beta:  30.00267\n",
      "episode: 1140, actor_loss: -0.931, critic_loss: 155.363, mean_reward: -1216.128, best_return: -898.205\n",
      "episode:  1141.0 alpha+beta:  43.108353\n",
      "episode: 1141, actor_loss: -2.932, critic_loss: 34.462, mean_reward: -1117.467, best_return: -898.205\n",
      "episode:  1142.0 alpha+beta:  31.840872\n",
      "episode: 1142, actor_loss: 0.598, critic_loss: 52.190, mean_reward: -1169.800, best_return: -898.205\n",
      "episode:  1143.0 alpha+beta:  25.279728\n",
      "episode: 1143, actor_loss: 4.139, critic_loss: 60.509, mean_reward: -1282.158, best_return: -898.205\n",
      "episode:  1144.0 alpha+beta:  30.580437\n",
      "episode: 1144, actor_loss: 4.158, critic_loss: 72.763, mean_reward: -1272.849, best_return: -898.205\n",
      "episode:  1145.0 alpha+beta:  52.79762\n",
      "episode: 1145, actor_loss: -1.526, critic_loss: 60.918, mean_reward: -1221.278, best_return: -898.205\n",
      "episode:  1146.0 alpha+beta:  47.222855\n",
      "episode: 1146, actor_loss: 6.178, critic_loss: 60.625, mean_reward: -1273.850, best_return: -898.205\n",
      "episode:  1147.0 alpha+beta:  32.739902\n",
      "episode: 1147, actor_loss: -1.580, critic_loss: 45.600, mean_reward: -1206.348, best_return: -898.205\n",
      "episode:  1148.0 alpha+beta:  32.572624\n",
      "episode: 1148, actor_loss: -0.091, critic_loss: 29.868, mean_reward: -1218.970, best_return: -898.205\n",
      "episode:  1149.0 alpha+beta:  26.657145\n",
      "episode: 1149, actor_loss: 5.807, critic_loss: 75.024, mean_reward: -1216.744, best_return: -898.205\n",
      "episode:  1150.0 alpha+beta:  27.134617\n",
      "episode: 1150, actor_loss: -2.180, critic_loss: 89.478, mean_reward: -1148.846, best_return: -898.205\n",
      "last 50 episode mean reward:  -1184.5986830277739\n",
      "\n",
      "\n",
      "episode:  1151.0 alpha+beta:  51.170288\n",
      "episode: 1151, actor_loss: -5.088, critic_loss: 21.983, mean_reward: -1343.493, best_return: -898.205\n",
      "episode:  1152.0 alpha+beta:  47.940575\n",
      "episode: 1152, actor_loss: -2.691, critic_loss: 31.939, mean_reward: -1207.293, best_return: -898.205\n",
      "episode:  1153.0 alpha+beta:  42.52767\n",
      "episode: 1153, actor_loss: 3.356, critic_loss: 17.975, mean_reward: -1161.817, best_return: -898.205\n",
      "episode:  1154.0 alpha+beta:  26.691792\n",
      "episode: 1154, actor_loss: -2.986, critic_loss: 31.211, mean_reward: -1196.593, best_return: -898.205\n",
      "episode:  1155.0 alpha+beta:  26.029259\n",
      "episode: 1155, actor_loss: -0.367, critic_loss: 4.750, mean_reward: -1263.692, best_return: -898.205\n",
      "episode:  1156.0 alpha+beta:  25.321184\n",
      "episode: 1156, actor_loss: 2.546, critic_loss: 48.206, mean_reward: -1224.664, best_return: -898.205\n",
      "episode:  1157.0 alpha+beta:  22.32141\n",
      "episode: 1157, actor_loss: 1.181, critic_loss: 8.656, mean_reward: -1306.313, best_return: -898.205\n",
      "episode:  1158.0 alpha+beta:  25.44725\n",
      "episode: 1158, actor_loss: 1.718, critic_loss: 34.731, mean_reward: -1195.287, best_return: -898.205\n",
      "episode:  1159.0 alpha+beta:  24.116297\n",
      "episode: 1159, actor_loss: 2.369, critic_loss: 47.038, mean_reward: -1296.904, best_return: -898.205\n",
      "episode:  1160.0 alpha+beta:  24.532885\n",
      "episode: 1160, actor_loss: 9.200, critic_loss: 133.998, mean_reward: -1247.857, best_return: -898.205\n",
      "episode:  1161.0 alpha+beta:  42.505497\n",
      "episode: 1161, actor_loss: -7.277, critic_loss: 54.557, mean_reward: -1210.950, best_return: -898.205\n",
      "episode:  1162.0 alpha+beta:  33.970303\n",
      "episode: 1162, actor_loss: 0.145, critic_loss: 126.249, mean_reward: -1155.015, best_return: -898.205\n",
      "episode:  1163.0 alpha+beta:  43.992176\n",
      "episode: 1163, actor_loss: -5.557, critic_loss: 50.805, mean_reward: -1214.248, best_return: -898.205\n",
      "episode:  1164.0 alpha+beta:  29.746616\n",
      "episode: 1164, actor_loss: -3.371, critic_loss: 50.125, mean_reward: -1125.285, best_return: -898.205\n",
      "episode:  1165.0 alpha+beta:  26.263538\n",
      "episode: 1165, actor_loss: 0.797, critic_loss: 92.998, mean_reward: -1191.111, best_return: -898.205\n",
      "episode:  1166.0 alpha+beta:  27.180326\n",
      "episode: 1166, actor_loss: -0.061, critic_loss: 10.359, mean_reward: -1225.020, best_return: -898.205\n",
      "episode:  1167.0 alpha+beta:  39.763622\n",
      "episode: 1167, actor_loss: 0.363, critic_loss: 15.699, mean_reward: -1106.465, best_return: -898.205\n",
      "episode:  1168.0 alpha+beta:  28.147905\n",
      "episode: 1168, actor_loss: 1.294, critic_loss: 19.991, mean_reward: -1123.506, best_return: -898.205\n",
      "episode:  1169.0 alpha+beta:  27.742105\n",
      "episode: 1169, actor_loss: 1.093, critic_loss: 10.200, mean_reward: -1199.454, best_return: -898.205\n",
      "episode:  1170.0 alpha+beta:  28.729618\n",
      "episode: 1170, actor_loss: 5.709, critic_loss: 143.256, mean_reward: -1198.636, best_return: -898.205\n",
      "episode:  1171.0 alpha+beta:  48.871426\n",
      "episode: 1171, actor_loss: -5.210, critic_loss: 34.316, mean_reward: -1152.659, best_return: -898.205\n",
      "episode:  1172.0 alpha+beta:  40.714523\n",
      "episode: 1172, actor_loss: 3.711, critic_loss: 49.501, mean_reward: -1170.503, best_return: -898.205\n",
      "episode:  1173.0 alpha+beta:  36.64147\n",
      "episode: 1173, actor_loss: -2.777, critic_loss: 87.182, mean_reward: -1177.012, best_return: -898.205\n",
      "episode:  1174.0 alpha+beta:  35.08563\n",
      "episode: 1174, actor_loss: -1.230, critic_loss: 63.076, mean_reward: -1215.919, best_return: -898.205\n",
      "episode:  1175.0 alpha+beta:  47.106777\n",
      "episode: 1175, actor_loss: -0.941, critic_loss: 18.141, mean_reward: -1210.899, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1176.0 alpha+beta:  40.309605\n",
      "episode: 1176, actor_loss: -2.021, critic_loss: 9.414, mean_reward: -1242.696, best_return: -898.205\n",
      "episode:  1177.0 alpha+beta:  36.44001\n",
      "episode: 1177, actor_loss: -1.120, critic_loss: 6.008, mean_reward: -1228.739, best_return: -898.205\n",
      "episode:  1178.0 alpha+beta:  36.574047\n",
      "episode: 1178, actor_loss: -0.111, critic_loss: 4.349, mean_reward: -1262.407, best_return: -898.205\n",
      "episode:  1179.0 alpha+beta:  36.164837\n",
      "episode: 1179, actor_loss: -0.419, critic_loss: 3.588, mean_reward: -1137.386, best_return: -898.205\n",
      "episode:  1180.0 alpha+beta:  50.36656\n",
      "episode: 1180, actor_loss: 8.532, critic_loss: 167.720, mean_reward: -1189.078, best_return: -898.205\n",
      "episode:  1181.0 alpha+beta:  46.065674\n",
      "episode: 1181, actor_loss: -5.648, critic_loss: 19.105, mean_reward: -1202.189, best_return: -898.205\n",
      "episode:  1182.0 alpha+beta:  51.2589\n",
      "episode: 1182, actor_loss: 3.084, critic_loss: 44.202, mean_reward: -1253.406, best_return: -898.205\n",
      "episode:  1183.0 alpha+beta:  57.114098\n",
      "episode: 1183, actor_loss: 0.639, critic_loss: 25.051, mean_reward: -1159.033, best_return: -898.205\n",
      "episode:  1184.0 alpha+beta:  23.244759\n",
      "episode: 1184, actor_loss: -3.233, critic_loss: 80.161, mean_reward: -1213.494, best_return: -898.205\n",
      "episode:  1185.0 alpha+beta:  22.996014\n",
      "episode: 1185, actor_loss: 1.752, critic_loss: 18.762, mean_reward: -1137.023, best_return: -898.205\n",
      "episode:  1186.0 alpha+beta:  33.010963\n",
      "episode: 1186, actor_loss: 0.775, critic_loss: 14.221, mean_reward: -1113.160, best_return: -898.205\n",
      "episode:  1187.0 alpha+beta:  32.635933\n",
      "episode: 1187, actor_loss: 1.029, critic_loss: 6.017, mean_reward: -1155.527, best_return: -898.205\n",
      "episode:  1188.0 alpha+beta:  32.569054\n",
      "episode: 1188, actor_loss: 0.461, critic_loss: 4.794, mean_reward: -1246.232, best_return: -898.205\n",
      "episode:  1189.0 alpha+beta:  32.977524\n",
      "episode: 1189, actor_loss: 0.463, critic_loss: 4.646, mean_reward: -1158.814, best_return: -898.205\n",
      "episode:  1190.0 alpha+beta:  32.32932\n",
      "episode: 1190, actor_loss: 6.406, critic_loss: 140.162, mean_reward: -1103.441, best_return: -898.205\n",
      "episode:  1191.0 alpha+beta:  53.081875\n",
      "episode: 1191, actor_loss: -8.343, critic_loss: 67.123, mean_reward: -1193.566, best_return: -898.205\n",
      "episode:  1192.0 alpha+beta:  54.784836\n",
      "episode: 1192, actor_loss: -3.204, critic_loss: 35.194, mean_reward: -1088.049, best_return: -898.205\n",
      "episode:  1193.0 alpha+beta:  37.161957\n",
      "episode: 1193, actor_loss: -4.175, critic_loss: 52.074, mean_reward: -1248.662, best_return: -898.205\n",
      "episode:  1194.0 alpha+beta:  33.273247\n",
      "episode: 1194, actor_loss: -2.145, critic_loss: 13.196, mean_reward: -1246.580, best_return: -898.205\n",
      "episode:  1195.0 alpha+beta:  35.00634\n",
      "episode: 1195, actor_loss: 0.828, critic_loss: 13.471, mean_reward: -1122.264, best_return: -898.205\n",
      "episode:  1196.0 alpha+beta:  35.344395\n",
      "episode: 1196, actor_loss: 1.092, critic_loss: 10.762, mean_reward: -1178.371, best_return: -898.205\n",
      "episode:  1197.0 alpha+beta:  35.922626\n",
      "episode: 1197, actor_loss: 1.270, critic_loss: 6.521, mean_reward: -1160.949, best_return: -898.205\n",
      "episode:  1198.0 alpha+beta:  44.840313\n",
      "episode: 1198, actor_loss: 5.721, critic_loss: 10.195, mean_reward: -1104.417, best_return: -898.205\n",
      "episode:  1199.0 alpha+beta:  35.77085\n",
      "episode: 1199, actor_loss: 0.744, critic_loss: 7.300, mean_reward: -1118.415, best_return: -898.205\n",
      "episode:  1200.0 alpha+beta:  35.539913\n",
      "episode: 1200, actor_loss: 7.532, critic_loss: 162.631, mean_reward: -1121.108, best_return: -898.205\n",
      "last 50 episode mean reward:  -1190.111882077544\n",
      "\n",
      "\n",
      "episode:  1201.0 alpha+beta:  47.84001\n",
      "episode: 1201, actor_loss: -6.957, critic_loss: 37.337, mean_reward: -1202.146, best_return: -898.205\n",
      "episode:  1202.0 alpha+beta:  43.763817\n",
      "episode: 1202, actor_loss: 1.181, critic_loss: 35.327, mean_reward: -1268.047, best_return: -898.205\n",
      "episode:  1203.0 alpha+beta:  42.00713\n",
      "episode: 1203, actor_loss: -2.769, critic_loss: 49.140, mean_reward: -1243.566, best_return: -898.205\n",
      "episode:  1204.0 alpha+beta:  39.50198\n",
      "episode: 1204, actor_loss: -1.045, critic_loss: 31.969, mean_reward: -1252.883, best_return: -898.205\n",
      "episode:  1205.0 alpha+beta:  44.103794\n",
      "episode: 1205, actor_loss: -0.277, critic_loss: 8.484, mean_reward: -1239.258, best_return: -898.205\n",
      "episode:  1206.0 alpha+beta:  53.977882\n",
      "episode: 1206, actor_loss: 3.623, critic_loss: 13.201, mean_reward: -1261.535, best_return: -898.205\n",
      "episode:  1207.0 alpha+beta:  52.23944\n",
      "episode: 1207, actor_loss: 0.162, critic_loss: 8.522, mean_reward: -1174.272, best_return: -898.205\n",
      "episode:  1208.0 alpha+beta:  42.077522\n",
      "episode: 1208, actor_loss: -1.072, critic_loss: 7.677, mean_reward: -1130.326, best_return: -898.205\n",
      "episode:  1209.0 alpha+beta:  40.968357\n",
      "episode: 1209, actor_loss: -0.554, critic_loss: 6.883, mean_reward: -1268.019, best_return: -898.205\n",
      "episode:  1210.0 alpha+beta:  38.3877\n",
      "episode: 1210, actor_loss: 0.260, critic_loss: 116.397, mean_reward: -1178.587, best_return: -898.205\n",
      "episode:  1211.0 alpha+beta:  50.687477\n",
      "episode: 1211, actor_loss: -6.335, critic_loss: 34.882, mean_reward: -1197.035, best_return: -898.205\n",
      "episode:  1212.0 alpha+beta:  40.231216\n",
      "episode: 1212, actor_loss: -1.761, critic_loss: 25.308, mean_reward: -1265.543, best_return: -898.205\n",
      "episode:  1213.0 alpha+beta:  34.0009\n",
      "episode: 1213, actor_loss: -1.421, critic_loss: 60.480, mean_reward: -1240.832, best_return: -898.205\n",
      "episode:  1214.0 alpha+beta:  32.211517\n",
      "episode: 1214, actor_loss: -0.195, critic_loss: 33.568, mean_reward: -1200.999, best_return: -898.205\n",
      "episode:  1215.0 alpha+beta:  40.291122\n",
      "episode: 1215, actor_loss: 6.599, critic_loss: 27.157, mean_reward: -1249.581, best_return: -898.205\n",
      "episode:  1216.0 alpha+beta:  28.910625\n",
      "episode: 1216, actor_loss: 0.598, critic_loss: 8.889, mean_reward: -1213.518, best_return: -898.205\n",
      "episode:  1217.0 alpha+beta:  30.054502\n",
      "episode: 1217, actor_loss: 0.460, critic_loss: 5.717, mean_reward: -1190.532, best_return: -898.205\n",
      "episode:  1218.0 alpha+beta:  29.977703\n",
      "episode: 1218, actor_loss: 0.386, critic_loss: 4.764, mean_reward: -1205.675, best_return: -898.205\n",
      "episode:  1219.0 alpha+beta:  38.57673\n",
      "episode: 1219, actor_loss: -0.422, critic_loss: 2.503, mean_reward: -1279.405, best_return: -898.205\n",
      "episode:  1220.0 alpha+beta:  29.952448\n",
      "episode: 1220, actor_loss: 7.623, critic_loss: 119.067, mean_reward: -1262.322, best_return: -898.205\n",
      "episode:  1221.0 alpha+beta:  51.512253\n",
      "episode: 1221, actor_loss: -8.409, critic_loss: 32.622, mean_reward: -1253.151, best_return: -898.205\n",
      "episode:  1222.0 alpha+beta:  41.21268\n",
      "episode: 1222, actor_loss: 0.087, critic_loss: 56.999, mean_reward: -1087.108, best_return: -898.205\n",
      "episode:  1223.0 alpha+beta:  39.92577\n",
      "episode: 1223, actor_loss: -3.092, critic_loss: 29.920, mean_reward: -1175.278, best_return: -898.205\n",
      "episode:  1224.0 alpha+beta:  31.663597\n",
      "episode: 1224, actor_loss: 6.803, critic_loss: 30.315, mean_reward: -1137.269, best_return: -898.205\n",
      "episode:  1225.0 alpha+beta:  41.04534\n",
      "episode: 1225, actor_loss: -0.467, critic_loss: 6.981, mean_reward: -1235.272, best_return: -898.205\n",
      "episode:  1226.0 alpha+beta:  43.220196\n",
      "episode: 1226, actor_loss: 0.269, critic_loss: 10.079, mean_reward: -1140.529, best_return: -898.205\n",
      "episode:  1227.0 alpha+beta:  46.69837\n",
      "episode: 1227, actor_loss: 0.966, critic_loss: 5.439, mean_reward: -1254.737, best_return: -898.205\n",
      "episode:  1228.0 alpha+beta:  40.88906\n",
      "episode: 1228, actor_loss: 0.709, critic_loss: 9.323, mean_reward: -1187.844, best_return: -898.205\n",
      "episode:  1229.0 alpha+beta:  40.55781\n",
      "episode: 1229, actor_loss: -0.827, critic_loss: 7.255, mean_reward: -1195.786, best_return: -898.205\n",
      "episode:  1230.0 alpha+beta:  41.510548\n",
      "episode: 1230, actor_loss: 0.410, critic_loss: 143.480, mean_reward: -1216.424, best_return: -898.205\n",
      "episode:  1231.0 alpha+beta:  39.603485\n",
      "episode: 1231, actor_loss: -2.442, critic_loss: 48.708, mean_reward: -1128.388, best_return: -898.205\n",
      "episode:  1232.0 alpha+beta:  40.34565\n",
      "episode: 1232, actor_loss: 0.577, critic_loss: 45.507, mean_reward: -1214.040, best_return: -898.205\n",
      "episode:  1233.0 alpha+beta:  37.03964\n",
      "episode: 1233, actor_loss: -2.839, critic_loss: 21.202, mean_reward: -1233.565, best_return: -898.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1234.0 alpha+beta:  31.997952\n",
      "episode: 1234, actor_loss: 3.952, critic_loss: 56.837, mean_reward: -1179.994, best_return: -898.205\n",
      "episode:  1235.0 alpha+beta:  26.50475\n",
      "episode: 1235, actor_loss: 0.071, critic_loss: 12.721, mean_reward: -1213.002, best_return: -898.205\n",
      "episode:  1236.0 alpha+beta:  25.897598\n",
      "episode: 1236, actor_loss: -0.108, critic_loss: 9.607, mean_reward: -1256.747, best_return: -898.205\n",
      "episode:  1237.0 alpha+beta:  25.32213\n",
      "episode: 1237, actor_loss: 1.939, critic_loss: 39.075, mean_reward: -1192.054, best_return: -898.205\n",
      "episode:  1238.0 alpha+beta:  24.688288\n",
      "episode: 1238, actor_loss: 1.080, critic_loss: 12.773, mean_reward: -1198.065, best_return: -898.205\n",
      "episode:  1239.0 alpha+beta:  25.23667\n",
      "episode: 1239, actor_loss: 0.842, critic_loss: 12.313, mean_reward: -1179.546, best_return: -898.205\n",
      "episode:  1240.0 alpha+beta:  33.33332\n",
      "episode: 1240, actor_loss: 50.450, critic_loss: 214.266, mean_reward: -1175.079, best_return: -898.205\n",
      "episode:  1241.0 alpha+beta:  47.98592\n",
      "episode: 1241, actor_loss: -12.872, critic_loss: 96.189, mean_reward: -1161.461, best_return: -898.205\n",
      "episode:  1242.0 alpha+beta:  47.699192\n",
      "episode: 1242, actor_loss: 1.458, critic_loss: 140.370, mean_reward: -1172.423, best_return: -898.205\n",
      "episode:  1243.0 alpha+beta:  25.263983\n",
      "episode: 1243, actor_loss: 1.570, critic_loss: 198.224, mean_reward: -1217.889, best_return: -898.205\n",
      "episode:  1244.0 alpha+beta:  31.974653\n",
      "episode: 1244, actor_loss: -2.590, critic_loss: 40.840, mean_reward: -1081.508, best_return: -898.205\n",
      "episode:  1245.0 alpha+beta:  29.580189\n",
      "episode: 1245, actor_loss: 0.191, critic_loss: 11.528, mean_reward: -1185.598, best_return: -898.205\n",
      "episode:  1246.0 alpha+beta:  29.43697\n",
      "episode: 1246, actor_loss: 0.996, critic_loss: 5.694, mean_reward: -1128.000, best_return: -898.205\n",
      "episode:  1247.0 alpha+beta:  29.186018\n",
      "episode: 1247, actor_loss: 0.394, critic_loss: 3.583, mean_reward: -1169.916, best_return: -898.205\n",
      "episode:  1248.0 alpha+beta:  37.612972\n",
      "episode: 1248, actor_loss: -0.112, critic_loss: 4.779, mean_reward: -1183.723, best_return: -898.205\n",
      "episode:  1249.0 alpha+beta:  36.836433\n",
      "episode: 1249, actor_loss: 2.781, critic_loss: 24.108, mean_reward: -1170.597, best_return: -898.205\n",
      "episode:  1250.0 alpha+beta:  29.632753\n",
      "episode: 1250, actor_loss: 10.921, critic_loss: 192.333, mean_reward: -1260.957, best_return: -898.205\n",
      "last 50 episode mean reward:  -1202.2006320862042\n",
      "\n",
      "\n",
      "episode:  1251.0 alpha+beta:  40.532307\n",
      "episode: 1251, actor_loss: -4.570, critic_loss: 52.005, mean_reward: -1275.063, best_return: -898.205\n",
      "episode:  1252.0 alpha+beta:  32.877487\n",
      "episode: 1252, actor_loss: -0.737, critic_loss: 64.863, mean_reward: -1265.020, best_return: -898.205\n",
      "episode:  1253.0 alpha+beta:  31.475868\n",
      "episode: 1253, actor_loss: -3.261, critic_loss: 52.812, mean_reward: -1119.258, best_return: -898.205\n",
      "episode:  1254.0 alpha+beta:  30.385967\n",
      "episode: 1254, actor_loss: -1.680, critic_loss: 22.796, mean_reward: -1198.364, best_return: -898.205\n",
      "episode:  1255.0 alpha+beta:  26.63358\n",
      "episode: 1255, actor_loss: -0.421, critic_loss: 12.606, mean_reward: -1150.470, best_return: -898.205\n",
      "episode:  1256.0 alpha+beta:  26.9349\n",
      "episode: 1256, actor_loss: 0.621, critic_loss: 14.686, mean_reward: -1218.585, best_return: -898.205\n",
      "episode:  1257.0 alpha+beta:  26.393856\n",
      "episode: 1257, actor_loss: 0.808, critic_loss: 11.391, mean_reward: -1173.260, best_return: -898.205\n",
      "episode:  1258.0 alpha+beta:  35.325603\n",
      "episode: 1258, actor_loss: -0.175, critic_loss: 5.765, mean_reward: -1200.780, best_return: -898.205\n",
      "episode:  1259.0 alpha+beta:  30.246517\n",
      "episode: 1259, actor_loss: 1.868, critic_loss: 46.694, mean_reward: -1099.351, best_return: -898.205\n",
      "episode:  1260.0 alpha+beta:  22.749784\n",
      "episode: 1260, actor_loss: 11.696, critic_loss: 38.256, mean_reward: -1188.317, best_return: -898.205\n",
      "episode:  1261.0 alpha+beta:  46.977753\n",
      "episode: 1261, actor_loss: -15.357, critic_loss: 59.029, mean_reward: -1156.329, best_return: -898.205\n",
      "episode:  1262.0 alpha+beta:  32.897083\n",
      "episode: 1262, actor_loss: 1.231, critic_loss: 61.838, mean_reward: -1142.288, best_return: -898.205\n",
      "episode:  1263.0 alpha+beta:  33.05051\n",
      "episode: 1263, actor_loss: -2.353, critic_loss: 33.999, mean_reward: -1139.899, best_return: -898.205\n",
      "episode:  1264.0 alpha+beta:  33.5342\n",
      "episode: 1264, actor_loss: -1.380, critic_loss: 35.461, mean_reward: -1224.708, best_return: -898.205\n",
      "episode:  1265.0 alpha+beta:  31.758831\n",
      "episode: 1265, actor_loss: 0.587, critic_loss: 13.938, mean_reward: -1115.284, best_return: -898.205\n",
      "episode:  1266.0 alpha+beta:  32.695152\n",
      "episode: 1266, actor_loss: -0.708, critic_loss: 6.273, mean_reward: -1227.361, best_return: -898.205\n",
      "episode:  1267.0 alpha+beta:  32.21929\n",
      "episode: 1267, actor_loss: 0.787, critic_loss: 7.408, mean_reward: -1129.113, best_return: -898.205\n",
      "episode:  1268.0 alpha+beta:  33.332474\n",
      "episode: 1268, actor_loss: -0.742, critic_loss: 3.463, mean_reward: -1253.751, best_return: -898.205\n",
      "episode:  1269.0 alpha+beta:  36.871643\n",
      "episode: 1269, actor_loss: -0.619, critic_loss: 4.856, mean_reward: -1154.185, best_return: -898.205\n",
      "episode:  1270.0 alpha+beta:  41.334396\n",
      "episode: 1270, actor_loss: 19.556, critic_loss: 86.074, mean_reward: -1211.511, best_return: -898.205\n",
      "episode:  1271.0 alpha+beta:  45.117325\n",
      "episode: 1271, actor_loss: -8.884, critic_loss: 46.478, mean_reward: -1108.109, best_return: -898.205\n",
      "episode:  1272.0 alpha+beta:  40.01162\n",
      "episode: 1272, actor_loss: -4.715, critic_loss: 31.054, mean_reward: -1112.590, best_return: -898.205\n",
      "episode:  1273.0 alpha+beta:  29.67867\n",
      "episode: 1273, actor_loss: 1.083, critic_loss: 39.198, mean_reward: -1163.701, best_return: -898.205\n",
      "episode:  1274.0 alpha+beta:  27.271334\n",
      "episode: 1274, actor_loss: 0.061, critic_loss: 54.174, mean_reward: -1070.245, best_return: -898.205\n",
      "episode:  1275.0 alpha+beta:  24.031382\n",
      "episode: 1275, actor_loss: 2.202, critic_loss: 53.042, mean_reward: -1213.318, best_return: -898.205\n",
      "episode:  1276.0 alpha+beta:  27.296211\n",
      "episode: 1276, actor_loss: 2.412, critic_loss: 9.866, mean_reward: -1144.606, best_return: -898.205\n",
      "episode:  1277.0 alpha+beta:  28.880148\n",
      "episode:  1322.0 alpha+beta:  32.3381\n",
      "episode: 1322, actor_loss: 2.435, critic_loss: 87.107, mean_reward: -1188.425, best_return: -898.205\n",
      "episode:  1323.0 alpha+beta:  37.962704\n",
      "episode: 1323, actor_loss: 2.295, critic_loss: 27.506, mean_reward: -1144.524, best_return: -898.205\n",
      "episode:  1324.0 alpha+beta:  28.594952\n",
      "episode: 1324, actor_loss: 1.173, critic_loss: 7.152, mean_reward: -1082.097, best_return: -898.205\n",
      "episode:  1325.0 alpha+beta:  30.003557\n",
      "episode: 1325, actor_loss: 2.460, critic_loss: 28.623, mean_reward: -1215.158, best_return: -898.205\n",
      "episode:  1326.0 alpha+beta:  31.787914\n",
      "episode: 1326, actor_loss: -0.115, critic_loss: 14.211, mean_reward: -1203.331, best_return: -898.205\n",
      "episode:  1327.0 alpha+beta:  40.433983\n",
      "episode: 1327, actor_loss: 1.000, critic_loss: 26.857, mean_reward: -1118.751, best_return: -898.205\n",
      "episode:  1328.0 alpha+beta:  31.47154\n",
      "episode: 1328, actor_loss: 0.724, critic_loss: 8.842, mean_reward: -1121.074, best_return: -898.205\n",
      "episode:  1329.0 alpha+beta:  31.042591\n",
      "episode: 1329, actor_loss: 1.385, critic_loss: 12.543, mean_reward: -1176.588, best_return: -898.205\n",
      "episode:  1330.0 alpha+beta:  32.65913\n",
      "episode: 1330, actor_loss: 10.593, critic_loss: 192.757, mean_reward: -1215.140, best_return: -898.205\n",
      "episode:  1331.0 alpha+beta:  53.525944\n",
      "episode: 1331, actor_loss: -3.952, critic_loss: 42.796, mean_reward: -1296.775, best_return: -898.205\n",
      "episode:  1332.0 alpha+beta:  36.474823\n",
      "episode: 1332, actor_loss: -0.959, critic_loss: 87.134, mean_reward: -1089.127, best_return: -898.205\n",
      "episode:  1333.0 alpha+beta:  34.671837\n",
      "episode: 1333, actor_loss: -5.290, critic_loss: 71.468, mean_reward: -1163.557, best_return: -898.205\n",
      "episode:  1334.0 alpha+beta:  32.556053\n",
      "episode: 1334, actor_loss: -0.575, critic_loss: 13.756, mean_reward: -1124.149, best_return: -898.205\n",
      "episode:  1335.0 alpha+beta:  33.822014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3d9761eb5a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage, episode_count, \n\u001b[0;32m--> 168\u001b[0;31m                test_reward, best_return)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-166489fa3bbd>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, episode_count, test_reward, best_return, clip_param)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mppo_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0334219a1021>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0malpha_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_beta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20 # 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "use_hindsight    = True\n",
    "\n",
    "model = ActorCritic(2*num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "max_frames = 10000000 # 50000\n",
    "frame_idx  = 0\n",
    "\n",
    "test_rewards = []\n",
    "mean_actor_loss_list = []\n",
    "mean_critic_loss_list = []\n",
    "\n",
    "state = envs.reset()\n",
    "\n",
    "early_stop = False\n",
    "\n",
    "episode_count = 0\n",
    "best_return = -9999\n",
    "multi_goal =  True\n",
    "\n",
    "desired_goal = np.asarray([0,0,0])\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "    \n",
    "    # sample state from previous episode\n",
    "    if multi_goal:\n",
    "        if frame_idx == 1: \n",
    "            goal = initial_subgoal\n",
    "        else: \n",
    "            if len(state) > 1:\n",
    "                goal = state[random.randint(0, num_envs - 1)]\n",
    "            else:\n",
    "                goal = state[0]\n",
    "    else:\n",
    "        goal = desired_goal\n",
    "        \n",
    "    log_probs = []\n",
    "    log_probs_desired = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        state_goals = []\n",
    "        state_desired_goals = []\n",
    "        next_state_goals = []\n",
    "        \n",
    "        # append state with subgoal and desired goal\n",
    "        for s in state: \n",
    "            state_goal = np.concatenate((s,goal),0)\n",
    "            state_goals.append((state_goal))\n",
    "            state_desired_goal = np.concatenate((s, desired_goal), 0)\n",
    "            state_desired_goals.append((state_desired_goal))\n",
    "            \n",
    "        state_goals = np.array(state_goals)\n",
    "        state_goals = torch.FloatTensor(state_goals).to(device)\n",
    "        state_desired_goals = np.array(state_desired_goals)\n",
    "        state_desired_goals = torch.FloatTensor(state_desired_goals).to(device)\n",
    "        \n",
    "        # for subgoal\n",
    "        dist, value, alpha, beta = model(state_goals)\n",
    "        action = dist.sample() * 4 - 2 # pendulum's action range [-2, 2] real value\n",
    "        action = action.reshape(len(action), 1)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        # for desired goal\n",
    "        dist_desired, value_desired, alpha_desired, beta_desired = model(state_desired_goals)\n",
    "        action_desired = dist_desired.sample() * 4 - 2\n",
    "        action_desired = action_desired.reshape(len(action_desired), 1)\n",
    "        \n",
    "        # append next state with sub goal\n",
    "        for n_s in next_state: \n",
    "            next_state_goal = np.concatenate((n_s, goal), 0)\n",
    "            next_state_desired_goal = np.concatenate((n_s, desired_goal), 0)\n",
    "            next_state_goals.append((next_state_goal)) \n",
    "        next_state_goals = np.array(next_state_goals)\n",
    "        \n",
    "        # clip action to range from 0 to 1 for beta distribution\n",
    "        # for subgoal\n",
    "        log_prob = dist.log_prob((action+2)/4)\n",
    "        # for desired goal\n",
    "        log_prob_desired = dist_desired.log_prob((action_desired+2)/4)\n",
    "        \n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        log_probs_desired.append(log_prob_desired)\n",
    "        \n",
    "        values.append(value)\n",
    "        # normalized reward\n",
    "        reward = (reward - np.mean(reward))/(np.std(reward) + 1e-5)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        states.append(state_goals)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % num_steps == 0:\n",
    "            test_reward = np.mean([test_env(model, desired_goal) for _ in range(5)])\n",
    "            test_rewards.append(test_reward)\n",
    "            print ('episode: ', frame_idx/num_steps, 'alpha+beta: ', (alpha.mean(0)+beta.mean(0)).data.cpu().numpy()[0])\n",
    "            if test_reward >= best_return:\n",
    "                best_return = test_reward\n",
    "            # plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "                \n",
    "    episode_count += 1\n",
    "    \n",
    "#     print ('rewards: ', rewards)\n",
    "#     print ('values: ', values)\n",
    "    \n",
    "    next_state_goals = torch.FloatTensor(next_state_goals).to(device)\n",
    "    _, next_value, next_alpha, next_beta = model(next_state_goals)\n",
    "    \n",
    "    old_logprobs = log_probs \n",
    "    current_logprobs = log_probs_desired\n",
    "    \n",
    "    returns   = hindsight_gae(rewards, old_logprobs, current_logprobs, masks, values)\n",
    "#     returns = compute_gae (next_value, rewards, masks, values)\n",
    "                              \n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "#     advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-5)\n",
    "\n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage, episode_count, \n",
    "               test_reward, best_return)\n",
    "    \n",
    "    if frame_idx % (num_steps * 50) == 0:\n",
    "        lower_bound = int((frame_idx - (num_steps * 50)) / num_steps)\n",
    "        upper_bound = int(frame_idx / num_steps)\n",
    "        last_fifty_episode_mean_reward = np.mean(test_rewards[lower_bound:upper_bound])\n",
    "        print ('last 50 episode mean reward: ', last_fifty_episode_mean_reward)\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Testing Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Test Reward Plot/test_rewards_beta', 'wb') as fp1:\n",
    "    pickle.dump(test_rewards, fp1)\n",
    "with open('./Loss Plot/mean_actor_loss_beta', 'wb') as fp2:\n",
    "    pickle.dump(mean_actor_loss_list, fp2)\n",
    "with open('./Loss Plot/mean_critic_loss_beta', 'wb') as fp3:\n",
    "    pickle.dump(mean_critic_loss_list, fp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Save and Load Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weixiang/DML_MF_BO/env3/lib/python3.5/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ActorCritic. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './Model/model_beta' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_model = torch.load('./Model/model_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_test_rewards = []\n",
    "# for i in range(10): \n",
    "# #     env = gym.wrappers.Monitor(env, 'test_video'+str(i), video_callable=lambda episode_id: True)\n",
    "#     expert_test_reward = test_env(expert_model, [0,0,0], False)\n",
    "#     expert_test_rewards.append(expert_test_reward)\n",
    "#     print ('test {0}, total_reward from 28000 steps load model: {1}'.format(i+1, expert_test_reward))\n",
    "\n",
    "# print ('mean expert test reward: ', np.mean(expert_test_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weixiang",
   "language": "python",
   "name": "weixiang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
